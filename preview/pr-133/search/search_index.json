{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OpenShift Examples","text":"<p>The OpenShift Examples is a personal collection of valuable information, code snippets, and practical demonstrations related to OpenShift and Kubernetes. It serves as a repository of Robert's own experiences &amp; contributions, solutions, and best practices in managing and deploying applications on OpenShift.</p> <p>Contributions to this collection are warmly welcomed and highly appreciated!</p> <p>They foster collaboration and knowledge sharing within the OpenShift community, making the repository even more valuable as a collective resource.</p> <p>Feel free to explore the examples, contribute your own insights, and benefit from the expertise shared in this repository.</p>"},{"location":"#usefull-red-hat-solutions-articles-blog-posts","title":"Usefull Red Hat Solutions articles &amp; blog posts","text":"<ul> <li>How can a user update OpenShift 4 console route</li> <li>Red Hat Operators Supported in Disconnected Mode</li> <li>Support Policies for Red Hat OpenShift Container Platform Clusters - Deployments Spanning Multiple Sites(Data Centers/Regions)</li> <li>Red Hat OpenShift Container Platform Update Graph</li> <li>Consolidated Troubleshooting Article OpenShift Container Platform 4.x</li> <li>Red Hat Container Support Policy</li> <li>Red Hat Enterprise Linux Container Compatibility Matrix</li> <li>Consolidated Troubleshooting Article OpenShift Container Platform 4.x</li> <li>Red Hat OpenShift Container Platform Life Cycle Policy</li> <li>Deploying OpenShift 4.x on non-tested platforms using the bare metal install method</li> <li>Certified OpenShift CNI Plug-ins</li> <li>Is it supported to have mixed environment (VMware+Baremetal) setup for RHOCP 4.x cluster</li> <li>Demystifying the OpenShift release image</li> <li>OpenShift Virtualization \u2013 Fencing and VM High Availability Guide</li> </ul>"},{"location":"#stargazers-over-time","title":"Stargazers over time","text":""},{"location":"certificate/","title":"Certificates","text":"","tags":["certificates"]},{"location":"certificate/#debugging-certificate-errors","title":"Debugging Certificate Errors","text":"<p>https://www.netmeister.org/blog/debugging-certificate-errors.html</p>","tags":["certificates"]},{"location":"certificate/#usefull-openssl-comments","title":"Usefull OpenSSL Comments","text":"","tags":["certificates"]},{"location":"certificate/#get-the-list-of-certificates-from-a-secrets","title":"Get the list of certificates from a secrets","text":"<pre><code>SECRET=letsencrypt-router-certs\n\nopenssl crl2pkcs7 -nocrl -certfile \\\n  &lt;(oc get secrets $SECRET -o go-template='{{ index .data \"tls.crt\" | base64decode }}' ) \\\n  | openssl pkcs7 -print_certs  -noout\n</code></pre>","tags":["certificates"]},{"location":"certificate/#check-certificate","title":"Check certificate","text":"<pre><code>echo -n | openssl s_client -connect q.bohne.io:8443 -servername q.bohne.io 2&gt;/dev/null | openssl x509 -noout -subject -issuer\n</code></pre>","tags":["certificates"]},{"location":"certificate/#general-create-a-self-signed-certificate","title":"General: Create a self signed certificate","text":"","tags":["certificates"]},{"location":"certificate/#create-opensslself-signed-certificateconf","title":"Create openssl.self-signed-certificate.conf","text":"Downloadopenssl.self-signed-certificate.conf <pre><code>curl -L -O https://examples.openshift.pub/pr-133/certificate/openssl.self-signed-certificate.conf\n</code></pre> <pre><code>[req]\ndistinguished_name = req_distinguished_name\nx509_extensions = v3_req\nprompt = no\n\n[req_distinguished_name]\nC = US\nST = VA\nL = SomeCity\nO = MyCompany\nOU = MyDivision\nCN = nginx-ex-ssl-stc-pipeline.6923.rh-us-east-1.openshiftapps.com\n\n[v3_req]\nkeyUsage = nonRepudiation, digitalSignature, keyEncipherment\nextendedKeyUsage = serverAuth\nsubjectAltName = @alt_names\n\n[alt_names]\nDNS.1 = nginx-ex-ssl-stc-pipeline.6923.rh-us-east-1.openshiftapps.com\nDNS.2 = company.com\nDNS.3 = company.net\n</code></pre>","tags":["certificates"]},{"location":"certificate/#create-self-signed-certificate","title":"Create self signed certificate","text":"<pre><code>openssl req -x509 -nodes -days 730 \\\n  -newkey rsa:2048 -keyout cert.pem \\\n  -out cert.pem \\\n  -config openssl.self-signed-certificate.conf \\\n  -extensions 'v3_req'\n</code></pre>","tags":["certificates"]},{"location":"certificate/#3-print-self-signed-certificate","title":"3) Print self signed certificate","text":"<pre><code>$ openssl x509 -in cert.pem -noout -text\n    Certificate:\n        Data:\n            Version: 3 (0x2)\n            Serial Number: 15062780286100647982 (0xd109bedd3ac9302e)\n        Signature Algorithm: sha256WithRSAEncryption\n            Issuer: C=US, ST=VA, L=SomeCity, O=MyCompany, OU=MyDivision, CN=nginx-ex-ssl-stc-pipeline.6923.rh-us-east-1.openshiftapps.com\n            Validity\n                Not Before: Apr  7 13:26:38 2019 GMT\n                Not After : Apr  6 13:26:38 2021 GMT\n            Subject: C=US, ST=VA, L=SomeCity, O=MyCompany, OU=MyDivision, CN=nginx-ex-ssl-stc-pipeline.6923.rh-us-east-1.openshiftapps.com\n    [...snipped...]\n            X509v3 extensions:\n                X509v3 Key Usage:\n                    Digital Signature, Non Repudiation, Key Encipherment\n                X509v3 Extended Key Usage:\n                    TLS Web Server Authentication\n                X509v3 Subject Alternative Name:\n                    DNS:nginx-ex-ssl-stc-pipeline.6923.rh-us-east-1.openshiftapps.com, DNS:company.com, DNS:company.net\n    [...snipped...]\n</code></pre>","tags":["certificates"]},{"location":"certificate/#general-own-root-ca-and-certificate","title":"General: Own root ca and certificate","text":"","tags":["certificates"]},{"location":"certificate/#create-openssl-configuration","title":"Create OpenSSL Configuration","text":"","tags":["certificates"]},{"location":"certificate/#create-opensslroot-caconf","title":"Create openssl.root-ca.conf","text":"Downloadopenssl.root-ca.conf <pre><code>curl -L -O https://examples.openshift.pub/pr-133/certificate/openssl.root-ca.conf\n</code></pre> <pre><code># OpenSSL root CA configuration file.\n\n[ req ]\n# Options for the `req` tool (`man req`).\ndefault_bits        = 2048\ndistinguished_name  = req_distinguished_name\nstring_mask         = utf8only\n\n# SHA-1 is deprecated, so use SHA-2 instead.\ndefault_md          = sha256\n\n# Extension to add when the -x509 option is used.\nx509_extensions     = v3_ca\nreq_extensions      = v3_req\n\n[ v3_req ]\n# Extensions to add to a certificate request\nbasicConstraints = CA:FALSE\nkeyUsage = nonRepudiation, digitalSignature, keyEncipherment\nsubjectAltName = @alt_names\n\n[alt_names]\nDNS.1 = rootca.example.com\n# DNS.2 = *.pass.example.com\n# DNS.3 = ...\n# DNS.4 = ...\n\n\n[ req_distinguished_name ]\n# See &lt;https://en.wikipedia.org/wiki/Certificate_signing_request&gt;.\ncountryName                     = Country Name (2 letter code)\nstateOrProvinceName             = State or Province Name\nlocalityName                    = Locality Name\n0.organizationName              = Organization Name\norganizationalUnitName          = Organizational Unit Name\ncommonName                      = Common Name\nemailAddress                    = Email Address\n\n# Optionally, specify some defaults.\ncountryName_default             = DE\nstateOrProvinceName_default     = Bavaria\nlocalityName_default            = Munich\n0.organizationName_default      = My Private Root CA\norganizationalUnitName_default  = My Private Root CA\nemailAddress_default            = email@domain.tld\ncommonName_default              = rootca.example.com\n\n[ v3_ca ]\n# Extensions for a typical CA (`man x509v3_config`).\nsubjectKeyIdentifier = hash\nauthorityKeyIdentifier = keyid:always,issuer\nbasicConstraints = critical, CA:true\nkeyUsage = critical, digitalSignature, cRLSign, keyCertSign\n\n[ v3_intermediate_ca ]\n# Extensions for a typical intermediate CA (`man x509v3_config`).\nsubjectKeyIdentifier = hash\nauthorityKeyIdentifier = keyid:always,issuer\nbasicConstraints = critical, CA:true, pathlen:0\nkeyUsage = critical, digitalSignature, cRLSign, keyCertSign\n\n[ server_cert ]\n# Extensions for server certificates (`man x509v3_config`).\nbasicConstraints = CA:FALSE\nnsCertType = server\nnsComment = \"OpenSSL Generated Server Certificate\"\nsubjectKeyIdentifier = hash\nauthorityKeyIdentifier = keyid,issuer:always\nkeyUsage = critical, digitalSignature, keyEncipherment\nextendedKeyUsage = serverAuth\n\n[ crl_ext ]\n# Extension for CRLs (`man x509v3_config`).\nauthorityKeyIdentifier=keyid:always\n</code></pre>","tags":["certificates"]},{"location":"certificate/#create-opensslcertificateconf","title":"Create openssl.certificate.conf","text":"Downloadopenssl.certificate.conf <pre><code>curl -L -O https://examples.openshift.pub/pr-133/certificate/openssl.certificate.conf\n</code></pre> <pre><code># OpenSSL root CA configuration file.\n\n[ req ]\n# Options for the `req` tool (`man req`).\ndefault_bits        = 2048\ndistinguished_name  = req_distinguished_name\nstring_mask         = utf8only\n\n# SHA-1 is deprecated, so use SHA-2 instead.\ndefault_md          = sha256\n\n# Extension to add when the -x509 option is used.\nx509_extensions     = v3_ca\nreq_extensions      = v3_req\n\n[ v3_req ]\n# Extensions to add to a certificate request\nbasicConstraints = CA:FALSE\nkeyUsage = nonRepudiation, digitalSignature, keyEncipherment\nsubjectAltName = @alt_names\n\n[alt_names]\nDNS.1 = api.example.com\nDNS.2 = *.pass.example.com\nDNS.3 = nginx-ex-ssl-stc-pipeline.6923.rh-us-east-1.openshiftapps.com\n# DNS.4 = ...\n# IP.1 = 172.16.0.5\n\n[ req_distinguished_name ]\n# See &lt;https://en.wikipedia.org/wiki/Certificate_signing_request&gt;.\ncountryName                     = Country Name (2 letter code)\nstateOrProvinceName             = State or Province Name\nlocalityName                    = Locality Name\n0.organizationName              = Organization Name\norganizationalUnitName          = Organizational Unit Name\ncommonName                      = Common Name\nemailAddress                    = Email Address\n\n# Optionally, specify some defaults.\ncountryName_default             = DE\nstateOrProvinceName_default     = Bavaria\nlocalityName_default            = Munich\n0.organizationName_default      = Private\norganizationalUnitName_default  = Private\nemailAddress_default            = email@domain.tld\ncommonName_default              = api.example.com\n\n[ v3_ca ]\n# Extensions for a typical CA (`man x509v3_config`).\nsubjectKeyIdentifier = hash\nauthorityKeyIdentifier = keyid:always,issuer\nbasicConstraints = critical, CA:true\nkeyUsage = critical, digitalSignature, cRLSign, keyCertSign\n\n[ v3_intermediate_ca ]\n# Extensions for a typical intermediate CA (`man x509v3_config`).\nsubjectKeyIdentifier = hash\nauthorityKeyIdentifier = keyid:always,issuer\nbasicConstraints = critical, CA:true, pathlen:0\nkeyUsage = critical, digitalSignature, cRLSign, keyCertSign\n\n[ server_cert ]\n# Extensions for server certificates (`man x509v3_config`).\nbasicConstraints = CA:FALSE\nnsCertType = server\nnsComment = \"OpenSSL Generated Server Certificate\"\nsubjectKeyIdentifier = hash\nauthorityKeyIdentifier = keyid,issuer:always\nkeyUsage = critical, digitalSignature, keyEncipherment\nextendedKeyUsage = serverAuth\n\n[ crl_ext ]\n# Extension for CRLs (`man x509v3_config`).\nauthorityKeyIdentifier=keyid:always\n</code></pre>","tags":["certificates"]},{"location":"certificate/#adjust-opensslcertificateconf","title":"Adjust openssl.certificate.conf","text":"<p>Change commonName, DNS...</p>","tags":["certificates"]},{"location":"certificate/#generate-the-root","title":"Generate the root","text":"<pre><code>openssl genrsa -aes256 -out ca.key -passout pass:openshift 2048\n\nopenssl req -config openssl.root-ca.conf \\\n  -new -x509 -days 7300 -key ca.key -sha256 \\\n  -extensions v3_ca -out ca.crt \\\n  -passin pass:openshift\n</code></pre>","tags":["certificates"]},{"location":"certificate/#generate-the-domain-key","title":"Generate the domain key","text":"<pre><code>openssl genrsa -out ssl.key 2048\n</code></pre>","tags":["certificates"]},{"location":"certificate/#generate-the-certificate-signing-request","title":"Generate the certificate signing request","text":"<pre><code>openssl req -config openssl.certificate.conf \\\n  -sha256 -new -key ssl.key -out ssl.csr\n</code></pre>","tags":["certificates"]},{"location":"certificate/#sign-the-request-with-your-root-key","title":"Sign the request with your root key","text":"<pre><code>openssl x509 -sha256 -req -in ssl.csr \\\n  -CA ca.crt -CAkey ca.key -CAcreateserial \\\n  -out yoursite.org.crt -days 7300 \\\n  -extfile openssl.certificate.conf \\\n  -extensions v3_req \\\n  -passin pass:openshift\n</code></pre>","tags":["certificates"]},{"location":"certificate/#checking-certificate","title":"Checking certificate","text":"<pre><code># Check your homework:\nopenssl verify -CAfile ca.crt yoursite.org.crt\n\n# Deployed certificate on openshift online and test it:\n$ curl -I --cacert ca.crt  --header 'Host: nginx-ex-ssl-stc-pipeline.6923.rh-us-east-1.openshiftapps.com' https://nginx-ex-ssl-stc-pipeline.6923.rh-us-east-1.openshiftapps.com\nHTTP/1.1 200 OK\nServer: nginx/1.12.1\nDate: Sun, 07 Apr 2019 13:47:08 GMT\nContent-Type: text/html\nContent-Length: 37451\nLast-Modified: Sun, 07 Apr 2019 13:08:57 GMT\nETag: \"5ca9f669-924b\"\nAccept-Ranges: bytes\nSet-Cookie: 301fe4e688e3d65605266d24021c9c12=dba6b61bf38d9d364b636a33c0341f4a; path=/; HttpOnly; Secure\nCache-control: private\n</code></pre>","tags":["certificates"]},{"location":"certificate/#trust-own-root-ca-on-your-linux-box","title":"Trust own root CA on your Linux box","text":"<p>https://access.redhat.com/solutions/1519813</p> <pre><code>update-ca-trust enable\ncp -v ca.crt /etc/pki/ca-trust/source/anchors/\nupdate-ca-trust extract\n</code></pre>","tags":["certificates"]},{"location":"certificate/#openshift-4-setup-own-routeringress-certifcate","title":"OpenShift 4 setup own router/ingress certifcate","text":"<p>Official docs:</p> <p>Replacing the default ingress certificate</p> <p>Setting a custom default certificate</p> <p>WARNING: If the default certificate is replaced, it must be signed by a public certificate authority already included in the CA bundle as provided by the container userspace.</p>","tags":["certificates"]},{"location":"certificate/#create-secret-with-certificates","title":"Create secret with certificates","text":"<pre><code>oc create secret tls router-certs \\\n  --cert=letsencrypt/fullchain.crt \\\n  --key=letsencrypt/cert.key \\\n  -n openshift-ingress\n</code></pre>","tags":["certificates"]},{"location":"certificate/#add-secret-to-ingresscontroller","title":"Add secret to ingresscontroller","text":"<pre><code>oc patch ingresscontroller default \\\n  -n openshift-ingress-operator \\\n  --type=merge \\\n  --patch='{\"spec\": { \"defaultCertificate\": { \"name\": \"router-certs\" }}}'\n</code></pre>","tags":["certificates"]},{"location":"certificate/#openshift-4-inject-own-ca-to-trusted-ca-bundle","title":"OpenShift 4 - Inject own CA to trusted CA bundle","text":"<p>Quick notes not tested yet:</p> <pre><code>$ oc get cm/trusted-ca-bundle -o yaml -n openshift-console  |grep MyP\n\n$ oc apply -f - &lt;&lt;EOF\napiVersion: v1\ndata:\n  ca-bundle.crt: |\n    # MyPrivateCA\n    -----BEGIN CERTIFICATE-----\n   zzzzz\n    -----END CERTIFICATE-----\nkind: ConfigMap\nmetadata:\n  name: user-ca-bundle\n  namespace: openshift-config\nEOF\n\n$ oc patch proxy/cluster \\\n      --type=merge \\\n      --patch='{\"spec\": { \"trustedCA\": { \"name\": \"user-ca-bundle\" }}}'\n\n$ oc get cm/trusted-ca-bundle -o yaml -n openshift-console  |grep MyP\n# MyPrivateCA\n</code></pre> <p>CONFIGURING A CUSTOM PKI</p>","tags":["certificates"]},{"location":"certificate/#some-usefull-openssl-commands","title":"Some usefull openssl commands","text":"<pre><code>oc get svc --all-namespaces  -o=custom-columns=\"tls:.metadata.annotations.service\\.alpha\\.openshift\\.io/serving-cert-secret-name,namespace:.metadata.namespace\"  | grep -v '^&lt;none&gt;' | awk '{ print \"oc delete secret/\" $1 \" -n \" $2}'\n</code></pre>","tags":["certificates"]},{"location":"impressum/","title":"Impressum","text":"<p>Angaben gem\u00e4\u00df \u00a7 5 TMG</p> <p>Robert Bohne Hoferichterweg 15 81827 Muenchen</p>"},{"location":"impressum/#vertreten-durch","title":"Vertreten durch:","text":"<p>Robert Bohne</p>"},{"location":"impressum/#kontakt","title":"Kontakt:","text":"<p>Telefon: +49 (0)160 - 8452809 E-Mail: robert (at) bohne (dot) io</p>"},{"location":"impressum/#haftungsausschluss","title":"Haftungsausschluss:","text":""},{"location":"impressum/#haftung-fur-inhalte","title":"Haftung f\u00fcr Inhalte","text":"<p>Die Inhalte unserer Seiten wurden mit gr\u00f6\u00dfter Sorgfalt erstellt. F\u00fcr die Richtigkeit, Vollst\u00e4ndigkeit und Aktualit\u00e4t der Inhalte k\u00f6nnen wir jedoch keine Gew\u00e4hr \u00fcbernehmen. Als Diensteanbieter sind wir gem\u00e4\u00df \u00a7 7 Abs.1 TMG f\u00fcr eigene Inhalte auf diesen Seiten nach den allgemeinen Gesetzen verantwortlich. Nach \u00a7\u00a7 8 bis 10 TMG sind wir als Diensteanbieter jedoch nicht verpflichtet, \u00fcbermittelte oder gespeicherte fremde Informationen zu \u00fcberwachen oder nach Umst\u00e4nden zu forschen, die auf eine rechtswidrige T\u00e4tigkeit hinweisen. Verpflichtungen zur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleiben hiervon unber\u00fchrt. Eine diesbez\u00fcgliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntnis einer konkreten Rechtsverletzung m\u00f6glich. Bei Bekanntwerden von entsprechenden Rechtsverletzungen werden wir diese Inhalte umgehend entfernen.</p>"},{"location":"impressum/#haftung-fur-links","title":"Haftung f\u00fcr Links","text":"<p>Unser Angebot enth\u00e4lt Links zu externen Webseiten Dritter, auf deren Inhalte wir keinen Einfluss haben. Deshalb k\u00f6nnen wir f\u00fcr diese fremden Inhalte auch keine Gew\u00e4hr \u00fcbernehmen. F\u00fcr die Inhalte der verlinkten Seiten ist stets der jeweilige Anbieter oder Betreiber der Seiten verantwortlich. Die verlinkten Seiten wurden zum Zeitpunkt der Verlinkung auf m\u00f6gliche Rechtsverst\u00f6\u00dfe \u00fcberpr\u00fcft. Rechtswidrige Inhalte waren zum Zeitpunkt der Verlinkung nicht erkennbar. Eine permanente inhaltliche Kontrolle der verlinkten Seiten ist jedoch ohne konkrete Anhaltspunkte einer Rechtsverletzung nicht zumutbar. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Links umgehend entfernen.</p>"},{"location":"impressum/#urheberrecht","title":"Urheberrecht","text":"<p>Die durch die Seitenbetreiber erstellten Inhalte und Werke auf diesen Seiten unterliegen dem deutschen Urheberrecht. Die Vervielf\u00e4ltigung, Bearbeitung, Verbreitung und jede Art der Verwertung au\u00dferhalb der Grenzen des Urheberrechtes bed\u00fcrfen der schriftlichen Zustimmung des jeweiligen Autors bzw. Erstellers. Downloads und Kopien dieser Seite sind nur f\u00fcr den privaten, nicht kommerziellen Gebrauch gestattet. Soweit die Inhalte auf dieser Seite nicht vom Betreiber erstellt wurden, werden die Urheberrechte Dritter beachtet. Insbesondere werden Inhalte Dritter als solche gekennzeichnet. Sollten Sie trotzdem auf eine Urheberrechtsverletzung aufmerksam werden, bitten wir um einen entsprechenden Hinweis. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Inhalte umgehend entfernen.</p>"},{"location":"impressum/#datenschutz","title":"Datenschutz","text":"<p>Die Nutzung unserer Webseite ist in der Regel ohne Angabe personenbezogener Daten m\u00f6glich. Soweit auf unseren Seiten personenbezogene Daten (beispielsweise Name, Anschrift oder eMail-Adressen) erhoben werden, erfolgt dies, soweit m\u00f6glich, stets auf freiwilliger Basis. Diese Daten werden ohne Ihre ausdr\u00fcckliche Zustimmung nicht an Dritte weitergegeben. Wir weisen darauf hin, dass die Daten\u00fcbertragung im Internet (z.B. bei der Kommunikation per E-Mail) Sicherheitsl\u00fccken aufweisen kann. Ein l\u00fcckenloser Schutz der Daten vor dem Zugriff durch Dritte ist nicht m\u00f6glich. Der Nutzung von im Rahmen der Impressumspflicht ver\u00f6ffentlichten Kontaktdaten durch Dritte zur \u00dcbersendung von nicht ausdr\u00fccklich angeforderter Werbung und Informationsmaterialien wird hiermit ausdr\u00fccklich widersprochen. Die Betreiber der Seiten behalten sich ausdr\u00fccklich rechtliche Schritte im Falle der unverlangten Zusendung von Werbeinformationen, etwa durch Spam-Mails, vor.</p> <p>Impressum vom Impressum Generator der Kanzlei Hasselbach, Bonn</p>"},{"location":"build/","title":"Build examples","text":"","tags":["build"]},{"location":"build/#simple-docker-build","title":"Simple Docker build","text":"<pre><code># oc create is simple-docker-build\noc apply -f - &lt;&lt;EOF\napiVersion: image.openshift.io/v1\nkind: ImageStream\nmetadata:\n  name: simple-docker-build\nspec:\n  lookupPolicy:\n    local: false\nEOF\n\noc apply -f - &lt;&lt;EOF\napiVersion: build.openshift.io/v1\nkind: BuildConfig\nmetadata:\n  name: simple-docker-build\n  labels:\n    name: simple-docker-build\nspec:\n  triggers:\n    - type: ConfigChange\n  source:\n    contextDir: \"simple-docker-build/\"\n    type: Git\n    git:\n      uri: 'https://github.com/openshift-examples/container-build.git'\n  strategy:\n    type: Docker\n  output:\n    to:\n      kind: ImageStreamTag\n      name: 'simple-docker-build:latest'\nEOF\n</code></pre>","tags":["build"]},{"location":"build/#simple-container-build","title":"Simple Container build","text":"<pre><code>oc create is simple-container-build\n\noc apply -f - &lt;&lt;EOF\napiVersion: build.openshift.io/v1\nkind: BuildConfig\nmetadata:\n  name: simple-container-build\n  labels:\n    name: simple-container-build\nspec:\n  triggers:\n    - type: ConfigChange\n  source:\n    contextDir: \"simple-container-build/\"\n    type: Git\n    git:\n      uri: 'https://github.com/openshift-examples/container-build.git'\n  strategy:\n    type: Docker\n    dockerStrategy:\n      dockerfilePath: \"Containerfile\"\n  output:\n    to:\n      kind: ImageStreamTag\n      name: 'simple-container-build:latest'\nEOF\n</code></pre>","tags":["build"]},{"location":"build/#container-build-w-buildargs","title":"Container build w/ buildArgs","text":"<p>Warning</p> <p>Doesn't work:</p> <ul> <li> <p>https://issues.redhat.com/browse/BUILD-88</p> </li> <li> <p>https://access.redhat.com/solutions/4501551</p> </li> <li> <p>https://bugzilla.redhat.com/show_bug.cgi?id=1959415</p> </li> </ul> <p>Create secret</p> <pre><code>oc create secret generic build-args \\\n  --from-literal=PASSWORD=IeNae1eigheBiz8ne\n\noc create configmap build-args \\\n  --from-literal=USERNAME=foobar24\n</code></pre> <p>Create BuildConfig</p> <pre><code>oc create is build-args\n\noc apply -f - &lt;&lt;EOF\napiVersion: build.openshift.io/v1\nkind: BuildConfig\nmetadata:\n  name: build-args\n  labels:\n    name: build-args\nspec:\n  triggers:\n    - type: ConfigChange\n  source:\n    contextDir: \"buildArgs/\"\n    type: Git\n    git:\n      uri: 'https://github.com/openshift-examples/container-build.git'\n  strategy:\n    type: Docker\n    dockerStrategy:\n      dockerfilePath: \"Containerfile\"\n      buildArgs:\n        - name: VERSION\n          value: staging\n        - name: USERNAME\n          valueFrom:\n            configMapKeyRef:\n              key: USERNAME\n              name: build-args\n        - name: PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: PASSWORD\n              name: build-args\n  output:\n    to:\n      kind: ImageStreamTag\n      name: 'build-args:latest'\nEOF\n</code></pre>","tags":["build"]},{"location":"build/#simple-context-dir","title":"Simple context dir","text":"<pre><code>oc create is simple-context-dir\n\noc apply -f - &lt;&lt;EOF\napiVersion: build.openshift.io/v1\nkind: BuildConfig\nmetadata:\n  name: simple-context-dir\n  labels:\n    name: simple-context-dir\nspec:\n  triggers:\n    - type: ConfigChange\n  source:\n    contextDir: \"simple-context-dir/\"\n    type: Git\n    git:\n      uri: 'https://github.com/openshift-examples/container-build.git'\n  strategy:\n    type: Docker\n    dockerStrategy:\n      dockerfilePath: \"Containerfile\"\n  output:\n    to:\n      kind: ImageStreamTag\n      name: 'simple-context-dir:latest'\nEOF\n</code></pre> <p>Start &amp; Follow build: <code>oc start-build bc/build-args --follow</code></p>","tags":["build"]},{"location":"build/#complex-context-dir","title":"Complex context dir","text":"<p>```yaml hl_li&gt;nes=\"14 20 21\" oc create is complex-context-dir</p> <p>oc apply -f - &lt;&lt;EOF apiVersion: build.openshift.io/v1 kind: BuildConfig metadata:   name: complex-context-dir   labels:     name: complex-context-dir spec:   triggers:     - type: ConfigChange   source:     contextDir: \"complex-context-dir/\"     type: Git     git:       uri: 'https://github.com/openshift-examples/container-build.git'   strategy:     type: Docker     dockerStrategy:       dockerfilePath: \"containerfiles/Containerfile\"   output:     to:       kind: ImageStreamTag       name: 'complex-context-dir:latest' EOF <pre><code>## Multi-stage - builder &amp; runner\n\n** Nothing special at BuildConfig, checkout the Containerfile: **\n\n```Dockerfile hl_lines=\"1 12\"\nFROM centos:8 AS builder\n\nRUN yum groupinstall -y 'Development Tools'\n\nRUN curl -L -O https://bird.network.cz/download/bird-1.6.8.tar.gz &amp;&amp; \\\n    tar xzf bird-1.6.8.tar.gz &amp;&amp; \\\n    cd bird-1.6.8 &amp;&amp; \\\n    ./configure --disable-client --prefix=/opt/bird-1.6.8 &amp;&amp; \\\n    make install\n\nFROM registry.access.redhat.com/ubi8/ubi-minimal AS runner\nCOPY --from=builder /opt/bird-1.6.8 /opt/bird-1.6.8\nENTRYPOINT [\"/opt/bird-1.6.8/sbin/bird\", \"-f\"]\n</code></pre></p>","tags":["build"]},{"location":"build/#java-jar-binary-build","title":"Java / JAR Binary Build","text":"<ul> <li>Drag &amp; Drop in OpenShift 4.8:</li> </ul>","tags":["build"]},{"location":"build/#create-a-jar","title":"Create a JAR","text":"<pre><code>git clone https://github.com/spring-projects/spring-petclinic.git\ncd spring-petclinic\npodman run -ti --rm --user 0 -v $(pwd):/work:Z registry.redhat.io/ubi8/openjdk-11 bash\ncd /work\nmvn package\nexit\n</code></pre>","tags":["build"]},{"location":"build/#build-with-odo","title":"Build with ODO","text":"<pre><code>odo create java test1 --s2i --binary target/*.jar\nodo push\n</code></pre> <p>Note</p> <p>Checkout: <code>odo catalog list components</code></p>","tags":["build"]},{"location":"build/#build-with-classic-buildconfig","title":"Build with classic BuildConfig","text":"<pre><code>oc new-build java --name=java-binary-build --binary=true\noc start-build bc/java-binary-build \\\n    --from-file=./target/*.jar \\\n    --follow\n</code></pre>","tags":["build"]},{"location":"build/#go-source-2-image","title":"Go source-2-image","text":"<ul> <li>via Containerfile: https://github.com/openshift-examples/container-helper</li> <li>odo/s2i:  <code>odo create golang --s2i --git https://github.com/openshift-examples/container-helper.git --port 8080</code></li> </ul>","tags":["build"]},{"location":"build/#buildconfig","title":"BuildConfig","text":"<pre><code>oc create is multi-stage\n\noc apply -f - &lt;&lt;EOF\napiVersion: build.openshift.io/v1\nkind: BuildConfig\nmetadata:\n  name: multi-stage\n  labels:\n    name: multi-stage\nspec:\n  triggers:\n    - type: ConfigChange\n  source:\n    contextDir: \"multi-stage/\"\n    type: Git\n    git:\n      uri: 'https://github.com/openshift-examples/container-build.git'\n  strategy:\n    type: Docker\n    dockerStrategy:\n      dockerfilePath: \"Containerfile\"\n  output:\n    to:\n      kind: ImageStreamTag\n      name: 'multi-stage:latest'\nEOF\n</code></pre>","tags":["build"]},{"location":"build/#build-and-push-to-quay","title":"Build and push to quay","text":"","tags":["build"]},{"location":"build/#create-push-secret","title":"Create push-secret","text":"<pre><code>oc create -f - &lt;&lt;EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: openshift-examples-openshift-push-demo-pull-secret\ndata:\n  .dockerconfigjson: xxxxx\ntype: kubernetes.io/dockerconfigjson\nEOF\n</code></pre>","tags":["build"]},{"location":"build/#create-build-config","title":"Create build config","text":"<pre><code>oc new-build --name=simple-http-server \\\n  --push-secret='openshift-examples-openshift-push-demo-pull-secret' \\\n  --to-docker=true \\\n  --to=\"quay.io/openshift-examples/simple-http-server:dev\" \\\n  https://github.com/openshift-examples/simple-http-server.git\n</code></pre>","tags":["build"]},{"location":"build/#custom-build-with-buildah","title":"Custom build with Buildah","text":"","tags":["build"]},{"location":"build/#add-git-config","title":"Add git config","text":"<p>OpenShift 3.11 documenation</p> <p>Create <code>/tmp/gitconfig</code></p> <pre><code>[http]\n    sslVerify = false\n# Just for information:\n[core]\n    sshCommand = \"ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no\"\n</code></pre> <p>Run commands: ( Create secret &amp; add <code>--source-secret=build</code> to new-build )</p> <pre><code>oc create secret generic build --from-file=.gitconfig=/tmp/gitconfig \\\n    --from-file=ssh-privatekey=/tmp/github_rsa \\\n    --type=kubernetes.io/ssh-auth\n\noc new-build registry.access.redhat.com/redhat-openjdk-18/openjdk18-openshift~git@github.com:rbo/chaos-professor.git --source-secret=build --env BUILD_LOGLEVEL=5\n\n# If you like, create app\noc new-app chaos-professor\n</code></pre>","tags":["build"]},{"location":"build/entitled/","title":"Entitled builds and OpenShift 4","text":"","tags":["entitlement","build","v4.18"]},{"location":"build/entitled/#what-is-an-entitlement","title":"What is an entitlement","text":"<p>Technically, the entitlement is a certificate to get access to specific Red Hat Enterprise Linux content and has to be refreshed regularly. Red Hat introduced Simple Content Access to simplify the access, for example for container builds.</p> <p>With <code>openssl</code> or <code>rct</code> command you can get some information from your entitlement:</p> <pre><code>$ rct stat-cert /etc/pki/entitlement/entitlement.pem\nType: Entitlement Certificate\nVersion: 3.4\nDER size: 1610b\nSubject Key ID size: 20b\nContent sets: 5835\n$ openssl x509 -in /etc/pki/entitlement/entitlement.pem -noout -issuer\nissuer=C = US, ST = North Carolina, O = \"Red Hat, Inc.\", OU = Red Hat Network, CN = Red Hat Candlepin Authority, emailAddress = ca-support@redhat.com\n$ rct cat-cert  /etc/pki/entitlement/entitlement.pem  | head -n15\n\n+-------------------------------------------+\n    Entitlement Certificate\n+-------------------------------------------+\n\nCertificate:\n    Path: /etc/pki/entitlement/entitlement.pem\n    Version: 3.4\n    Serial: &lt;Cert Serial&gt;\n    Start Date: 2022-07-10 03:19:11+00:00\n    End Date: 2023-07-10 03:19:11+00:00\n    Pool ID: Not Available\n\nSubject:\n    CN: xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx\n</code></pre> <p>How to get the entitlement certificate? If simple content access is enabled at your organisation/redhat account, the insights Operatos automatically provide and refresh and entitlement to your OpenShift 4 Cluster.</p> <p>You can enable and check the Simple content access at https://access.redhat.com/management, it should look like this:</p> <p>At your OpenShift 4 Cluster you can take a look your entitlement via:</p> <pre><code>$ oc get secrets etc-pki-entitlement -n openshift-config-managed  -o jsonpath=\"{.data.entitlement\\.pem}\" | base64 -d &gt; entitlement.pem\n\n$ rct cat-cert entitlement.pem | head -n15\n\n+-------------------------------------------+\n    Entitlement Certificate\n+-------------------------------------------+\n\nCertificate:\n    Path: entitlement.pem\n    Version: 3.4\n    Serial: &lt;Cert Serial&gt;\n    Start Date: 2022-07-10 05:06:45+00:00\n    End Date: 2023-07-10 05:06:45+00:00\n    Pool ID: Not Available\n\nSubject:\n    CN: xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx\n$\n</code></pre> <p>Relevant documentation part: Importing simple content access certificates with Insights Operator</p> <p>Another option to get an entitlement from your Red Hat Satellite installation in your environment. Or copy the entitlement from a subscribed Red Hat Enterprise Linux - this is not recommended, and I assume this is against Red Hat Terms and conditions.</p>","tags":["entitlement","build","v4.18"]},{"location":"build/entitled/#prerequisites-to-run-an-entitled-build","title":"Prerequisites to run an entitled build","text":"","tags":["entitlement","build","v4.18"]},{"location":"build/entitled/#install-operator","title":"Install Operator","text":"<ul> <li>Install Builds for Red Hat OpenShift Operator (tested with v1.4.0)</li> </ul>","tags":["entitlement","build","v4.18"]},{"location":"build/entitled/#share-the-entitlement-secrets","title":"Share the entitlement secrets","text":"OCsharedsecret.yaml <pre><code>oc apply -f https://examples.openshift.pub/pr-133/build/entitled/sharedsecret.yaml\n</code></pre> <pre><code>apiVersion: sharedresource.openshift.io/v1alpha1\nkind: SharedSecret\nmetadata:\n  name: etc-pki-entitlement\nspec:\n  secretRef:\n    name: etc-pki-entitlement\n    namespace: openshift-config-managed\n</code></pre>","tags":["entitlement","build","v4.18"]},{"location":"build/entitled/#add-the-permissions-to-share-the-secret","title":"Add the permissions to share the secret","text":"OCsharedsecret-permissions.yaml <pre><code>oc apply -f https://examples.openshift.pub/pr-133/build/entitled/sharedsecret-permissions.yaml\n</code></pre> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: share-etc-pki-entitlement\n  namespace: openshift-config-managed\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - secrets\n    resourceNames:\n      - etc-pki-entitlement\n    verbs:\n      - get\n      - list\n      - watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: share-etc-pki-entitlement\n  namespace: openshift-config-managed\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: share-etc-pki-entitlement\nsubjects:\n  - kind: ServiceAccount\n    name: csi-driver-shared-resource\n    namespace: openshift-builds\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: use-share-etc-pki-entitlement\nrules:\n  - apiGroups:\n      - sharedresource.openshift.io\n    resources:\n      - sharedsecrets\n    resourceNames:\n      - etc-pki-entitlement\n    verbs:\n      - use\n</code></pre>","tags":["entitlement","build","v4.18"]},{"location":"build/entitled/#create-a-projectnamespace","title":"Create a project/namespace","text":"OC <pre><code>oc new-project entitled-build-demo\n</code></pre>","tags":["entitlement","build","v4.18"]},{"location":"build/entitled/#allow-access-to-shared-secrets","title":"Allow access to shared secrets","text":"<p>In case you want to roll out automaticly for every new project, please use the project request tempalte.</p> OCsharedsecret-allow-namespace.yaml <pre><code>oc apply -f https://examples.openshift.pub/pr-133/build/entitled/sharedsecret-allow-namespace.yaml\n</code></pre> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: use-share-etc-pki-entitlement\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: use-share-etc-pki-entitlement\nsubjects:\n  - kind: ServiceAccount\n    name: pipeline\n  - kind: ServiceAccount\n    name: builder\n</code></pre>","tags":["entitlement","build","v4.18"]},{"location":"build/entitled/#lets-create-a-build","title":"Let's create a build","text":"build.yamlOC <pre><code>apiVersion: shipwright.io/v1beta1\nkind: Build\nmetadata:\n  name: buildah-rhel\n  namespace: entitled-build-demo\nspec:\n  output:\n    image: 'image-registry.openshift-image-registry.svc:5000/entitled-build-demo/demo:latest'\n  paramValues:\n    - name: dockerfile\n      value: Containerfile\n  source:\n    contextDir: entitled-build\n    git:\n      url: 'https://github.com/openshift-examples/container-build'\n    type: Git\n  strategy:\n    kind: ClusterBuildStrategy\n    name: buildah\n  volumes:\n    - csi:\n        driver: csi.sharedresource.openshift.io\n        readOnly: true\n        volumeAttributes:\n          sharedSecret: etc-pki-entitlement\n      name: etc-pki-entitlement\n</code></pre> <pre><code>oc apply -f https://examples.openshift.pub/pr-133/build/entitled/build.yaml\n</code></pre>","tags":["entitlement","build","v4.18"]},{"location":"build/entitled/#start-the-build","title":"Start the build","text":"build-run.yamlOC <pre><code>apiVersion: shipwright.io/v1beta1\nkind: BuildRun\nmetadata:\n  generateName: buildah-rhel-\n  namespace: entitled-build-demo\nspec:\n  build:\n    name: buildah-rhel\n</code></pre> <pre><code>oc apply -f https://examples.openshift.pub/pr-133/build/entitled/build-run.yaml\n</code></pre>","tags":["entitlement","build","v4.18"]},{"location":"build/entitled/#additional-resources","title":"Additional resources","text":"<ul> <li>https://shipwright.io/</li> </ul>","tags":["entitlement","build","v4.18"]},{"location":"build/jenkins/","title":"Jenkins Pipeline","text":"<p>Note</p> <p>Deprecated in OpenShift 4</p>"},{"location":"build/jenkins/#simple-pipeline-demo","title":"Simple pipeline Demo","text":"<pre><code>oc new-project pipeline\noc new-app jenkins-ephemeral\noc create -f https://raw.githubusercontent.com/openshift/origin/master/examples/jenkins/pipeline/nodejs-sample-pipeline.yaml\n</code></pre>"},{"location":"build/jenkins/#build-and-push-image-into-many-registries","title":"Build and push image into many registries","text":"<p>Based on Promoting container images between registries with skopeo</p> <p>Two different ways to get Skopoe \"into\" Jenkins</p> <p>1) Custom Jenkins Slave</p> <pre><code>https://github.com/siamaksade/openshift-cd-demo/blob/ocp-3.11/cicd-template.yaml#L229\n\nSource for the slave image https://github.com/siamaksade/jenkins-slave-skopeo\n\nBased on https://docs.openshift.com/container-platform/3.11/dev_guide/dev_tutorials/openshift_pipeline.html\n\nhttps://github.com/redhat-cop/containers-quickstarts/tree/master/jenkins-slaves/jenkins-slave-image-mgmt\n</code></pre> <p>2) Custom Jenkins Agent, was the differents?</p> <p>https://github.com/jenkinsci/kubernetes-plugin https://github.com/openshift/jenkins-client-plugin</p> <pre><code>podTemplate(\n  label: \"scopeo\",\n  cloud: \"openshift\",\n  inheritFrom: \"maven\",\n  containers: [\n    containerTemplate(\n      name: \"jnlp\",\n      image: \"quay.io/your_repo/jenkins-slave-skopeo-centos:master\",\n      resourceRequestMemory: \"512Mi\",\n      resourceLimitMemory: \"1Gi\"\n    )\n  ]\n)\n</code></pre> <p>Dockerfile</p> <pre><code>FROM openshift/jenkins-slave-base-centos7\nMAINTAINER Tero Ahonen &lt;tero@redhat.com&gt;\nUSER root\nRUN yum -y install skopeo\nUSER 1001\n</code></pre>"},{"location":"build/jenkins/#build-deploy-namespace-one-deploy-namespace-2","title":"Build &amp; Deploy namespace one -&gt; deploy namespace 2","text":"<pre><code>oc new-project prod\n\noc new-project dev\n\noc process -f https://raw.githubusercontent.com/rbo/openshift-tasks/master/app-template.yaml -p SOURCE_URL=https://github.com/rbo/openshift-tasks | oc create -f -\n\noc policy add-role-to-group edit system:serviceaccounts:default -n prod\n\nTRIGER: OpenShift ONLY\n\noc tag dev/tasks:latest prod/tasks:latest\noc project prod\noc new-app tasks\noc expose svc/tasks\n\n# Rollback - OPENSHIFT ONLY!\noc rollback dc/tasks\n</code></pre>"},{"location":"build/s2i-r-shiny/","title":"Source to image example for R shiny","text":"","tags":["s2i"]},{"location":"build/s2i-r-shiny/#build-builder-image","title":"Build builder image","text":"<pre><code>oc create -f - -n openshift &lt;&lt;EOF\n---\napiVersion: image.openshift.io/v1\nkind: ImageStream\nmetadata:\n  name: r-shiny-s2i\n---\napiVersion: build.openshift.io/v1\nkind: BuildConfig\nmetadata:\n  labels:\n    build: r-shiny-s2i\n  name: r-shiny-s2i\nspec:\n  failedBuildsHistoryLimit: 5\n  nodeSelector: null\n  output:\n    to:\n      kind: ImageStreamTag\n      name: r-shiny-s2i:latest\n  postCommit: {}\n  resources: {}\n  runPolicy: Serial\n  source:\n    contextDir: build/s2i-R-shiny/builder\n    git:\n      uri: 'https://github.com/rbo/openshift-examples.git'\n    type: Git\n  strategy:\n    dockerStrategy:\n      from:\n        kind: DockerImage\n        name: 'rhscl/s2i-base-rhel7:latest'\n    type: Docker\n  successfulBuildsHistoryLimit: 5\nEOF\noc start-build r-shiny-s2i [--follow] -n openshift\noc create -f - -n openshif &lt;&lt;EOF\napiVersion: template.openshift.io/v1\nkind: Template\nlabels:\n  template: s2i-r-shiny-example\nmessage: |-\n  The following service(s) have been created in your project: ${NAME}.\n\n  For more information about using this template, including OpenShift considerations, see https://github.com/rbo/openshift-examples/tree/master/build/s2i-R-shiny.\nmetadata:\n  annotations:\n    description: An example R shiny https://github.com/rbo/openshift-examples/tree/master/build/s2i-R-shiny.\n    iconClass: icon-play\n    openshift.io/display-name: R Shiny exmaple\n    openshift.io/documentation-url: https://github.com/rbo/openshift-examples/tree/master/build/s2i-R-shiny\n    openshift.io/long-description: R Shiny exmaple\n    openshift.io/provider-display-name: Robert Bohne\n    openshift.io/support-url: https://github.com/rbo/openshift-examples/tree/master/build/s2i-R-shiny\n    tags: quickstart,r,shiny,r-shiny,s2i\n  creationTimestamp: null\n  name: s2i-r-shiny-example\nobjects:\n- apiVersion: v1\n  kind: Service\n  metadata:\n    annotations:\n      description: Exposes and load balances the application pods\n    name: ${NAME}\n  spec:\n    ports:\n    - name: web\n      port: 8080\n      targetPort: 8080\n    selector:\n      name: ${NAME}\n- apiVersion: v1\n  kind: Route\n  metadata:\n    annotations:\n      template.openshift.io/expose-uri: http://{.spec.host}{.spec.path}\n    name: ${NAME}\n  spec:\n    host: ${APPLICATION_DOMAIN}\n    to:\n      kind: Service\n      name: ${NAME}\n- apiVersion: v1\n  kind: ImageStream\n  metadata:\n    annotations:\n      description: Keeps track of changes in the application image\n    name: ${NAME}\n- apiVersion: v1\n  kind: BuildConfig\n  metadata:\n    annotations:\n      description: Defines how to build the application\n      template.alpha.openshift.io/wait-for-ready: \"true\"\n    name: ${NAME}\n  spec:\n    output:\n      to:\n        kind: ImageStreamTag\n        name: ${NAME}:latest\n    source:\n      contextDir: ${CONTEXT_DIR}\n      git:\n        ref: ${SOURCE_REPOSITORY_REF}\n        uri: ${SOURCE_REPOSITORY_URL}\n      type: Git\n    strategy:\n      sourceStrategy:\n        from:\n          kind: ImageStreamTag\n          name: r-shiny-s2i:latest\n          namespace: ${NAMESPACE}\n      type: Source\n    triggers:\n    - type: ImageChange\n    - type: ConfigChange\n    - github:\n        secret: ${GITHUB_WEBHOOK_SECRET}\n      type: GitHub\n    - generic:\n        secret: ${GENERIC_WEBHOOK_SECRET}\n      type: Generic\n- apiVersion: v1\n  kind: DeploymentConfig\n  metadata:\n    annotations:\n      description: Defines how to deploy the application server\n      template.alpha.openshift.io/wait-for-ready: \"true\"\n    name: ${NAME}\n  spec:\n    replicas: 1\n    selector:\n      name: ${NAME}\n    strategy:\n      type: Rolling\n    template:\n      metadata:\n        labels:\n          name: ${NAME}\n        name: ${NAME}\n      spec:\n        containers:\n        - env: []\n          image: ' '\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 8080\n            initialDelaySeconds: 30\n            timeoutSeconds: 3\n          name: s2i-r-shiny\n          ports:\n          - containerPort: 8080\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8080\n            initialDelaySeconds: 3\n            timeoutSeconds: 3\n          resources:\n            limits:\n              memory: ${MEMORY_LIMIT}\n    triggers:\n    - imageChangeParams:\n        automatic: true\n        containerNames:\n        - s2i-r-shiny\n        from:\n          kind: ImageStreamTag\n          name: ${NAME}:latest\n      type: ImageChange\n    - type: ConfigChange\nparameters:\n- description: The name assigned to all of the frontend objects defined in this template.\n  displayName: Name\n  name: NAME\n  required: true\n  value: word-cloud\n- description: The OpenShift Namespace where the ImageStream resides.\n  displayName: Namespace\n  name: NAMESPACE\n  required: true\n  value: openshift\n- description: Maximum amount of memory the container can use.\n  displayName: Memory Limit\n  name: MEMORY_LIMIT\n  required: true\n  value: 512Mi\n- description: The URL of the repository with your application source code.\n  displayName: Git Repository URL\n  name: SOURCE_REPOSITORY_URL\n  required: true\n  value: https://github.com/rstudio/shiny-examples.git\n- description: Set this to a branch name, tag or other ref of your repository if you\n    are not using the default branch.\n  displayName: Git Reference\n  name: SOURCE_REPOSITORY_REF\n  value: master\n- description: Set this to the relative path to your project if it is not in the root\n    of your repository.\n  displayName: Context Directory\n  name: CONTEXT_DIR\n  value: 082-word-cloud\n- description: The exposed hostname that will route to the r shiny service, if left\n    blank a value will be defaulted.\n  displayName: Application Hostname\n  name: APPLICATION_DOMAIN\n- description: Github trigger secret.  A difficult to guess string encoded as part\n    of the webhook URL.  Not encrypted.\n  displayName: GitHub Webhook Secret\n  from: '[a-zA-Z0-9]{40}'\n  generate: expression\n  name: GITHUB_WEBHOOK_SECRET\n- description: A secret string used to configure the Generic webhook.\n  displayName: Generic Webhook Secret\n  from: '[a-zA-Z0-9]{40}'\n  generate: expression\n  name: GENERIC_WEBHOOK_SECRET\nEOF\n</code></pre>","tags":["s2i"]},{"location":"build/s2i-r-shiny/#insides-from-the-builder","title":"Insides from the builder","text":"<p>Install app lication and dependencies with</p> <pre><code>$ R -s -e \"library(deplearning); depl_check()\"\n$ R -s -e \"menu = function(choices, graphics = FALSE, title = NULL) { return(1) };  library(deplearning); depl_check()\"\n</code></pre> <p>Run app with</p> <pre><code>$ R -s -e 'library(\"shiny\"); runApp()'\n</code></pre>","tags":["s2i"]},{"location":"build/s2i-r-shiny/#build-app-with-builder-image","title":"Build app with builder image","text":"<pre><code>oc new-app r-shiny-s2i~https://github.com/rstudio/shiny-examples \\\n    --context-dir=082-word-cloud \\\n    --name=word-cloud \\\n    --strategy=source\noc expose svc/word-cloud\n</code></pre>","tags":["s2i"]},{"location":"build/s2i-r-shiny/#resources","title":"Resources","text":"<ul> <li>https://www.r-bloggers.com/permanently-setting-the-cran-repository/</li> <li>https://rdrr.io/github/MilesMcBain/deplearning/</li> </ul>","tags":["s2i"]},{"location":"build/s2i-r-shiny/#playground","title":"Playground","text":"<p>Dockerfile.playground:</p> <pre><code>FROM docker-registry-default.ocp3.bohne.io/openshift/r-shiny-s2i:latest\nCOPY ./s2i/bin/ /usr/libexec/s2i\n\nUSER 1001\n\n# Set the default port for applications built using this image\nEXPOSE 8080\n\nCMD [\"/usr/libexec/s2i/usage\"]\n</code></pre> <p>And build and run:</p> <pre><code>docker pull docker-registry-default.ocp3.bohne.io/openshift/r-shiny-s2i:latest\ndocker build -t b -f Dockerfile.playground .\ns2i build https://github.com/rstudio/shiny-examples b b-app --context-dir=027-absolutely-positioned-panels --loglevel=99\n# Or by hand: docker run -ti --entrypoint bash -v $(pwd):/tmp/src b\n\n\ndocker run word-cloud\n</code></pre>","tags":["s2i"]},{"location":"build/s2i-r-shiny/#usefull-deplay-allsh","title":"Usefull deplay-all.sh","text":"<pre><code>#!/usr/bin/env bash\n\nLIST=\"001-hello\n002-text\n003-reactivity\n004-mpg\n005-sliders\n006-tabsets\n007-widgets\n008-html\n009-upload\n010-download\n011-timer\n012-datatables\n013-selectize\n014-onflushed\n015-layout-navbar\n015-layout-sidebar\n016-knitr-pdf\n017-select-vs-selectize\n018-datatable-options\n019-mathjax\n020-knit-html\n021-selectize-plot\n022-unicode-chinese\n023-optgroup-server\n024-optgroup-selectize\n025-loop-ui\n026-shiny-inline\n027-absolutely-positioned-panels\n028-actionbutton-demo\n030-basic-datatable\n032-client-data-and-query-string\n033-conditionalpanel-demo\n034-current-time\n035-custom-input-bindings\n036-custom-input-control\n037-date-and-date-range\n039-download-file\n040-dynamic-clustering\n041-dynamic-ui\n047-image-output\n048-including-html-text-and-markdown-files\n049-isolate-demo\n050-kmeans-example\n051-movie-explorer\n052-navbar-example\n053-navlistpanel-example\n054-nvd3-line-chart-output\n055-observer-demo\n057-plot-plus-three-columns\n059-reactive-poll-and-file-reader\n060-retirement-simulation\n061-server-to-client-custom-messages\n062-submitbutton-demo\n063-superzip-example\n064-telephones-by-region\n065-update-input-demo\n066-upload-file\n067-vertical-layout\n068-widget-action-button\n069-widget-check-group\n070-widget-checkbox\n071-widget-date\n072-widget-date-range\n073-widget-file\n074-widget-numeric\n075-widget-radio\n076-widget-select\n077-widget-slider\n078-widget-slider-range\n079-widget-submit\n080-widget-text\n081-widgets-gallery\n082-word-cloud\n083-front-page\n084-single-file\n085-progress\n086-bus-dashboard\n087-crandash\n088-action-pattern1\n089-action-pattern2\n090-action-pattern3\n091-action-pattern4\n092-action-pattern5\n093-plot-interaction-basic\n094-image-interaction-basic\n095-plot-interaction-advanced\n096-plot-interaction-article-1\n097-plot-interaction-article-2\n098-plot-interaction-article-3\n099-plot-interaction-article-4\n100-plot-interaction-article-5\n101-plot-interaction-article-6\n102-plot-interaction-article-7\n103-plot-interaction-article-8\n104-plot-interaction-select\n105-plot-interaction-zoom\n106-plot-interaction-exclude\n107-events\n108-module-output\n109-render-table\n110-error-sanitization\n111-insert-ui\n112-generate-report\n113-bookmarking-url\n114-modal-dialog\n115-bookmarking-updatequerystring\n116-notifications\n117-shinythemes\n118-highcharter-births\n119-namespaced-conditionalpanel-demo\n120-goog-index\n121-async-timer\n122-async-outputs\n123-async-renderprint\n124-async-download\n125-async-req\n126-async-ticks\n127-async-flush\n128-plot-dim-error\n129-async-perf\n130-output-null\n131-renderplot-args\n132-async-events\n133-async-hold-inputs\n134-async-hold-timers\n135-bookmark-uioutput\n136-plot-cache\n137-plot-cache-key\n138-icon-fontawesome\n139-plot-brush-scaling\n140-selectize-inputs\n141-radiant\n142-reactive-timer\n143-async-plot-caching\n144-colors\n145-dt-replacedata\n146-ames-explorer\n147-websocket\n148-addresourcepath-deleted\n149-onRender\n150-networkD3-sankey\n151-reactr-input\n152-set-reactivevalue\n153-connection-header\n154-index-html-server-r\n155-index-html-app-r\n156-subapps\"\n\n# LIST=\"079-widget-submit\"\n\nfor i in $(echo $LIST) ; do\n  CONTEXT_DIR=$i;\n  NAME=$(echo $i | cut -d'-' -f2-)\n  echo \"----&gt; Deploy $NAME\"\n  oc new-app --template=s2i-r-shiny-example \\\n    --param=CONTEXT_DIR=$CONTEXT_DIR \\\n    --param=NAME=$NAME\ndone;\n</code></pre>","tags":["s2i"]},{"location":"build/universal-base-image/","title":"Universal Base Images","text":"","tags":["ubi"]},{"location":"build/universal-base-image/#install-packages-from-rhel-repos","title":"Install packages from RHEL Repos","text":"<pre><code>FROM registry.access.redhat.com/ubi8\n# https://redhat-connect.gitbook.io/best-practices-guide/base-image#adding-rhel-packages-to-ubi\nRUN dnf install -y --enablerepo=ansible-2.9-for-rhel-8-x86_64-rpms ansible\nRUN dnf install -y --enablerepo=rhocp-4.6-for-rhel-8-x86_64-rpms python3-requests-oauthlib python3-openshift openshift-clients\n</code></pre>","tags":["ubi"]},{"location":"build/universal-base-image/#resources","title":"Resources","text":"<ul> <li>Red Hat Universal Base Images<ul> <li>FAQ</li> <li>Universal Base Images (UBI): Images, repositories, packages, and source code</li> </ul> </li> <li>CHAPTER 4. USING RED HAT UNIVERSAL BASE IMAGES (STANDARD, MINIMAL, AND RUNTIMES)</li> <li>Blog: How to use entitled image builds to build DriverContainers with UBI on OpenShift</li> <li>Blog: Red Hat simplifies container development and redistribution of Red Hat Enterprise Linux packages</li> </ul>","tags":["ubi"]},{"location":"build/universal-base-image/#example","title":"Example","text":"<ul> <li>openshift-examples / gitlab-runner</li> </ul>","tags":["ubi"]},{"location":"build/pipeline/","title":"Tekton / OpenShift Pipelines","text":"","tags":["tekton","pipeline","build"]},{"location":"build/pipeline/#container-build-buildah-with-secrets","title":"Container build (buildah) with secrets","text":"<p>Task documentation: https://tekton.dev/docs/pipelines/tasks/</p>","tags":["tekton","pipeline","build"]},{"location":"build/pipeline/#create-secret","title":"Create secret","text":"<pre><code>oc create secret generic build-args \\\n  --from-literal=USERNAME=web-auth-user \\\n  --from-literal=PASSWORD=IeNae1eigheBiz8ne\n</code></pre>","tags":["tekton","pipeline","build"]},{"location":"build/pipeline/#apply-buildah-with-secret-task","title":"Apply buildah-with-secret task:","text":"OCbuildah-with-secret.yaml <pre><code>oc apply -f https://examples.openshift.pub/pr-133/build/pipeline/buildah-with-secret.yaml\n</code></pre> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\n  annotations:\n    tekton.dev/pipelines.minVersion: \"0.19\"\n    tekton.dev/tags: image-build\n  creationTimestamp: \"2021-05-11T14:49:48Z\"\n  labels:\n    app.kubernetes.io/version: \"0.1-with-secret\"\n    operator.tekton.dev/provider-type: redhat\n  name: buildah-with-secret\nspec:\n  description: |-\n    Buildah task builds source into a container image and then pushes it to a container registry.\n    Buildah Task builds source into a container image using Project Atomic's Buildah build tool.It uses Buildah's support for building from Dockerfiles, using its buildah bud command.This command executes the directives in the Dockerfile to assemble a container image, then pushes that image to a container registry.\n  params:\n  - description: Reference of the image buildah will produce.\n    name: IMAGE\n    type: string\n  - default: registry.redhat.io/rhel8/buildah:latest\n    description: The location of the buildah builder image.\n    name: BUILDER_IMAGE\n    type: string\n  - default: vfs\n    description: Set buildah storage driver\n    name: STORAGE_DRIVER\n    type: string\n  - default: ./Dockerfile\n    description: Path to the Dockerfile to build.\n    name: DOCKERFILE\n    type: string\n  - default: .\n    description: Path to the directory to use as context.\n    name: CONTEXT\n    type: string\n  - default: \"true\"\n    description: Verify the TLS on the registry endpoint (for push/pull to a non-TLS registry)\n    name: TLSVERIFY\n    type: string\n  - default: oci\n    description: The format of the built container, oci or docker\n    name: FORMAT\n    type: string\n  - default: \"\"\n    description: Extra parameters passed for the build command when building images.\n    name: BUILD_EXTRA_ARGS\n    type: string\n  - default: \"\"\n    description: Extra parameters passed for the push command when pushing images.\n    name: PUSH_EXTRA_ARGS\n    type: string\n  results:\n  - description: Digest of the image just built.\n    name: IMAGE_DIGEST\n  steps:\n  - image: $(params.BUILDER_IMAGE)\n    name: build\n    resources: {}\n    script: |\n      buildah --storage-driver=$(params.STORAGE_DRIVER) bud \\\n        $(params.BUILD_EXTRA_ARGS) --format=$(params.FORMAT) \\\n        --tls-verify=$(params.TLSVERIFY) --no-cache \\\n        -f $(params.DOCKERFILE) -t $(params.IMAGE) $(params.CONTEXT)\n    volumeMounts:\n    - mountPath: /var/lib/containers\n      name: varlibcontainers\n    workingDir: $(workspaces.source.path)\n    envFrom:\n      - secretRef:\n          name: build-args\n  - image: $(params.BUILDER_IMAGE)\n    name: push\n    resources: {}\n    script: |\n      buildah --storage-driver=$(params.STORAGE_DRIVER) push \\\n        $(params.PUSH_EXTRA_ARGS) --tls-verify=$(params.TLSVERIFY) \\\n        --digestfile $(workspaces.source.path)/image-digest $(params.IMAGE) \\\n        docker://$(params.IMAGE)\n    volumeMounts:\n    - mountPath: /var/lib/containers\n      name: varlibcontainers\n    workingDir: $(workspaces.source.path)\n  - image: $(params.BUILDER_IMAGE)\n    name: digest-to-results\n    resources: {}\n    script: cat $(workspaces.source.path)/image-digest | tee /tekton/results/IMAGE_DIGEST\n  volumes:\n  - emptyDir: {}\n    name: varlibcontainers\n  workspaces:\n  - name: source\n</code></pre>","tags":["tekton","pipeline","build"]},{"location":"build/pipeline/#apply-pipeline","title":"Apply pipeline:","text":"OCbuildah-pipeline.yaml <pre><code>oc apply -f https://examples.openshift.pub/pr-133/build/pipeline/buildah-pipeline.yaml\n</code></pre> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\n  name: buildah-pipeline\nspec:\n  params:\n  - default: image-registry.openshift-image-registry.svc:5000/$(context.pipelineRun.namespace)/buildargs:latest\n    name: IMAGE_NAME\n    type: string\n  - default: https://github.com/openshift-examples/container-build.git\n    name: GIT_REPO\n    type: string\n  - default: master\n    name: GIT_REVISION\n    type: string\n  - name: DOCKERFILE\n    type: string\n    default: \"./Containerfile\"\n  - name: CONTEXT\n    type: string\n    default: \"buildArgs\"\n  - name: BUILD_EXTRA_ARGS\n    type: string\n    default: \"--build-arg USERNAME=$USERNAME --build-arg PASSWORD=$PASSWORD\"\n  tasks:\n  - name: fetch-repository\n    params:\n    - name: url\n      value: $(params.GIT_REPO)\n    - name: revision\n      value: $(params.GIT_REVISION)\n    taskRef:\n      kind: ClusterTask\n      name: git-clone\n    workspaces:\n    - name: output\n      workspace: workspace\n  - name: buildah-with-secret\n    params:\n    - name: IMAGE\n      value: $(params.IMAGE_NAME)\n    - name: DOCKERFILE\n      value: $(params.DOCKERFILE)\n    - name: CONTEXT\n      value: $(params.CONTEXT)\n    - name: BUILD_EXTRA_ARGS\n      value: $(params.BUILD_EXTRA_ARGS)\n    runAfter:\n    - fetch-repository\n    taskRef:\n      kind: Task\n      name: buildah-with-secret\n    workspaces:\n    - name: source\n      workspace: workspace\n  workspaces:\n  - name: workspace\n</code></pre>","tags":["tekton","pipeline","build"]},{"location":"build/pipeline/#start-pipeline","title":"Start pipeline","text":"OCbuildah-pipelinerun.yamltkn <pre><code>oc apply -f https://examples.openshift.pub/pr-133/build/pipeline/buildah-pipelinerun.yaml\n</code></pre> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: PipelineRun\nmetadata:\n  labels:\n    tekton.dev/pipeline: buildah-pipeline\n  name: buildah-pipeline-1\nspec:\n  pipelineRef:\n    name: buildah-pipeline\n  serviceAccountName: pipeline\n  timeout: 1h0m0s\n  workspaces:\n  - name: workspace\n    # persistentVolumeClaim:\n    #   claimName: workspace\n    volumeClaimTemplate:\n      spec:\n        accessModes:\n          - ReadWriteMany # access mode may affect how you can use this volume in parallel tasks\n        resources:\n          requests:\n            storage: 1Gi\n</code></pre> <p>Missing --workspac argument becuase missnig documentation:   https://github.com/tektoncd/cli/issues/1169</p> <pre><code>tkn pipeline start buildah-pipeline -p \\\n  IMAGE_NAME=image-registry.openshift-image-registry.svc:5000/demo-app/container-build:latest \\\n  GIT_REPO=https://github.com/openshift-examples/container-build.git \\\n  CONTEXT=buildArgs \\\n  DOCKERFILE=./Containerfile \\\n  BUILD_EXTRA_ARGS='--build-arg USERNAME=$USERNAME --build-arg PASSWORD=$PASSWORD' \\\n</code></pre>","tags":["tekton","pipeline","build"]},{"location":"build/pipeline/buildpacks/","title":"Cloud Native BuildPacks &amp; Tekton","text":"<p>Based on official Buildpacks.io doc</p>","tags":["pipeline","tekton","buildpacks"]},{"location":"client/","title":"OpenShift/Kubernetes Client","text":"","tags":["client","kubectl","oc"]},{"location":"client/#resources","title":"Resources","text":"<ul> <li>https://github.com/openshift-examples/client-go-template</li> <li>https://www.openshift.com/blog/customizing-oc-output-with-go-templates</li> <li>https://github.com/brandisher/openshift-and-gotemplates-workshop</li> </ul>","tags":["client","kubectl","oc"]},{"location":"client/#different-pod-details","title":"Different Pod details","text":"","tags":["client","kubectl","oc"]},{"location":"client/#list-of-pods-with-resources","title":"List of Pods with resources","text":"CLIGo TemplateExample Output <pre><code>curl -O -L https://examples.openshift.pub/pr-133/client/podlist-with-resources.gotemplate\noc get pods -n openshift-monitoring -o go-template-file=podlist-with-resources.gotemplate\n</code></pre> <pre><code>{{$printf_format := \"%-50s\\t%-50s\\t%-50s\\t%s\\n\" }}\n{{- printf $printf_format \"NAMESPACE\" \"POD NAME\" \"CONTAINER NAME\" \"RESOURCES\" -}}\n{{- range .items -}}\n    {{- if eq .kind \"Pod\" -}}\n            {{$namespace := .metadata.namespace -}}\n            {{$pod_name := .metadata.name}}\n        {{- range .spec.containers -}}\n            {{$resources := \"There are no resources.\"}}\n            {{- if .resources -}}\n                {{$resources = .resources }}\n            {{- end -}}\n            {{- printf $printf_format $namespace $pod_name .name $resources -}}\n        {{- end -}}\n    {{- end -}}\n{{- end -}}\n</code></pre> <pre><code>NAMESPACE                                           POD NAME                                            CONTAINER NAME                                      RESOURCES\nopenshift-monitoring                                alertmanager-main-0                                 alertmanager                                        map[requests:map[cpu:4m memory:40Mi]]\nopenshift-monitoring                                alertmanager-main-0                                 config-reloader                                     map[requests:map[cpu:1m memory:10Mi]]\nopenshift-monitoring                                alertmanager-main-0                                 alertmanager-proxy                                  map[requests:map[cpu:1m memory:20Mi]]\nopenshift-monitoring                                alertmanager-main-0                                 kube-rbac-proxy                                     map[requests:map[cpu:1m memory:15Mi]]\nopenshift-monitoring                                alertmanager-main-0                                 prom-label-proxy                                    map[requests:map[cpu:1m memory:20Mi]]\nopenshift-monitoring                                alertmanager-main-1                                 alertmanager                                        map[requests:map[cpu:4m memory:40Mi]]\nopenshift-monitoring                                alertmanager-main-1                                 config-reloader                                     map[requests:map[cpu:1m memory:10Mi]]\nopenshift-monitoring                                alertmanager-main-1                                 alertmanager-proxy                                  map[requests:map[cpu:1m memory:20Mi]]\nopenshift-monitoring                                alertmanager-main-1                                 kube-rbac-proxy                                     map[requests:map[cpu:1m memory:15Mi]]\nopenshift-monitoring                                alertmanager-main-1                                 prom-label-proxy                                    map[requests:map[cpu:1m memory:20Mi]]\nopenshift-monitoring                                alertmanager-main-2                                 alertmanager                                        map[requests:map[cpu:4m memory:40Mi]]\nopenshift-monitoring                                alertmanager-main-2                                 config-reloader                                     map[requests:map[cpu:1m memory:10Mi]]\nopenshift-monitoring                                alertmanager-main-2                                 alertmanager-proxy                                  map[requests:map[cpu:1m memory:20Mi]]\nopenshift-monitoring                                alertmanager-main-2                                 kube-rbac-proxy                                     map[requests:map[cpu:1m memory:15Mi]]\nopenshift-monitoring                                alertmanager-main-2                                 prom-label-proxy                                    map[requests:map[cpu:1m memory:20Mi]]\nopenshift-monitoring                                cluster-monitoring-operator-75f69c8797-5427v        kube-rbac-proxy                                     map[requests:map[cpu:1m memory:20Mi]]\nopenshift-monitoring                                cluster-monitoring-operator-75f69c8797-5427v        cluster-monitoring-operator                         map[requests:map[cpu:10m memory:75Mi]]\nopenshift-monitoring                                grafana-6d68f8478f-fx2bp                            grafana                                             map[requests:map[cpu:4m memory:64Mi]]\nopenshift-monitoring                                grafana-6d68f8478f-fx2bp                            grafana-proxy                                       map[requests:map[cpu:1m memory:20Mi]]\nopenshift-monitoring                                kube-state-metrics-5f457b6bc7-59fsw                 kube-state-metrics                                  map[requests:map[cpu:2m memory:80Mi]]\nopenshift-monitoring                                kube-state-metrics-5f457b6bc7-59fsw                 kube-rbac-proxy-main                                map[requests:map[cpu:1m memory:15Mi]]\nopenshift-monitoring                                kube-state-metrics-5f457b6bc7-59fsw                 kube-rbac-proxy-self                                map[requests:map[cpu:1m memory:15Mi]]\nopenshift-monitoring                                node-exporter-5q2cs                                 node-exporter                                       map[requests:map[cpu:8m memory:32Mi]]\nopenshift-monitoring                                node-exporter-5q2cs                                 kube-rbac-proxy                                     map[requests:map[cpu:1m memory:15Mi]]\nopenshift-monitoring                                node-exporter-kz2rc                                 node-exporter                                       map[requests:map[cpu:8m memory:32Mi]]\nopenshift-monitoring                                node-exporter-kz2rc                                 kube-rbac-proxy                                     map[requests:map[cpu:1m memory:15Mi]]\nopenshift-monitoring                                node-exporter-s545f                                 node-exporter                                       map[requests:map[cpu:8m memory:32Mi]]\nopenshift-monitoring                                node-exporter-s545f                                 kube-rbac-proxy                                     map[requests:map[cpu:1m memory:15Mi]]\nopenshift-monitoring                                node-exporter-xdlqs                                 node-exporter                                       map[requests:map[cpu:8m memory:32Mi]]\nopenshift-monitoring                                node-exporter-xdlqs                                 kube-rbac-proxy                                     map[requests:map[cpu:1m memory:15Mi]]\nopenshift-monitoring                                openshift-state-metrics-67cb99cb76-n9s2z            kube-rbac-proxy-main                                map[requests:map[cpu:1m memory:20Mi]]\nopenshift-monitoring                                openshift-state-metrics-67cb99cb76-n9s2z            kube-rbac-proxy-self                                map[requests:map[cpu:1m memory:20Mi]]\nopenshift-monitoring                                openshift-state-metrics-67cb99cb76-n9s2z            openshift-state-metrics                             map[requests:map[cpu:1m memory:32Mi]]\nopenshift-monitoring                                prometheus-adapter-55dd69754d-ff477                 prometheus-adapter                                  map[requests:map[cpu:1m memory:40Mi]]\nopenshift-monitoring                                prometheus-adapter-55dd69754d-kv8pd                 prometheus-adapter                                  map[requests:map[cpu:1m memory:40Mi]]\nopenshift-monitoring                                prometheus-k8s-0                                    prometheus                                          map[requests:map[cpu:70m memory:1Gi]]\nopenshift-monitoring                                prometheus-k8s-0                                    config-reloader                                     map[requests:map[cpu:1m memory:10Mi]]\nopenshift-monitoring                                prometheus-k8s-0                                    thanos-sidecar                                      map[requests:map[cpu:1m memory:25Mi]]\nopenshift-monitoring                                prometheus-k8s-0                                    prometheus-proxy                                    map[requests:map[cpu:1m memory:20Mi]]\nopenshift-monitoring                                prometheus-k8s-0                                    kube-rbac-proxy                                     map[requests:map[cpu:1m memory:15Mi]]\nopenshift-monitoring                                prometheus-k8s-0                                    prom-label-proxy                                    map[requests:map[cpu:1m memory:15Mi]]\nopenshift-monitoring                                prometheus-k8s-0                                    kube-rbac-proxy-thanos                              map[requests:map[cpu:1m memory:10Mi]]\nopenshift-monitoring                                prometheus-k8s-1                                    prometheus                                          map[requests:map[cpu:70m memory:1Gi]]\nopenshift-monitoring                                prometheus-k8s-1                                    config-reloader                                     map[requests:map[cpu:1m memory:10Mi]]\nopenshift-monitoring                                prometheus-k8s-1                                    thanos-sidecar                                      map[requests:map[cpu:1m memory:25Mi]]\nopenshift-monitoring                                prometheus-k8s-1                                    prometheus-proxy                                    map[requests:map[cpu:1m memory:20Mi]]\nopenshift-monitoring                                prometheus-k8s-1                                    kube-rbac-proxy                                     map[requests:map[cpu:1m memory:15Mi]]\nopenshift-monitoring                                prometheus-k8s-1                                    prom-label-proxy                                    map[requests:map[cpu:1m memory:15Mi]]\nopenshift-monitoring                                prometheus-k8s-1                                    kube-rbac-proxy-thanos                              map[requests:map[cpu:1m memory:10Mi]]\nopenshift-monitoring                                prometheus-operator-974874c6d-vn425                 prometheus-operator                                 map[requests:map[cpu:5m memory:150Mi]]\nopenshift-monitoring                                prometheus-operator-974874c6d-vn425                 kube-rbac-proxy                                     map[requests:map[cpu:1m memory:15Mi]]\nopenshift-monitoring                                telemeter-client-56fb9b5d77-q5wpp                   telemeter-client                                    map[requests:map[cpu:1m memory:40Mi]]\nopenshift-monitoring                                telemeter-client-56fb9b5d77-q5wpp                   reload                                              map[requests:map[cpu:1m memory:10Mi]]\nopenshift-monitoring                                telemeter-client-56fb9b5d77-q5wpp                   kube-rbac-proxy                                     map[requests:map[cpu:1m memory:20Mi]]\nopenshift-monitoring                                thanos-querier-674d9c9bcd-gswsz                     thanos-query                                        map[requests:map[cpu:10m memory:12Mi]]\nopenshift-monitoring                                thanos-querier-674d9c9bcd-gswsz                     oauth-proxy                                         map[requests:map[cpu:1m memory:20Mi]]\nopenshift-monitoring                                thanos-querier-674d9c9bcd-gswsz                     kube-rbac-proxy                                     map[requests:map[cpu:1m memory:15Mi]]\nopenshift-monitoring                                thanos-querier-674d9c9bcd-gswsz                     prom-label-proxy                                    map[requests:map[cpu:1m memory:15Mi]]\nopenshift-monitoring                                thanos-querier-674d9c9bcd-gswsz                     kube-rbac-proxy-rules                               map[requests:map[cpu:1m memory:15Mi]]\nopenshift-monitoring                                thanos-querier-674d9c9bcd-zs6qf                     thanos-query                                        map[requests:map[cpu:10m memory:12Mi]]\nopenshift-monitoring                                thanos-querier-674d9c9bcd-zs6qf                     oauth-proxy                                         map[requests:map[cpu:1m memory:20Mi]]\nopenshift-monitoring                                thanos-querier-674d9c9bcd-zs6qf                     kube-rbac-proxy                                     map[requests:map[cpu:1m memory:15Mi]]\nopenshift-monitoring                                thanos-querier-674d9c9bcd-zs6qf                     prom-label-proxy                                    map[requests:map[cpu:1m memory:15Mi]]\nopenshift-monitoring                                thanos-querier-674d9c9bcd-zs6qf                     kube-rbac-proxy-rules                               map[requests:map[cpu:1m memory:15Mi]]\n</code></pre>","tags":["client","kubectl","oc"]},{"location":"client/#list-of-nodes-with-resources","title":"List of Nodes with resources","text":"CLIGo TemplateExample Output <pre><code>curl -O -L https://examples.openshift.pub/pr-133/client/nodelist-with-resources.gotemplate\noc get nodes -o go-template-file=nodelist-with-resources.gotemplate\n</code></pre> <pre><code>{{$printf_format := \"%-40s\\t%-20s\\t%-20s\\n\" }}\n{{- printf $printf_format \"NODE\" \"CAPACITY CPU\" \"CAPACITY MEMORY\" -}}\n{{- range .items -}}\n    {{- if eq .kind \"Node\" -}}\n            {{$name := .metadata.name}}\n            {{- printf $printf_format .metadata.name .status.capacity.cpu .status.capacity.memory -}}\n    {{- end -}}\n{{- end -}}\n</code></pre> <pre><code>NODE                                        CAPACITY CPU            CAPACITY MEMORY     \ncontrol-plane-0.hc1.openshift.pub           8                       15954776Ki          \nworker-0-arm.hc1.openshift.pub              8                       15954776Ki          \nworker-0-x86.hc1.openshift.pub              4                       7927544Ki           \n</code></pre>","tags":["client","kubectl","oc"]},{"location":"client/#curl-kubernetesopenshift-api-examples","title":"cURL &amp; Kubernetes/OpenShift API examples","text":"<pre><code>$ curl --cacert /run/secrets/kubernetes.io/serviceaccount/ca.crt \\\n  --header \"Authorization: Bearer $(cat /run/secrets/kubernetes.io/serviceaccount/token)\" \\\n  https://kubernetes.default.svc.cluster.local/version\n\n{\n  \"major\": \"1\",\n  \"minor\": \"16+\",\n  \"gitVersion\": \"v1.16.2\",\n  \"gitCommit\": \"4320e48\",\n  \"gitTreeState\": \"clean\",\n  \"buildDate\": \"2020-01-21T19:50:59Z\",\n  \"goVersion\": \"go1.12.12\",\n  \"compiler\": \"gc\",\n  \"platform\": \"linux/amd64\"\n}\n</code></pre>","tags":["client","kubectl","oc"]},{"location":"cluster-configuration/","title":"Cluster configuration","text":""},{"location":"cluster-configuration/#content","title":"Content","text":"<ul> <li> <p>Authentication</p> </li> <li> <p>Cluster entitlement</p> </li> <li> <p>External DNS</p> </li> <li> <p>Kernel Module</p> </li> <li> <p>Storage</p> </li> <li> <p>NTP</p> </li> <li> <p>Monitoring</p> </li> <li> <p>Logging</p> </li> <li> <p>Image Registry</p> </li> <li> <p>MachineConfig</p> </li> <li> <p>MachineSets</p> </li> <li> <p>Cluster autoscaler</p> </li> </ul>"},{"location":"cluster-configuration/cluster-autoscaling/","title":"Cluster autoscaling","text":"<p>Documentation: Applying autoscaling to an OpenShift Container Platform cluster</p>","tags":["autoscaling","autoscaler"]},{"location":"cluster-configuration/cluster-autoscaling/#machineautoscaler-resource-definition","title":"MachineAutoscaler resource definition","text":"<pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: \"autoscaling.openshift.io/v1beta1\"\nkind: \"MachineAutoscaler\"\nmetadata:\n  name: \"vmw-mbqt7-worker\"\n  namespace: \"openshift-machine-api\"\nspec:\n  minReplicas: 3\n  maxReplicas: 6\n  scaleTargetRef:\n    apiVersion: machine.openshift.io/v1beta1\n    kind: MachineSet\n    name: vmw-mbqt7-worker\nEOF\n</code></pre>","tags":["autoscaling","autoscaler"]},{"location":"cluster-configuration/cluster-autoscaling/#clusterautoscaler-resource-definition","title":"ClusterAutoscaler resource definition","text":"<pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: \"autoscaling.openshift.io/v1\"\nkind: \"ClusterAutoscaler\"\nmetadata:\n  name: \"default\"\nspec:\n  podPriorityThreshold: -10\n  resourceLimits:\n    maxNodesTotal: 10\n    cores:\n      min: 8\n      max: 128\n    memory:\n      min: 4\n      max: 256\n  scaleDown:\n    enabled: true\n    delayAfterAdd: 10m\n    delayAfterDelete: 5m\n    delayAfterFailure: 30s\n    unneededTime: 60s\nEOF\n</code></pre>","tags":["autoscaling","autoscaler"]},{"location":"cluster-configuration/cluster-autoscaling/#start-resources","title":"Start resources","text":"<pre><code>oc new-project nginx\noc new-app nginx-example -p NGINX_VERSION=1.20-ubi8\n\n$ oc get deploymentconfig\nNAME            REVISION   DESIRED   CURRENT   TRIGGERED BY\nnginx-example   0          1         0         config,image(nginx-example:latest)\n\n\n$ oc patch dc/nginx-example \\\n    --type='json' \\\n    --patch='[\n        {\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/resources\", \"value\": {\"limits\":{\"memory\":\"1Gi\",\"cpu\": 1},\"requests\":{\"memory\":\"1Gi\",\"cpu\": 1}}}\n    ]'\n\n$ oc get nodes\nNAME                     STATUS   ROLES    AGE   VERSION\nvmw-mbqt7-master-0       Ready    master   25d   v1.19.0+9f84db3\nvmw-mbqt7-master-1       Ready    master   25d   v1.19.0+9f84db3\nvmw-mbqt7-master-2       Ready    master   25d   v1.19.0+9f84db3\nvmw-mbqt7-worker-dsstf   Ready    worker   25d   v1.19.0+9f84db3\nvmw-mbqt7-worker-plvzh   Ready    worker   25d   v1.19.0+9f84db3\nvmw-mbqt7-worker-s8p6t   Ready    worker   25d   v1.19.0+9f84db3\n$ oc get machines -n openshift-machine-api\nNAME                     PHASE     TYPE   REGION   ZONE   AGE\nvmw-mbqt7-master-0       Running                          25d\nvmw-mbqt7-master-1       Running                          25d\nvmw-mbqt7-master-2       Running                          25d\nvmw-mbqt7-worker-dsstf   Running                          25d\nvmw-mbqt7-worker-plvzh   Running                          25d\nvmw-mbqt7-worker-s8p6t   Running                          25d\n\n$ oc scale dc/nginx-example --replicas=100\ndeploymentconfig.apps.openshift.io/nginx-example scaled\n\n# After a while:\n\n$ oc get nodes\nNAME                     STATUS   ROLES    AGE   VERSION\nvmw-mbqt7-master-0       Ready    master   25d   v1.19.0+9f84db3\nvmw-mbqt7-master-1       Ready    master   25d   v1.19.0+9f84db3\nvmw-mbqt7-master-2       Ready    master   25d   v1.19.0+9f84db3\nvmw-mbqt7-worker-dsstf   Ready    worker   25d   v1.19.0+9f84db3\nvmw-mbqt7-worker-plvzh   Ready    worker   25d   v1.19.0+9f84db3\nvmw-mbqt7-worker-s8p6t   Ready    worker   25d   v1.19.0+9f84db3\n$ oc get machines\nNAME                     PHASE          TYPE   REGION   ZONE   AGE\nvmw-mbqt7-master-0       Running                               25d\nvmw-mbqt7-master-1       Running                               25d\nvmw-mbqt7-master-2       Running                               25d\nvmw-mbqt7-worker-2sgmh   Provisioning                          21s\nvmw-mbqt7-worker-dsstf   Running                               25d\nvmw-mbqt7-worker-plvzh   Running                               25d\nvmw-mbqt7-worker-s6cr2   Provisioning                          21s\nvmw-mbqt7-worker-s8p6t   Running                               25d\nvmw-mbqt7-worker-tmx82   Provisioning                          21s\n$ oc get pods -n nginx | wc -l\n     104\n\n\n# Some coffees later:\n\n$ oc get machines,nodes,machineset\nNAME                                                  PHASE     TYPE   REGION   ZONE   AGE\nmachine.machine.openshift.io/vmw-mbqt7-master-0       Running                          25d\nmachine.machine.openshift.io/vmw-mbqt7-master-1       Running                          25d\nmachine.machine.openshift.io/vmw-mbqt7-master-2       Running                          25d\nmachine.machine.openshift.io/vmw-mbqt7-worker-dsstf   Running                          25d\nmachine.machine.openshift.io/vmw-mbqt7-worker-plvzh   Running                          25d\nmachine.machine.openshift.io/vmw-mbqt7-worker-s8p6t   Running                          25d\nmachine.machine.openshift.io/vmw-mbqt7-worker-sxr6m   Running                          8m42s\nmachine.machine.openshift.io/vmw-mbqt7-worker-t7tk8   Running                          8m43s\nmachine.machine.openshift.io/vmw-mbqt7-worker-xj65l   Running                          8m50s\n\nNAME                          STATUS   ROLES    AGE    VERSION\nnode/vmw-mbqt7-master-0       Ready    master   25d    v1.19.0+9f84db3\nnode/vmw-mbqt7-master-1       Ready    master   25d    v1.19.0+9f84db3\nnode/vmw-mbqt7-master-2       Ready    master   25d    v1.19.0+9f84db3\nnode/vmw-mbqt7-worker-dsstf   Ready    worker   25d    v1.19.0+9f84db3\nnode/vmw-mbqt7-worker-plvzh   Ready    worker   25d    v1.19.0+9f84db3\nnode/vmw-mbqt7-worker-s8p6t   Ready    worker   25d    v1.19.0+9f84db3\nnode/vmw-mbqt7-worker-sxr6m   Ready    worker   102s   v1.19.0+9f84db3\nnode/vmw-mbqt7-worker-t7tk8   Ready    worker   94s    v1.19.0+9f84db3\nnode/vmw-mbqt7-worker-xj65l   Ready    worker   85s    v1.19.0+9f84db3\n\nNAME                                               DESIRED   CURRENT   READY   AVAILABLE   AGE\nmachineset.machine.openshift.io/vmw-mbqt7-worker   6         6         6       6           25d\n</code></pre>","tags":["autoscaling","autoscaler"]},{"location":"cluster-configuration/image-registry/","title":"Image Registry","text":""},{"location":"cluster-configuration/image-registry/#use-readwriteonce-volumes-new-in-44","title":"Use ReadWriteOnce volumes - new in 4.4","text":"<p>Note</p> <p>Only available since OpenShift version 4.4.0</p> <p>Warning</p> <p>By using a ReadWriteOnce volume you have to</p> <ul> <li> <p>Change the Rollout Strategy from rolling to recreate and</p> </li> <li> <p>its only supported to have exactly one replica.</p> </li> </ul> <p>**That means, during a cluster or image-registry upgrade, your internal registry has downtime between stopping the old pod and starting the new pod! **</p> <p>Documentation bug for official documentation: https://bugzilla.redhat.com/show_bug.cgi?id=1826292</p>"},{"location":"cluster-configuration/image-registry/#on-vsphere-with-cloud-provider-integration","title":"On vSphere with cloud provider integration","text":""},{"location":"cluster-configuration/image-registry/#create-pvc","title":"Create PVC","text":"<pre><code>oc create -f - &lt;&lt;EOF\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: image-registry-pvc\n  namespace: openshift-image-registry\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 100Gi\n  storageClassName: thin\nEOF\n</code></pre>"},{"location":"cluster-configuration/image-registry/#patch-registry-operator-crd","title":"Patch registry operator crd","text":"<pre><code>oc patch configs.imageregistry.operator.openshift.io/cluster \\\n    --type='json' \\\n    --patch='[\n        {\"op\": \"replace\", \"path\": \"/spec/managementState\", \"value\": \"Managed\"},\n        {\"op\": \"replace\", \"path\": \"/spec/rolloutStrategy\", \"value\": \"Recreate\"},\n        {\"op\": \"replace\", \"path\": \"/spec/replicas\", \"value\": 1},\n        {\"op\": \"replace\", \"path\": \"/spec/storage\", \"value\": {\"pvc\":{\"claim\": \"image-registry-pvc\" }}}\n    ]'\n</code></pre>"},{"location":"cluster-configuration/image-registry/#exposing-the-registry","title":"Exposing the registry","text":"<p>Documentation: Exposing the registry </p>"},{"location":"cluster-configuration/image-registry/#discover-exposed-registry-work-in-progress-","title":"Discover exposed registry - Work in progress -","text":"<p>Note</p> <p>This is work in progress</p> <pre><code>export REGISTRY=$(oc get route default-route -n openshift-image-registry --template='{{ .spec.host }}')\nexport REGISTRY_TOKEN=$(oc whoami -t)\n\npodman login -u $(oc whoami) -p $REGISTRY_TOKEN --tls-verify=false $HOST\n\n# List all images\ncurl -s -H \"Authorization: Bearer $REGISTRY_TOKEN\" \\\n    https://$REGISTRY/v2/_catalog\n\n# List tages of an image\nexport IMAGE=openshift/python\ncurl -s -H \"Authorization: Bearer $REGISTRY_TOKEN\" \\\n    https://$REGISTRY/v2/$IMAGE/tags/list\n\n# Get sha/digest\nexport TAG=latest\ncurl -s -H \"Authorization: Bearer $REGISTRY_TOKEN\"  \\\n    https://$REGISTRY/v2/$IMAGE/manifests/$TAG\n</code></pre>"},{"location":"cluster-configuration/image-registry/#setup-non-aws-s3-storage-backend","title":"Setup non AWS S3 storage backend","text":""},{"location":"cluster-configuration/image-registry/#deploy-minio-via-helm3","title":"Deploy min.io via helm3","text":"<pre><code>helm repo add stable https://kubernetes-charts.storage.googleapis.com\n# https://github.com/helm/charts/tree/master/stable/minio#configuration\nhelm install minio stable/minio \\\n  --set persistence.storageClass=managed-premium \\\n  --set securityContext.runAsUser=$( oc get project $(oc project  -q) -ojsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.uid-range}' | cut -f1 -d '/' ) \\\n  --set securityContext.fsGroup=$( oc get project $(oc project  -q) -ojsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | cut -f1 -d '/' ) \\\n  --set ingress.enabled=true \\\n  --set ingress.hosts={minio-$(oc project  -q).$( oc get ingresses.config.openshift.io/cluster -o jsonpath=\"{.spec.domain}\" )}\n\n# helm uninstall minio\n</code></pre>"},{"location":"cluster-configuration/image-registry/#configure-image-registry","title":"Configure image registry","text":"<pre><code>minio-service.minio.svc.cluster.local\n\noc create secret generic image-registry-private-configuration-user \\\n    --from-literal=REGISTRY_STORAGE_S3_ACCESSKEY=minio \\\n    --from-literal=REGISTRY_STORAGE_S3_SECRETKEY=minio123 \\\n    --namespace openshift-image-registry\n\n\noc patch configs.imageregistry.operator.openshift.io/cluster \\\n    --type='json' \\\n    --patch='[\n        {\"op\": \"remove\", \"path\": \"/spec/storage\" },\n        {\"op\": \"add\", \"path\": \"/spec/storage\", \"value\": {\"s3\":{\"bucket\": \"minio-service\", \"regionEndpoint\": \"minio.svc.cluster.local:9000\", \"encrypt\": false, \"region\": \"dummyregion\"}}}\n    ]'\n\noc describe configs.imageregistry.operator.openshift.io/cluster\n[...snipped...]\nName:         cluster\nAPI Version:  imageregistry.operator.openshift.io/v1\nKind:         Config\n[...snipped...]\nSpec:\n[...snipped...\n  Storage:\n    s3:\n      Bucket:           minio-service\n      Encrypt:          false\n      Region:           dummyregionf\n      Region Endpoint:  minio.svc.cluster.local:9000\nStatus:\n  Conditions:\n    Last Transition Time:  2020-02-26T21:04:38Z\n    Message:               The registry is ready\n    Reason:                Ready\n    Status:                True\n    Type:                  Available\n    Last Transition Time:  2020-02-29T20:21:55Z\n    Message:               Unable to apply resources: unable to sync storage configuration: RequestError: send request failed\ncaused by: Head https://minio-service.minio.svc.cluster.local:9000/: dial tcp 172.30.202.88:9000: connect: connection refused\n    Reason:                Error\n    Status:                True\n[...snipped...]\n</code></pre>"},{"location":"cluster-configuration/ntp/","title":"NTP Server","text":""},{"location":"cluster-configuration/ntp/#usefull-chrony-commands","title":"Usefull chrony commands","text":"<pre><code>chronyc -a makestep\nchronyc tracking\nchronyc sources\n</code></pre>"},{"location":"cluster-configuration/ntp/#machineconfig","title":"MachineConfig","text":"<p>Warning</p> <p>If you apply the machine config, all nodes will reboot one after the other.</p> <pre><code>chronybase64=$(cat &lt;&lt; EOF | base64 -w 0\nserver &lt;NTPSERVER&gt; iburst\ndriftfile /var/lib/chrony/drift\nmakestep 1.0 3\nrtcsync\nkeyfile /etc/chrony.keys\nleapsectz right/UTC\nlogdir /var/log/chrony\nEOF\n)\n\noc apply -f - &lt;&lt; EOF\napiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfig\nmetadata:\n  labels:\n    machineconfiguration.openshift.io/role: worker\n  name: 50-worker-chrony\nspec:\n  config:\n    ignition:\n      version: 3.1.0\n    storage:\n      files:\n      - contents:\n          source: data:text/plain;charset=utf-8;base64,${chronybase64}\n        filesystem: root\n        mode: 0644\n        path: /etc/chrony.conf\nEOF\n\n\noc apply -f - &lt;&lt; EOF\napiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfig\nmetadata:\n  labels:\n    machineconfiguration.openshift.io/role: master\n  name: 50-master-chrony\nspec:\n  config:\n    ignition:\n      version: 3.1.0\n    storage:\n      files:\n      - contents:\n          source: data:text/plain;charset=utf-8;base64,${chronybase64}\n        filesystem: root\n        mode: 0644\n        path: /etc/chrony.conf\nEOF\n</code></pre>"},{"location":"cluster-configuration/MachineSets/debugging/","title":"MachineSets Debugging","text":""},{"location":"cluster-configuration/MachineSets/debugging/#check-config-daemon-pod-log","title":"Check config-daemon pod log","text":"<pre><code>$ oc get pods -o wide -n openshift-machine-config-operator\nNAME                                         READY   STATUS    RESTARTS   AGE    IP              NODE        NOMINATED NODE   READINESS GATES\netcd-quorum-guard-6ffd665dfd-jjlnm           1/1     Running   0          7d5h   192.168.55.10   master-0    &lt;none&gt;           &lt;none&gt;\netcd-quorum-guard-6ffd665dfd-nrt2q           1/1     Running   0          7d5h   192.168.55.12   master-2    &lt;none&gt;           &lt;none&gt;\netcd-quorum-guard-6ffd665dfd-nwn6w           1/1     Running   0          7d5h   192.168.55.11   master-1    &lt;none&gt;           &lt;none&gt;\nmachine-config-controller-6fdf949978-b6z2z   1/1     Running   1          7d5h   10.129.0.3      master-1    &lt;none&gt;           &lt;none&gt;\nmachine-config-daemon-c6c8d                  2/2     Running   0          7d5h   192.168.55.13   compute-0   &lt;none&gt;           &lt;none&gt;\nmachine-config-daemon-fphbh                  2/2     Running   0          7d5h   192.168.55.10   master-0    &lt;none&gt;           &lt;none&gt;\nmachine-config-daemon-pvvwh                  2/2     Running   0          7d5h   192.168.55.14   compute-1   &lt;none&gt;           &lt;none&gt;\nmachine-config-daemon-xmd9v                  2/2     Running   0          7d5h   192.168.55.11   master-1    &lt;none&gt;           &lt;none&gt;\nmachine-config-daemon-zlcm2                  2/2     Running   0          7d5h   192.168.55.15   compute-2   &lt;none&gt;           &lt;none&gt;\nmachine-config-daemon-zslvq                  2/2     Running   0          7d5h   192.168.55.12   master-2    &lt;none&gt;           &lt;none&gt;\nmachine-config-operator-94994fb57-z775v      1/1     Running   1          7d5h   10.128.0.10     master-0    &lt;none&gt;           &lt;none&gt;\nmachine-config-server-hrt28                  1/1     Running   0          7d5h   192.168.55.12   master-2    &lt;none&gt;           &lt;none&gt;\nmachine-config-server-nvtzs                  1/1     Running   0          7d5h   192.168.55.11   master-1    &lt;none&gt;           &lt;none&gt;\nmachine-config-server-zk2fc                  1/1     Running   0          7d5h   192.168.55.10   master-0    &lt;none&gt;           &lt;none&gt;\n\n$ oc logs -n openshift-machine-config-operator machine-config-daemon-zlcm2 -c machine-config-daemon\nI0722 08:59:47.489287    3820 start.go:74] Version: v4.4.0-202005180840-dirty (480accd5d4f631d34e560aa5c8a3dfab0c7bbe27)\nI0722 08:59:47.492281    3820 start.go:84] Calling chroot(\"/rootfs\")\nI0722 08:59:47.492553    3820 rpm-ostree.go:366] Running captured: rpm-ostree status --json\nI0722 08:59:47.976562    3820 daemon.go:209] Booted osImageURL: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b397960b7cc14c2e2603111b7385c6e8e4b0f683f9873cd9252a789175e5c4e1 (44.81.202005180831-0)\nI0722 08:59:47.980187    3820 metrics.go:106] Registering Prometheus metrics\nI0722 08:59:47.980274    3820 metrics.go:111] Starting metrics listener on 127.0.0.1:8797\nI0722 08:59:47.980510    3820 update.go:1291] Starting to manage node: compute-2\nI0722 08:59:47.988656    3820 rpm-ostree.go:366] Running captured: rpm-ostree status\nI0722 08:59:48.056755    3820 daemon.go:778] State: idle\nAutomaticUpdates: disabled\nDeployments:\n* pivot://quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b397960b7cc14c2e2603111b7385c6e8e4b0f683f9873cd9252a789175e5c4e1\n              CustomOrigin: Managed by machine-config-operator\n                   Version: 44.81.202005180831-0 (2020-05-18T08:38:30Z)\n\n  ostree://2062bce64e4932160feb58ce4976a885172d3f1017dc01f09177504bd55e035b\n                   Version: 44.81.202004260825-0 (2020-04-26T08:30:26Z)\nI0722 08:59:48.056786    3820 rpm-ostree.go:366] Running captured: journalctl --list-boots\nI0722 08:59:48.067098    3820 daemon.go:785] journalctl --list-boots:\n-1 e4e923bf2a394325a93d1f2fe662580a Wed 2020-07-22 08:44:48 UTC\u2014Wed 2020-07-22 08:57:35 UTC\n 0 aa38c334768a48be87af8e16941bc34a Wed 2020-07-22 08:57:44 UTC\u2014Wed 2020-07-22 08:59:48 UTC\nI0722 08:59:48.067127    3820 daemon.go:528] Starting MachineConfigDaemon\nI0722 08:59:48.067270    3820 daemon.go:535] Enabling Kubelet Healthz Monitor\nI0722 08:59:53.939985    3820 node.go:24] No machineconfiguration.openshift.io/currentConfig annotation on node compute-2: map[volumes.kubernetes.io/controller-managed-attach-detach:true], in cluster bootstrap, loading initial node annotation from /etc/machine-config-daemon/node-annotations.json\nI0722 08:59:53.940549    3820 node.go:45] Setting initial node config: rendered-worker-fe5cc8b490c8a4803a6f8ec96611bda2\nI0722 08:59:53.971509    3820 daemon.go:696] In bootstrap mode\nI0722 08:59:53.971594    3820 daemon.go:724] Current+desired config: rendered-worker-fe5cc8b490c8a4803a6f8ec96611bda2\nI0722 08:59:53.979288    3820 daemon.go:917] No bootstrap pivot required; unlinking bootstrap node annotations\nI0722 08:59:53.979438    3820 daemon.go:955] Validating against pending config rendered-worker-fe5cc8b490c8a4803a6f8ec96611bda2\nI0722 08:59:53.981831    3820 daemon.go:971] Validated on-disk state\nI0722 08:59:54.016509    3820 daemon.go:1005] Completing pending config rendered-worker-fe5cc8b490c8a4803a6f8ec96611bda2\nI0722 08:59:54.017021    3820 update.go:1291] completed update for config rendered-worker-fe5cc8b490c8a4803a6f8ec96611bda2\nI0722 08:59:54.021653    3820 daemon.go:1021] In desired config rendered-worker-fe5cc8b490c8a4803a6f8ec96611bda2\n</code></pre>"},{"location":"cluster-configuration/MachineSets/debugging/#in-case-of-a-cluster-upgrade-check-download-pod","title":"In case of a cluster upgrade: Check download pod","text":"<pre><code>$ oc get pods -n openshift-cluster-version\nNAME                                        READY   STATUS      RESTARTS   AGE\ncluster-version-operator-5c97586d4f-9hqwn   1/1     Running     0          2m25s\nversion-4.4.13-pwgd4-mhc2q                  0/1     Completed   0          2m59s\n</code></pre>"},{"location":"cluster-configuration/MachineSets/VMware-UPI/","title":"Create MachineSets on VMware UPI (integrated)","text":"","tags":["VMware","vSphere","UPI","integrated"]},{"location":"cluster-configuration/MachineSets/VMware-UPI/#resources","title":"Resources","text":"<ul> <li>KCS: How to create a MachineSet for VMware in OCP 4 UPI installations</li> <li>Internal gDoc: vSphere machinesets</li> </ul>","tags":["VMware","vSphere","UPI","integrated"]},{"location":"cluster-configuration/MachineSets/VMware-UPI/#create-vmware-tags-based-on-infrastructurename","title":"Create VMware Tags based on <code>infrastructureName</code>","text":"<p>Double check if tags are available or not at <code>Menu -&gt; Tags &amp; Custom Attributes</code> (VMware documentation)</p> <p>Here an example:</p> <ul> <li> <p>Get infrastructureName</p> <pre><code>[root@jump ~]# oc get infrastructure cluster -o jsonpath='{.status.infrastructureName}{\"\\n\"}'\ncluster-7f58w\n</code></pre> </li> <li> <p>Check if tag is already available</p> <p></p> </li> <li> <p>If not, create tag category and tag:</p> </li> <li>tag category   </li> <li>tag it self   </li> </ul>","tags":["VMware","vSphere","UPI","integrated"]},{"location":"cluster-configuration/MachineSets/VMware-UPI/#others","title":"Others","text":"<p>https://github.com/openshift/machine-config-operator/blob/master/templates/common/vsphere/files/vsphere-hostname.yaml</p>","tags":["VMware","vSphere","UPI","integrated"]},{"location":"cluster-configuration/authentication/","title":"Authentication","text":"<p>Gernerall Kubernetes authentication documentation:</p> <ul> <li>https://kubernetes.io/docs/reference/access-authn-authz/authentication/</li> </ul>","tags":["authentication"]},{"location":"cluster-configuration/authentication/activedirectory-ldap/","title":"ActiveDirectory / LDAP","text":""},{"location":"cluster-configuration/authentication/activedirectory-ldap/#setup-activedirectory-authentication","title":"Setup ActiveDirectory Authentication","text":"<p>Go to Administration -&gt; Cluster Settings -&gt; Global Configuration -&gt; OAuth -&gt; Add -&gt; LDAP</p> <p></p> <p>Note</p> <p>Important part is the URL!</p> Option Value Name Active Directory URL ldaps://domaincontroller/DC=demo,DC=openshift,DC=pub?sAMAccountName?sub Bind DN service-account Bind Password ********* Attributes **** ID sAMAccountName Preferred Username sAMAccountName Name cn Email mail"},{"location":"cluster-configuration/authentication/activedirectory-ldap/#user-filter-examples","title":"User filter examples","text":"<p><code>ldaps://domaincontroller/DC=demo,DC=openshift,DC=pub?sAMAccountName?sub?(&amp;(filter=asdfg)(!(adsf=asdf)))</code></p> <p>Filter:</p> <ol> <li>All user <code>objectclass=person</code></li> <li>Member of group (inclusive nested group) OpenShift-User</li> <li>Active AD Account</li> </ol> <pre><code>(&amp;\n    (memberOf:1.2.840.113556.1.4.1941:=CN=OpenShift-User,OU=Gruppen,DC=demo,DC=openshift,DC=pub)\n    (objectClass=person)\n    (!(userAccountControl:1.2.840.113556.1.4.803:=2))\n)\n</code></pre>"},{"location":"cluster-configuration/authentication/activedirectory-ldap/#group-sync","title":"Group Sync","text":"<p>Documentation: https://docs.openshift.com/container-platform/latest/authentication/ldap-syncing.html</p>"},{"location":"cluster-configuration/authentication/activedirectory-ldap/#create-ldap-sync-configuration-files","title":"Create ldap sync configuration files","text":""},{"location":"cluster-configuration/authentication/activedirectory-ldap/#ldap-syncyaml","title":"ldap-sync.yaml","text":"<pre><code>kind: LDAPSyncConfig\napiVersion: v1\nurl: ldaps://domaincontroller\nbindDN: service-account\nbindPassword: '********'\ninsecure: false\nca: /ldap-sync/ca.crt\ngroupUIDNameMapping:\n  \"CN=Basis Server Admins,OU=Groups,DC=demo,DC=openshift,DC=pub\": Basis-Server-Admins\naugmentedActiveDirectory:\n    groupsQuery:\n        derefAliases: never\n        pageSize: 0\n    groupUIDAttribute: dn\n    groupNameAttributes: [ cn ]\n    usersQuery:\n        baseDN: \"DC=demo,DC=openshift,DC=pub\"\n        scope: sub\n        derefAliases: never\n        filter: (objectclass=person)\n        pageSize: 0\n    userNameAttributes: [ sAMAccountName ]\n    groupMembershipAttributes: [ \"memberOf:1.2.840.113556.1.4.1941:\" ]\n</code></pre>"},{"location":"cluster-configuration/authentication/activedirectory-ldap/#whitelisttxt","title":"whitelist.txt","text":"<pre><code>CN=Basis Server Admins,OU=Groups,DC=demo,DC=openshift,DC=pub\nCN=OCP-Users,OU=Groups,DC=demo,DC=openshift,DC=pub\n</code></pre>"},{"location":"cluster-configuration/authentication/activedirectory-ldap/#cacrt","title":"ca.crt","text":"<pre><code>-----BEGIN CERTIFICATE-----\n.....\n-----END CERTIFICATE-----\n</code></pre>"},{"location":"cluster-configuration/authentication/activedirectory-ldap/#create-secret-with-all-ldap-sync-conf-files","title":"Create secret with all ldap sync conf files","text":"<pre><code>oc create secret generic ldap-sync \\\n    --from-file=ldap-sync.yaml=ldap-sync.yaml \\\n    --from-file=whitelist.txt=whitelist.txt \\\n    --from-file=ca.crt=ca.crt\n</code></pre>"},{"location":"cluster-configuration/authentication/activedirectory-ldap/#deploy-recular-sync-via-cronjobscheduledjob","title":"Deploy recular sync via CronJob/ScheduledJob","text":""},{"location":"cluster-configuration/authentication/activedirectory-ldap/#create-ldap-group-sync-cluster-role","title":"Create ldap-group-sync cluster role","text":"<p>via OC</p> <p><pre><code>oc create clusterrole ldap-group-sync \\\n    --verb=create,update,patch,delete,get,list \\\n    --resource=groups.user.openshift.io\n</code></pre> via YAML:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: ldap-group-sync\nrules:\n- apiGroups:\n  - user.openshift.io\n  resources:\n  - groups\n  verbs:\n  - create\n  - update\n  - patch\n  - delete\n  - get\n  - list\n</code></pre>"},{"location":"cluster-configuration/authentication/activedirectory-ldap/#create-project-service-account-and-cluster-role-binding","title":"Create project, service account and cluster-role-binding","text":"<pre><code>oc new-project ldap-sync\noc create sa ldap-sync\noc adm policy add-cluster-role-to-user ldap-group-sync \\\n    -z ldap-sync \\\n    -n ldap-sync\n</code></pre>"},{"location":"cluster-configuration/authentication/activedirectory-ldap/#create-cronjob","title":"Create CronJob","text":"<p>YAML</p> <pre><code>apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: ldap-group-sync\nspec:\n  # Format: https://en.wikipedia.org/wiki/Cron\n  schedule: '@hourly'\n  suspend: false\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          serviceAccountName: ldap-sync\n          restartPolicy: Never\n          containers:\n            - name: oc-cli\n              command:\n                - /bin/oc\n                - adm\n                - groups\n                - sync\n                - --whitelist=/ldap-sync/whitelist.txt\n                - --sync-config=/ldap-sync/ldap-sync.yaml\n                - --confirm\n              image: registry.redhat.io/openshift4/ose-cli\n              imagePullPolicy: Always\n              volumeMounts:\n              - mountPath: /ldap-sync/\n                name: config\n                readOnly: true\n          volumes:\n          - name: config\n            secret:\n              defaultMode: 420\n              secretName: ldap-sync\n</code></pre>"},{"location":"cluster-configuration/authentication/activedirectory-ldap/#how-to-debug-with-ldapsearch","title":"How to debug with ldapsearch","text":"<p>Install ldapsearch with <code>yum install openldap-clients.x86_64</code></p> <pre><code>$ ldapsearch -x -H ldap://domaincontroler -D service-account \\\n    -b \"DC=demo,DC=openshift.,DC=pub\" \\\n    -W '(sAMAccountName=rbohne)'\n</code></pre>"},{"location":"cluster-configuration/authentication/auth-basic-authentication/","title":"Basic Authentication (Remote)","text":"<p>https://docs.openshift.com/container-platform/3.3/install_config/configuring_authentication.html#BasicAuthPasswordIdentityProvider</p> <pre><code>oauthConfig:\n  ...\n  identityProviders:\n  - name: my_remote_basic_auth_provider\n    challenge: true\n    login: true\n    mappingMethod: claim\n    provider:\n      apiVersion: v1\n      kind: BasicAuthPasswordIdentityProvider\n      url: https://foobar.exmaple.com/basic.php\n      ca: /path/to/ca.file\n      certFile: /path/to/client.crt\n      keyFile: /path/to/client.key\n</code></pre> <p>basic.php <pre><code>&lt;?php\n    header('Content-Type: application/json');\nif (!isset($_SERVER['PHP_AUTH_USER'])) {\n    header('WWW-Authenticate: Basic realm=\"OpenShift Auth Server\"');\n    header('HTTP/1.0 401 Unauthorized');\n    echo '{\"error\":\"Error message\"}';\n    exit;\n} else {\n    if ($_SERVER['PHP_AUTH_USER'] == 'rbo' and $_SERVER['PHP_AUTH_PW'] == 'rbo' ){\n        echo '{\"sub\":\"rbo\", \"name\": \"Robert Bohne\", \"email\":\"Robert.Bohne@ConSol.de\"}';\n    }else{\n        header('HTTP/1.0 401 Unauthorized');\n        echo '{\"error\":\"Username &amp; passwort falsch.... \"}';\n    }\n}\n?&gt;\n</code></pre></p>"},{"location":"cluster-configuration/authentication/redhat-sso/","title":"Red Hat SSO - via Google","text":""},{"location":"cluster-configuration/authentication/redhat-sso/#official-documentation","title":"Official documentation","text":"<ul> <li>OpenShift 4 - latest</li> </ul>"},{"location":"cluster-configuration/authentication/redhat-sso/#setup-oauth-20-client-at-google","title":"Setup OAuth 2.0 client at Google","text":""},{"location":"cluster-configuration/authentication/redhat-sso/#visit-httpsconsoledevelopersgooglecomapiscredentials","title":"Visit: https://console.developers.google.com/apis/credentials","text":""},{"location":"cluster-configuration/authentication/redhat-sso/#create-oauth-client-id","title":"Create OAuth client ID","text":""},{"location":"cluster-configuration/authentication/redhat-sso/#copy-client-id-and-client-secret","title":"Copy Client ID and Client secret","text":"<p>Example Client ID and secret Client ID: <code>1079862778375-60es8d8ugvg3e54csnesgf94p6r4rc6s.apps.googleusercontent.com</code> Client Secret: <code>rJR1jmiUnk_ZWZFxqbMxhShn</code></p>"},{"location":"cluster-configuration/authentication/redhat-sso/#create-client-secret","title":"Create client secret","text":"<pre><code>oc create secret generic google-secret \\\n  --from-literal=clientSecret=rJR1jmiUnk_ZWZFxqbMxhShn \\\n  -n openshift-config\n</code></pre>"},{"location":"cluster-configuration/authentication/redhat-sso/#update-oauth-config","title":"Update OAuth Config","text":""},{"location":"cluster-configuration/authentication/redhat-sso/#oauthyaml","title":"oauth.yaml","text":"<pre><code>apiVersion: config.openshift.io/v1\nkind: OAuth\nmetadata:\n  name: cluster\nspec:\n  identityProviders:\n  - name: RedHatSSO\n    mappingMethod: claim\n    type: Google\n    google:\n      clientID: \"1079862778375-60es8d8ugvg3e54csnesgf94p6r4rc6s.apps.googleusercontent.com\"\n      clientSecret:\n        name: google-secret\n      hostedDomain: \"redhat.com\"\n</code></pre> <p>Apply: <code>oc apply -f oauth.yaml</code></p> <p>Optional Remove kubeadm account</p> <pre><code>oc delete secrets kubeadmin -n kube-system\n</code></pre> <p>Documentation</p> <p>Optional Remove self-provisioner</p> <pre><code>oc adm policy remove-cluster-role-from-group self-provisioner system:authenticated system:authenticated:oauth\n</code></pre> <p>Or the respective manifest:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"false\"\n  name: self-provisioners\nsubjects: null\n</code></pre>"},{"location":"cluster-configuration/authentication/token/","title":"Service Account authentication via Token","text":"<p>Official solutions (KCS):  * How to get the authentication token for an OpenShift Service account  * What are the automatically generated secrets for every service account?</p>"},{"location":"cluster-configuration/authentication/token/#create-service-account-sa","title":"Create service account (sa)","text":"<pre><code>oc create sa external-pipeline-user\n</code></pre>"},{"location":"cluster-configuration/authentication/token/#get-service-account-details","title":"Get service account details","text":"<pre><code>$ oc get serviceaccount/external-pipeline-user -o yaml\n\napiVersion: v1\nimagePullSecrets:\n- name: external-pipeline-user-dockercfg-2kjrl\nkind: ServiceAccount\nmetadata:\n  creationTimestamp: \"2020-10-12T13:56:11Z\"\n  name: external-pipeline-user\n  namespace: sa-test\n  resourceVersion: \"19995538\"\n  selfLink: /api/v1/namespaces/sa-test/serviceaccounts/external-pipeline-user\n  uid: 90f73aa4-f7c0-45ed-b99c-dec17d3fd864\nsecrets:\n- name: external-pipeline-user-token-cr7nq\n- name: external-pipeline-user-dockercfg-2kjrl\n\n$ oc describe serviceaccount/external-pipeline-user\nName:                external-pipeline-user\nNamespace:           sa-test\nLabels:              &lt;none&gt;\nAnnotations:         &lt;none&gt;\nImage pull secrets:  external-pipeline-user-dockercfg-2kjrl\nMountable secrets:   external-pipeline-user-token-cr7nq\n                     external-pipeline-user-dockercfg-2kjrl\nTokens:              external-pipeline-user-token-cr7nq\n                     external-pipeline-user-token-sdxck\nEvents:              &lt;none&gt;\n</code></pre>"},{"location":"cluster-configuration/authentication/token/#get-token","title":"Get Token","text":"<pre><code>TOKEN=$(oc serviceaccounts get-token external-pipeline-user)\n\n$ oc login --token=$TOKEN\nLogged into \"https://api.demo.openshift.pub:6443\" as \"system:serviceaccount:sa-test:external-pipeline-user\" using the token provided.\n\nYou don't have any projects. Contact your system administrator to request a project.\n$ oc whoami\nsystem:serviceaccount:sa-test:external-pipeline-user\n</code></pre>"},{"location":"cluster-configuration/authentication/token/#list-of-secrets","title":"List of secrets","text":"<pre><code>$ oc get secrets | grep external-pipeline-user\nexternal-pipeline-user-dockercfg-2kjrl   kubernetes.io/dockercfg               1      25m\nexternal-pipeline-user-token-cr7nq       kubernetes.io/service-account-token   4      25m\nexternal-pipeline-user-token-sdxck       kubernetes.io/service-account-token   4      25m\n</code></pre> <p>Both secrets</p> <ul> <li><code>external-pipeline-user-token-cr7nq</code></li> <li><code>external-pipeline-user-token-sdxck</code></li> </ul> <p>contains a valid token.</p> <p>The only differents is an annotation <code>kubernetes.io/created-by: openshift.io/create-dockercfg-secrets</code>.</p> <p>Token with the annotation is made for container image registry during the dockercfg secret creation. For details please check the source code of create_dockercfg_secrets.go.</p> <pre><code>$ oc describe secrets/external-pipeline-user-token-sdxck\nName:         external-pipeline-user-token-sdxck\nNamespace:    sa-test\nLabels:       &lt;none&gt;\nAnnotations:  kubernetes.io/created-by: openshift.io/create-dockercfg-secrets\n              kubernetes.io/service-account.name: external-pipeline-user\n              kubernetes.io/service-account.uid: 90f73aa4-f7c0-45ed-b99c-dec17d3fd864\n\nType:  kubernetes.io/service-account-token\n\nData\n====\ntoken:           eyJhbxxxxxxxxxxx\nca.crt:          8826 bytes\nnamespace:       7 bytes\nservice-ca.crt:  10039 bytes\n\n$ oc describe secrets/external-pipeline-user-token-cr7nq\nName:         external-pipeline-user-token-cr7nq\nNamespace:    sa-test\nLabels:       &lt;none&gt;\nAnnotations:  kubernetes.io/service-account.name: external-pipeline-user\n              kubernetes.io/service-account.uid: 90f73aa4-f7c0-45ed-b99c-dec17d3fd864\n\nType:  kubernetes.io/service-account-token\n\nData\n====\nservice-ca.crt:  10039 bytes\ntoken:           eyJhbxxxxxxxxxx\nca.crt:          8826 bytes\nnamespace:       7 bytes\n</code></pre>"},{"location":"cluster-configuration/authentication/token/#both-tokens-works-very-well","title":"Both tokens works very well","text":"<pre><code>$ TOKEN_cr7nq=$(oc get secret/external-pipeline-user-token-cr7nq -o jsonpath={.data.token} | base64 -d)\n$ TOKEN_sdxck=$(oc get secret/external-pipeline-user-token-sdxck -o jsonpath={.data.token} | base64 -d)\n\n$ oc login --token=$TOKEN_cr7nq\nLogged into \"https://api.demo.openshift.pub:6443\" as \"system:serviceaccount:sa-test:external-pipeline-user\" using the token provided.\n\n$ oc login --token=$TOKEN_sdxck\nLogged into \"https://api.demo.openshift.pub:6443\" as \"system:serviceaccount:sa-test:external-pipeline-user\" using the token provided.\n</code></pre>"},{"location":"cluster-configuration/authentication/token/#inspect-tokens","title":"Inspect tokens","text":"<ul> <li>Download jwt command line tool</li> </ul> <pre><code>$ oc get secrets | grep external-pipeline-user\nexternal-pipeline-user-dockercfg-2kjrl   kubernetes.io/dockercfg               1      25m\nexternal-pipeline-user-token-cr7nq       kubernetes.io/service-account-token   4      25m\nexternal-pipeline-user-token-sdxck       kubernetes.io/service-account-token   4      25m\n\n\n\n$ oc get secrets external-pipeline-user-token-cr7nq -o jsonpath='{.data.token}' | base64 --decode | jwt decode -\n\nToken header\n------------\n{\n  \"alg\": \"RS256\",\n  \"kid\": \"itDEzyYpqiEE5XVUFd8uIKPtPsZcFPU2QOhxRhwQqLM\"\n}\n\nToken claims\n------------\n{\n  \"iss\": \"kubernetes/serviceaccount\",\n  \"kubernetes.io/serviceaccount/namespace\": \"sa-test\",\n  \"kubernetes.io/serviceaccount/secret.name\": \"external-pipeline-user-token-cr7nq\",\n  \"kubernetes.io/serviceaccount/service-account.name\": \"external-pipeline-user\",\n  \"kubernetes.io/serviceaccount/service-account.uid\": \"90f73aa4-f7c0-45ed-b99c-dec17d3fd864\",\n  \"sub\": \"system:serviceaccount:sa-test:external-pipeline-user\"\n}\n\n$ oc get secrets external-pipeline-user-token-sdxck    -o jsonpath='{.data.token}' | base64 --decode | jwt decode -\n\nToken header\n------------\n{\n  \"alg\": \"RS256\",\n  \"kid\": \"itDEzyYpqiEE5XVUFd8uIKPtPsZcFPU2QOhxRhwQqLM\"\n}\n\nToken claims\n------------\n{\n  \"iss\": \"kubernetes/serviceaccount\",\n  \"kubernetes.io/serviceaccount/namespace\": \"sa-test\",\n  \"kubernetes.io/serviceaccount/secret.name\": \"external-pipeline-user-token-sdxck\",\n  \"kubernetes.io/serviceaccount/service-account.name\": \"external-pipeline-user\",\n  \"kubernetes.io/serviceaccount/service-account.uid\": \"90f73aa4-f7c0-45ed-b99c-dec17d3fd864\",\n  \"sub\": \"system:serviceaccount:sa-test:external-pipeline-user\"\n}\n</code></pre>"},{"location":"cluster-configuration/authentication/client-certificate/","title":"Client Certificate","text":"<p>Attention</p> <p>Please note before using client certificate authentication:</p> <ul> <li> <p>You can not revoke a client certificate, ones a client certificate is is compromised you can only restrict access using RBAC. Reistricting access using RBAC might not a solution. For example: you can add <code>system:cluster-admins</code> group to your client certificate. I don't know if your cluster survive if you remove all priviliges from this system group.</p> </li> <li> <p>After the certificate signing request is approved and deleted after a while. You don't know what client certificates are signed out there.</p> </li> <li> <p>Signed client certificate is valid for 365 days by default.</p> </li> </ul>","tags":["authentication","certificate","x509"]},{"location":"cluster-configuration/authentication/client-certificate/#create-a-certificate-signing-request","title":"Create a certificate signing request","text":"CommandExample output <pre><code>openssl req -new -newkey rsa:4096 -nodes -keyout rbohne.pem -out rbohne-csr.pem -subj \"/CN=rbohne/O=groupA/O=groupB\"\n</code></pre> <p>Since OpenSSLK 3.0 use <code>-noenc</code> instead of <code>-nodes</code></p> <pre><code>$ openssl req -new -newkey rsa:4096 -nodes -keyout rbohne.pem -out rbohne-csr.pem -subj \"/CN=rbohne/O=groupA/O=groupB\"\nGenerating a RSA private key\n...++++\n...............++++\nwriting new private key to 'rbohne.pem'\n-----\n</code></pre>","tags":["authentication","certificate","x509"]},{"location":"cluster-configuration/authentication/client-certificate/#sign-request","title":"Sign request","text":"CommandExample Output <p>Create CSR object</p> <pre><code>cat &lt;&lt;EOF | oc apply -f -\napiVersion: certificates.k8s.io/v1\nkind: CertificateSigningRequest\nmetadata:\n  name: client-cert-rbohne\nspec:\n  request: $(cat rbohne-csr.pem | base64 | tr -d '\\n')\n  signerName: kubernetes.io/kube-apiserver-client\n  usages:\n  - client auth\nEOF\n</code></pre> <p>Appropve CSR</p> <pre><code>oc adm certificate approve  client-cert-rbohne\n</code></pre> <p>Export Certificate</p> <pre><code>oc get csr/client-cert-rbohne -o jsonpath=\"{.status.certificate}\" | base64 -d &gt; rbohne-crt.pem\n</code></pre> <pre><code>$ cat &lt;&lt;EOF | oc apply -f -\napiVersion: certificates.k8s.io/v1\nkind: CertificateSigningRequest\nmetadata:\n  name: client-cert-rbohne\nspec:\n  request: $(cat rbohne-csr.pem | base64 | tr -d '\\n')\n  signerName: kubernetes.io/kube-apiserver-client\n  usages:\n  - client auth\nEOF\ncertificatesigningrequest.certificates.k8s.io/client-cert-rbohne created\n\n$ oc get csr\nNAME                 AGE   SIGNERNAME                            REQUESTOR      CONDITION\nclient-cert-rbohne   2s    kubernetes.io/kube-apiserver-client   system:admin   Pending\n\n$ oc adm certificate approve  client-cert-rbohne\ncertificatesigningrequest.certificates.k8s.io/client-cert-rbohne approved\n\n$ oc get csr\nNAME                 AGE   SIGNERNAME                            REQUESTOR      CONDITION\nclient-cert-rbohne   14s   kubernetes.io/kube-apiserver-client   system:admin   Approved,Issued\n\n$ oc get csr/client-cert-rbohne -o jsonpath=\"{.status.certificate}\" | base64 -d &gt; rbohne-crt.pem\n\n$ openssl x509 -in rbohne-crt.pem -noout -subject -issuer -dates\nsubject=O = groupA + O = groupB, CN = rbohne\nissuer=O = https://kubernetes.svc, CN = https://kubernetes.svc\nnotBefore=Dec 28 10:16:51 2021 GMT\nnotAfter=Dec 22 14:12:27 2022 GMT\n</code></pre>","tags":["authentication","certificate","x509"]},{"location":"cluster-configuration/authentication/client-certificate/#create-kubeconfig","title":"Create kubeconfig","text":"CommandExample output <pre><code>export KUBECONFIG=$(pwd)/kubeconfig\n\nkubectl config set-cluster microshift \\\n  --server=https://192.168.66.4:6443/ \\\n  --insecure-skip-tls-verify=true\n\nkubectl config set-credentials rbohne \\\n  --client-certificate=$( pwd )/rbohne-crt.pem \\\n  --client-key=$( pwd )/rbohne.pem \\\n  --embed-certs=true\n\nkubectl config set-context rbohne-at-microshift \\\n  --cluster=microshift \\\n  --user=rbohne \\\n  --namespace=default\n\nkubectl config use-context rbohne-at-microshift\n</code></pre> <pre><code>export KUBECONFIG=$(pwd)/kubeconfig\n\nkubectl config set-cluster microshift \\\n  --server=https://192.168.66.4:6443/ \\\n  --insecure-skip-tls-verify=true\n\nkubectl config set-credentials rbohne \\\n  --client-certificate=$( pwd )/rbohne-crt.pem \\\n  --client-key=$( pwd )/rbohne.pem \\\n  --embed-certs=true\n\n\nkubectl config set-context rbohne-at-microshift \\\n  --cluster=microshift \\\n  --user=rbohne \\\n  --namespace=default\n\nkubectl config use-context rbohne-at-microshift\nW1228 12:29:30.564154 1294981 loader.go:221] Config not found: /tmp/demo/kubeconfig\nW1228 12:29:30.564412 1294981 loader.go:221] Config not found: /tmp/demo/kubeconfig\nCluster \"microshift\" set.\nUser \"rbohne\" set.\nContext \"rbohne-at-microshift\" created.\nSwitched to context \"rbohne-at-microshift\".\n</code></pre>","tags":["authentication","certificate","x509"]},{"location":"cluster-configuration/build-load-kernel-module/","title":"Build and load Kernel Model","text":"<p>Note</p> <p>We need an entitlement to install kernel sources.</p>","tags":["kenrel"]},{"location":"cluster-configuration/build-load-kernel-module/#service-account-privileges","title":"Service account &amp; privileges","text":"OCbuild-load-kernel-module.daemonset.yaml <pre><code>oc apply -f https://examples.openshift.pub/pr-133/cluster-configuration/build-load-kernel-module/service-account-and-rbac.yaml.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: privileged\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: privileged\nrules:\n- apiGroups:\n  - security.openshift.io\n  resourceNames:\n  - privileged\n  resources:\n  - securitycontextconstraints\n  verbs:\n  - use\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: sa-to-privileged\nsubjects:\n  - kind: ServiceAccount\n    name: privileged\nroleRef:\n  kind: Role\n  name: privileged\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>","tags":["kenrel"]},{"location":"cluster-configuration/build-load-kernel-module/#entitlement","title":"Entitlement","text":"<pre><code>oc create secret generic etc-pki-entitlement \\\n  --from-file=entitlement.pem=./export/entitlement_certificates/xxx.pem\n  --from-file entitlement-key.pem=./export/entitlement_certificates/xxx.pem\n</code></pre>","tags":["kenrel"]},{"location":"cluster-configuration/build-load-kernel-module/#daemonset-deployment-build-runload","title":"DaemonSet / Deployment (Build &amp; Run/Load)","text":"build-load-kernel-module.daemonset.yamlOC <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nuc-led-kernel-module\nspec:\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      name: nuc-led-kernel-module\n  template:\n    metadata:\n      labels:\n        name: nuc-led-kernel-module\n    spec:\n      tolerations:\n        - operator: Exists\n      hostname: nuc-led-kernel-module\n      serviceAccount: privileged\n      serviceAccountName: privileged\n      volumes:\n      - name: etc-pki-entitlement\n        secret:\n          defaultMode: 420\n          secretName: etc-pki-entitlement\n      containers:\n        - name: rhel\n          image: registry.access.redhat.com/ubi8/s2i-base:latest\n          lifecycle:\n            preStop:\n              exec:\n                command:\n                  - /usr/bin/bash\n                  - -c\n                  - |\n                    set -euxo pipefail\n                    rmmod nuc_led\n          command:\n            - /usr/bin/bash\n            - -c\n            - |\n\n              set -euxo pipefail\n\n              rm /etc/rhsm-host\n\n              dnf install -y gcc-toolset-10-gcc kernel-devel \\\n                             kernel-headers kernel-modules \\\n                             elfutils-libelf-devel\n\n\n              git clone https://github.com/nomego/intel_nuc_led\n              cd intel_nuc_led\n\n              make\n              make install\n              modprobe nuc_led\n\n              while true; do\n                date\n                cat  /proc/acpi/nuc_led\n                sleep 60\n              done\n\n              sleep infinity\n\n          securityContext:\n            privileged: true\n            runAsUser: 0\n\n          volumeMounts:\n          - mountPath: /etc/pki/entitlement\n            name: etc-pki-entitlement\n</code></pre> <pre><code>oc apply -f https://examples.openshift.pub/pr-133/cluster-configuration/build-load-kernel-module/daemonset.yaml\n</code></pre>","tags":["kenrel"]},{"location":"cluster-configuration/external-dns/","title":"External DNS with FreeIPA (RFC2136)","text":"<p>Sadly the External DNS Operator do not support RFC2136. Let's use the upstream one.</p> <p>Thanks to astrid for the starting point.</p>","tags":["dns","freeipa"]},{"location":"cluster-configuration/external-dns/#prepare-ipa-dns-zone","title":"Prepare IPA DNS Zone","text":"","tags":["dns","freeipa"]},{"location":"cluster-configuration/external-dns/#generate-a-tsig-key-and-register-it","title":"Generate a TSIG key and register it","text":"<pre><code>dnssec-keygen -a HMAC-SHA512 -b 512 -n HOST openshift-external-dns\n</code></pre> <pre><code>$ cat Kopenshift-external-dns.+165+16478.private\nPrivate-key-format: v1.3\nAlgorithm: 165 (HMAC_SHA512)\nKey: c3LyD11u....xX6WA==\nBits: AAA=\nCreated: 20240205134832\nPublish: 20240205134832\nActivate: 20240205134832\n</code></pre>","tags":["dns","freeipa"]},{"location":"cluster-configuration/external-dns/#configure-the-key-at-ipa-server-and-all-replicas","title":"Configure the key at ipa server and all replicas","text":"<pre><code>$ cat /etc/named/ipa-ext.conf\n...\nkey \"openshift-external-dns\" {\n       algorithm hmac-sha512;\n       secret \"c3LyD11u....xX6WA==\";\n};\n</code></pre>","tags":["dns","freeipa"]},{"location":"cluster-configuration/external-dns/#allow-dns-updates-and-zone-transfer-for-the-key","title":"Allow DNS updates and zone transfer for the key","text":"<p>Select the zone you want to manage, in my example <code>.disco.local</code>:</p> <ul> <li>Enable <code>Dynamic update</code></li> <li>Add <code>grant openshift-external-dns subdomain disco.local ANY ;</code> to BIND update policy   Details about the policy configuration you can here</li> </ul> <p></p> <ul> <li> <p>Configure <code>Allow transfer</code> is not possible via WebUI. Because</p> ldap search example <p>At the ipa server</p> <pre><code># kinit admin\nPassword for admin@DISCO.LOCAL:\n# ldapsearch idnsname=disco.local.  dn idnsAllowTransfer\nSASL/GSSAPI authentication started\nSASL username: admin@disco.local\nSASL SSF: 256\nSASL data security layer installed.\n# extended LDIF\n#\n# LDAPv3\n# base &lt;cn=dns,dc=disco,dc=local&gt; (default) with scope subtree\n# filter: idnsname=disco.local.\n# requesting: dn idnsAllowTransfer\n#\n\n# disco.local., dns, disco.local\ndn: idnsname=disco.local.,cn=dns,dc=disco,dc=local\nidnsAllowTransfer: none;\n\n# search result\nsearch: 4\nresult: 0 Success\n\n# numResponses: 2\n# numEntries: 1\n#\n</code></pre> <pre><code>kinit admin\n\nldapmodify -Y GSSAPI &lt;&lt; EOF\ndn: idnsname=coe.muc.redhat.com.,cn=dns,dc=disco,dc=local\nchangetype: modify\nreplace: idnsAllowTransfer\nidnsAllowTransfer: key openshift-external-dns;\n-\nEOF\n</code></pre> </li> </ul>","tags":["dns","freeipa"]},{"location":"cluster-configuration/external-dns/#deploy-external-dns","title":"Deploy External DNS","text":"<p>based on Configuring RFC2136 provider</p> Deployment <p>Create a secret with the tsig key <code>c3LyD11u....xX6WA==</code></p> <pre><code> oc create secret generic external-dns-rfc2136-tsig-secret \\\n    --from-literal=EXTERNAL_DNS_RFC2136_TSIG_SECRET=\"c3LyD11u....xX6WA==\"\n</code></pre> YAMLoc apply -k .... <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: external-dns\n  labels:\n    name: external-dns\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - services\n      - endpoints\n      - pods\n      - nodes\n    verbs:\n      - get\n      - watch\n      - list\n  - apiGroups:\n      - extensions\n      - networking.k8s.io\n    resources:\n      - ingresses\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - \"route.openshift.io\"\n    resources:\n      - routes\n    verbs:\n      - get\n      - list\n      - watch\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n  - kind: ServiceAccount\n    name: external-dns\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      nodeSelector:\n        node-role.kubernetes.io/master: \"\"\n      tolerations:\n        - effect: NoSchedule\n          key: node-role.kubernetes.io/master\n          operator: Exists\n      containers:\n        - name: external-dns\n          env:\n            - name: EXTERNAL_DNS_RFC2136_TSIG_SECRET\n              valueFrom:\n                secretKeyRef:\n                  name: external-dns-rfc2136-tsig-secret\n                  key: EXTERNAL_DNS_RFC2136_TSIG_SECRET\n          image: registry.k8s.io/external-dns/external-dns:v0.14.0\n          args:\n            - --registry=txt\n            - --txt-suffix=-%{record_type}-external-dns\n            - --txt-owner-id=cluster.disco.local\n            - --provider=rfc2136\n            - --rfc2136-host=dns01.disco.local\n            - --rfc2136-port=53\n            - --rfc2136-zone=disco.local\n            - --rfc2136-tsig-secret-alg=hmac-sha512\n            - --rfc2136-tsig-keyname=openshift-external-dns\n            - --rfc2136-tsig-axfr\n            - --source=service\n            - --domain-filter=disco.local\n</code></pre> <pre><code>oc apply -k https://github.com/openshift-examples/web/tree/main/content/cluster-configuration/external-dns/deployment/\n</code></pre> <p>Check the logs of the external-dns pod</p> <pre><code>oc logs -n infra-external-dns deployment/external-dns\n</code></pre>","tags":["dns","freeipa"]},{"location":"cluster-configuration/external-dns/#example-deployment","title":"Example deployment","text":"<ul> <li>Required MetalLB or support of service type LoadBalancer.</li> </ul> <pre><code>oc new-project external-dns-demo\n\noc apply -f https://examples.openshift.pub/pr-133/cluster-configuration/external-dns/../../deploy/deployment-simple-nginx.yaml\n\noc patch service/simple-nginx --type merge -p '{\"spec\":{\"type\":\"LoadBalancer\"}}'\n\noc annotate service/simple-nginx external-dns.alpha.kubernetes.io/hostname='external-dns-demo.disco.local'\noc annotate service/simple-nginx external-dns.alpha.kubernetes.io/ttl='60'\n</code></pre>","tags":["dns","freeipa"]},{"location":"cluster-configuration/full-cluster-entitlement/","title":"How to get and configure full cluster entitlement","text":"","tags":["entitlement","GPU"]},{"location":"cluster-configuration/full-cluster-entitlement/#subscribe-the-cluster-and-fetch-the-entitlement","title":"Subscribe the cluster and fetch the entitlement","text":"","tags":["entitlement","GPU"]},{"location":"cluster-configuration/full-cluster-entitlement/#edit-subscription-settings","title":"Edit subscription settings","text":"<p>Select the cluster at cloud.redhat.com and edit subscription settings.</p> <p> </p>","tags":["entitlement","GPU"]},{"location":"cluster-configuration/full-cluster-entitlement/#attach-subscription-to-cluster","title":"Attach subscription to cluster","text":"<p>Go to access.redhat.com and select the virtual System:</p> <p></p> <p>Attach your subscription:</p> <p></p> <p>Download the entitlement</p> <p></p>","tags":["entitlement","GPU"]},{"location":"cluster-configuration/full-cluster-entitlement/#install-the-entitlement-to-your-cluster","title":"Install the entitlement to your cluster","text":"<p>Official solution: How to use entitled image builds on Red Hat OpenShift Container Platform 4.x cluster ?</p> <pre><code>$ unzip 7f6df4a3-a693-45ca-9f29-81f0328d4f5e_certificates.zip\nArchive:  7f6df4a3-a693-45ca-9f29-81f0328d4f5e_certificates.zip\nsigned Candlepin export for 7f6df4a3-a693-45ca-9f29-81f0328d4f5e\n  inflating: consumer_export.zip\n  inflating: signature\n$ unzip consumer_export.zip\nArchive:  consumer_export.zip\nCandlepin export for 7f6df4a3-a693-45ca-9f29-81f0328d4f5e\n  inflating: export/meta.json\n  inflating: export/entitlement_certificates/4562443306095312060.pem\n$ cp export/entitlement_certificates/4562443306095312060.pem nvidia.pem\n$ curl -L -O https://raw.githubusercontent.com/openshift-psap/blog-artifacts/master/how-to-use-entitled-builds-with-ubi/0003-cluster-wide-machineconfigs.yaml.template\n$ sed -i -f - 0003-cluster-wide-machineconfigs.yaml.template &lt;&lt; EOF\ns/BASE64_ENCODED_PEM_FILE/$(base64 -w0 nvidia.pem)/g\nEOF\n$ oc apply -f 0003-cluster-wide-machineconfigs.yaml.template\nmachineconfig.machineconfiguration.openshift.io/50-rhsm-conf created\nmachineconfig.machineconfiguration.openshift.io/50-entitlement-pem created\nmachineconfig.machineconfiguration.openshift.io/50-entitlement-key-pem created\n</code></pre>","tags":["entitlement","GPU"]},{"location":"cluster-configuration/full-cluster-entitlement/#watch-the-entitlement-rollout","title":"Watch the entitlement rollout","text":"<p>Copy&amp;Paste the command: <pre><code>watch 'oc get nodes -o custom-columns=NAME:.metadata.name,Status:.metadata.annotations.\"machineconfiguration\\.openshift\\.io/state\",currentConfig:.metadata.annotations.\"machineconfiguration\\.openshift\\.io/currentConfig\",desiredConfig:.metadata.annotations.\"machineconfiguration\\.openshift\\.io/desiredConfig\"'\n</code></pre></p> <p>Example output: <pre><code>$ oc get nodes -o custom-columns=NAME:.metadata.name,Status:.metadata.annotations.\"machineconfiguration\\.openshift\\.io/state\",currentConfig:.metadata.annotations.\"machineconfiguration\\.openshift\\.io/currentConfig\",desiredConfig:.metadata.annotations.\"machineconfiguration\\.openshift\\.io/desiredConfig\"\nNAME        Status    currentConfig                                      desiredConfig\ncompute-0   Done      rendered-worker-15dd6aa49dab77cc95f392b99ecca248   rendered-worker-15dd6aa49dab77cc95f392b99ecca248\ncompute-1   Done      rendered-worker-15dd6aa49dab77cc95f392b99ecca248   rendered-worker-15dd6aa49dab77cc95f392b99ecca248\ncompute-2   Working   rendered-worker-15dd6aa49dab77cc95f392b99ecca248   rendered-worker-c486cfec526faa411a36efa31b426237\nmaster-0    Done      rendered-master-26b61ef596395a9b098c4c1e0e5255cb   rendered-master-26b61ef596395a9b098c4c1e0e5255cb\nmaster-1    Done      rendered-master-26b61ef596395a9b098c4c1e0e5255cb   rendered-master-26b61ef596395a9b098c4c1e0e5255cb\nmaster-2    Done      rendered-master-26b61ef596395a9b098c4c1e0e5255cb   rendered-master-26b61ef596395a9b098c4c1e0e5255cb\n</code></pre></p> <p>You are done if all nodes have the new config, or the machineconfigpool is done: <pre><code>$ oc get machineconfigpool/worker\nNAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE\nworker   rendered-worker-15dd6aa49dab77cc95f392b99ecca248   True     False       False      3              0                   0                     0                      3h47m\n</code></pre></p>","tags":["entitlement","GPU"]},{"location":"cluster-configuration/full-cluster-entitlement/#how-to-test-it","title":"How to test it","text":"<pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: rhel\nspec:\n  containers:\n    - name: rhel\n      image: registry.access.redhat.com/ubi8/ubi:latest\n      command:\n        - /bin/sh\n        - -c\n        - |\n          dnf search kernel-headers\n  restartPolicy: Never\nEOF\n</code></pre> <p>Output of entitled cluster:</p> <pre><code>$ oc logs rhel\nUpdating Subscription Management repositories.\nUnable to read consumer identity\nSubscription Manager is operating in container mode.\nRed Hat Enterprise Linux 8 for x86_64 - BaseOS  2.5 MB/s |  22 MB     00:09\nRed Hat Enterprise Linux 8 for x86_64 - AppStre  15 MB/s |  19 MB     00:01\nRed Hat Universal Base Image 8 (RPMs) - BaseOS  2.8 MB/s | 772 kB     00:00\nRed Hat Universal Base Image 8 (RPMs) - AppStre  13 MB/s | 4.0 MB     00:00\nRed Hat Universal Base Image 8 (RPMs) - CodeRea  83 kB/s |  13 kB     00:00\n===================== Name Exactly Matched: kernel-headers =====================\nkernel-headers.x86_64 : Header files for the Linux kernel for use by glibc\n</code></pre> <p>Important are the enabled Red Hat Enterprise Linux 8 repositories. <pre><code>Red Hat Enterprise Linux 8 for x86_64 - BaseOS\nRed Hat Enterprise Linux 8 for x86_64 - AppStre\n</code></pre></p> <p>Output of non entitled cluster: <pre><code>$ oc logs rhel\nUpdating Subscription Management repositories.\nUnable to read consumer identity\nSubscription Manager is operating in container mode.\nThis system is not registered to Red Hat Subscription Management. You can use subscription-manager to register.\nRed Hat Universal Base Image 8 (RPMs) - BaseOS  141 kB/s | 772 kB     00:05\nRed Hat Universal Base Image 8 (RPMs) - AppStre 7.6 MB/s | 4.0 MB     00:00\nRed Hat Universal Base Image 8 (RPMs) - CodeRea  51 kB/s |  13 kB     00:00\n===================== Name Exactly Matched: kernel-headers =====================\nkernel-headers.x86_64 : Header files for the Linux kernel for use by glibc\n</code></pre></p>","tags":["entitlement","GPU"]},{"location":"cluster-configuration/full-cluster-entitlement/debugging/","title":"Entitlement debugging","text":"<pre><code>curl -I -XGET  \\\n  --cacert /etc/rhsm/ca/redhat-uep.pem \\\n  --key /etc/pki/entitlement-host/3034856157177793453-key.pem \\\n  --cert /etc/pki/entitlement-host/3034856157177793453.pem \\\n  https:/cdn.redhat.com/content/dist/rhel8/8/x86_64/baseos/os/repodata/repomd.xml\n</code></pre>","tags":["entitlement"]},{"location":"cluster-configuration/image-registry/vsphere-registry/","title":"Vsphere registry","text":""},{"location":"cluster-configuration/image-registry/vsphere-registry/#create-pvc","title":"Create PVC","text":"<pre><code>oc create -f - &lt;&lt;EOF\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: image-registry-pvc\n  namespace: openshift-image-registry\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 100Gi\n  storageClassName: thin\nEOF\n</code></pre>"},{"location":"cluster-configuration/image-registry/vsphere-registry/#patch-registry-operator-crd","title":"Patch registry operator crd","text":"<pre><code>oc patch configs.imageregistry.operator.openshift.io/cluster \\\n    --type='json' \\\n    --patch='[\n        {\"op\": \"replace\", \"path\": \"/spec/managementState\", \"value\": \"Managed\"},\n        {\"op\": \"replace\", \"path\": \"/spec/rolloutStrategy\", \"value\": \"Recreate\"},\n        {\"op\": \"replace\", \"path\": \"/spec/replicas\", \"value\": 1},\n        {\"op\": \"replace\", \"path\": \"/spec/storage\", \"value\": {\"pvc\":{\"claim\": \"image-registry-pvc\" }}}\n    ]'\n</code></pre>"},{"location":"cluster-configuration/logging/forwarding-demo/","title":"Log Forwarding demo","text":""},{"location":"cluster-configuration/logging/forwarding-demo/#start-fluentd-to-file-logging","title":"Start fluentd to file logging","text":""},{"location":"cluster-configuration/logging/forwarding-demo/#create-new-project","title":"Create new project","text":"OC <pre><code>oc new-project fluentd\n</code></pre>"},{"location":"cluster-configuration/logging/forwarding-demo/#deploy-fluentd","title":"Deploy fluentd","text":"OCbuildah-with-secret.yaml <pre><code>oc apply -f https://examples.openshift.pub/pr-133/cluster-configuration/logging/forwarding-demo/deploy-fluentd.yaml\n</code></pre> <pre><code>---\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: fluentd-log\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 100Gi\n  storageClassName: managed-nfs-storage\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: fluentd\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: fluentd\n    spec:\n      containers:\n        - name: fluentd\n          image: docker.io/fluent/fluentd:v1.3-debian-1\n          ports:\n            - containerPort: 24224\n          volumeMounts:\n          - mountPath: /fluentd/log\n            name: fluentd-log\n      volumes:\n      - name: fluentd-log\n        persistentVolumeClaim:\n          claimName: fluentd-log\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: fluentd\nspec:\n  selector:\n    app: fluentd\n  ports:\n    - protocol: TCP\n      port: 24224\n      targetPort: 24224\n</code></pre>"},{"location":"cluster-configuration/logging/forwarding-demo/#test-fluentd","title":"Test fluentd","text":"<p>Login into the pod:</p> <pre><code>$ oc rsh deployment/fluentd bash\n</code></pre> <p>Run command inside the pod: <pre><code>$ echo '{\"message\":\"fooobar\"}' | fluent-cat debug.log --host fluentd.fluentd.svc.cluster.local  --port 24224\n$ grep fooobar /fluentd/log/data.log\n2020-10-28T10:42:24+00:00   debug.log   {\"message\":\"fooobar\"}\n</code></pre></p>"},{"location":"cluster-configuration/logging/forwarding-demo/#deploy-cluster-logging","title":"Deploy Cluster Logging","text":"<ul> <li>Deploy OpenShift Logging Operator</li> <li>Deploy Elastic Search Operator from Red Hat.</li> </ul> <pre><code>oc create -f - &lt;&lt;EOF\napiVersion: logging.openshift.io/v1\nkind: ClusterLogging\nmetadata:\n  name: instance\n  namespace: openshift-logging\nspec:\n  collection:\n    logs:\n      fluentd: {}\n      type: fluentd\nEOF\n</code></pre>"},{"location":"cluster-configuration/logging/forwarding-demo/#deploy-log-forwarding-api","title":"Deploy Log forwarding api","text":"<pre><code>oc create -f - &lt;&lt;EOF\napiVersion: logging.openshift.io/v1\nkind: ClusterLogForwarder\nmetadata:\n  name: instance\n  namespace: openshift-logging\nspec:\n  outputs:\n   - name: fluentd-server-insecure\n     type: fluentdForward\n     url: 'tcp://fluentd.fluentd.svc.cluster.local:24224'\n  pipelines:\n   - name: fluentd-server-insecure-name\n     inputRefs:\n     - audit\n     outputRefs:\n     - fluentd-server-insecure\n     parse: json\n     labels:\n       clusterId: \"C1234\"\nEOF\n</code></pre>"},{"location":"cluster-configuration/logging/forwarding-demo/#viewing-audit-logs","title":"Viewing audit logs","text":"<p>Usefull tool: cluster-debug-tools</p> <pre><code># Extract auditlog from PV\n$ cat data.log | grep k8s-audit.log | cut -f3- &gt; audit.log\n\n$ kubectl dev_tool audit -f audit.log  | head\n14:31:55 [ WATCH][1h29m7.534747s] [200] /api/v1/namespaces/openshift-console-user-settings/configmaps?watch=true&amp;fieldSelector=metadata.name%3Duser-settings-kubeadmin                                                                                                                              [kube:admin]\n14:52:32 [ WATCH][1h8m30.575234s] [200] /apis/console.openshift.io/v1/consolenotifications?watch=true&amp;resourceVersion=79435                                                                                                                                                                         [kube:admin]\n15:02:13 [ WATCH][58m50.081621s] [200]  /apis/console.openshift.io/v1/consolelinks?watch=true&amp;resourceVersion=83292                                                                                                                                                                                 [kube:admin]\n15:08:26 [ WATCH][52m36.517786s] [200]  /apis/apiregistration.k8s.io/v1/apiservices?watch=true&amp;resourceVersion=85827                                                                                                                                                                                [kube:admin]\n15:14:16 [ WATCH][46m47.349073s] [200]  /apis/console.openshift.io/v1/consolequickstarts?watch=true&amp;resourceVersion=88167                                                                                                                                                                           [kube:admin]\n15:24:42 [ WATCH][36m20.76974s] [200]   /apis/config.openshift.io/v1/clusterversions?watch=true&amp;fieldSelector=metadata.name%3Dversion                                                                                                                                                               [kube:admin]\n15:49:29 [ WATCH][9m51.001757s] [200]   /api/v1/namespaces/openshift-kube-scheduler-operator/endpoints?allowWatchBookmarks=true&amp;resourceVersion=102447&amp;timeout=9m51s&amp;timeoutSeconds=591&amp;watch=true                                                                                                  [system:serviceaccount:openshift-monitoring:prometheus-k8s]\n15:49:30 [ WATCH][9m55.001174s] [200]   /apis/config.openshift.io/v1/ingresses?allowWatchBookmarks=true&amp;resourceVersion=102455&amp;timeout=9m55s&amp;timeoutSeconds=595&amp;watch=true                                                                                                                          [system:serviceaccount:openshift-apiserver-operator:openshift-apiserver-operator]\n15:49:37 [ WATCH][9m57.000757s] [200]   /api/v1/namespaces/openshift-marketplace/pods?allowWatchBookmarks=true&amp;resourceVersion=102513&amp;timeout=9m57s&amp;timeoutSeconds=597&amp;watch=true                                                                                                                   [system:serviceaccount:openshift-monitoring:prometheus-k8s]\n15:49:40 [ WATCH][9m50.001387s] [200]   /api/v1/namespaces?allowWatchBookmarks=true&amp;resourceVersion=102501&amp;timeoutSeconds=590&amp;watch=true                                                                                                                                                            [system:serviceaccount:openshift-monitoring:prometheus-operator]\n</code></pre>"},{"location":"cluster-configuration/machine-config/","title":"Machine Config","text":"<p>Create MachineConfig objects that modify files, systemd unit files, and other operating system features running on OpenShift Container Platform nodes. OpenShift Container Platform supports Ignition specification version 3.2. All new machine configs you create going forward should be based on Ignition specification version 3.2.</p>","tags":["MachineConfig","MCO","v4.17"]},{"location":"cluster-configuration/machine-config/#force-machine-config","title":"Force Machine Config","text":"<p>Inspired by https://bugzilla.redhat.com/show_bug.cgi?id=1766513</p> <pre><code>oc debug node/worker0 -- chroot /host touch /run/machine-config-daemon-force\n</code></pre>","tags":["MachineConfig","MCO","v4.17"]},{"location":"cluster-configuration/machine-config/#pause-rebooting","title":"Pause rebooting","text":"<pre><code>oc patch --type=merge --patch='{\"spec\":{\"paused\":true}}' machineconfigpool/master\n</code></pre>","tags":["MachineConfig","MCO","v4.17"]},{"location":"cluster-configuration/machine-config/#rollout-sshd-config-example","title":"Rollout sshd config example","text":"<ul> <li>Tested with OpenShift v4.17</li> <li>Documentation: 1.1. Creating machine configs with Butane</li> </ul> Butane Config for worker node <pre><code>---\nvariant: openshift\nversion: 4.17.0\nmetadata:\n  name: 99-worker-sshd-custom\n  labels:\n    machineconfiguration.openshift.io/role: worker\nstorage:\n  files:\n    - path: /etc/ssh/sshd_config.d/49-sshd-custom.conf\n      mode: 0644\n      overwrite: true\n      contents:\n        inline: |\n          Ciphers aes128-ctr,aes192-ctr,aes256-ctr,aes128-gcm@openssh.com,aes256-gcm@openssh.com\n          MACs hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com,hmac-sha2-256,hmac-sha2-512\n          KexAlgorithms curve25519-sha256@libssh.org,ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group-exchange-sha256\n</code></pre> Butane Config for worker node <pre><code>---\nvariant: openshift\nversion: 4.17.0\nmetadata:\n  name: 99-master-sshd-custom\n  labels:\n    machineconfiguration.openshift.io/role: master\nstorage:\n  files:\n    - path: /etc/ssh/sshd_config.d/49-sshd-custom.conf\n      mode: 0644\n      overwrite: true\n      contents:\n        inline: |\n          Ciphers aes128-ctr,aes192-ctr,aes256-ctr,aes128-gcm@openssh.com,aes256-gcm@openssh.com\n          MACs hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com,hmac-sha2-256,hmac-sha2-512\n          KexAlgorithms curve25519-sha256@libssh.org,ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group-exchange-sha256\n</code></pre>","tags":["MachineConfig","MCO","v4.17"]},{"location":"cluster-configuration/machine-config/#convert-butane-in-machine-config","title":"Convert butane in Machine Config","text":"<pre><code>butane sshd-worker.yaml -o sshd-worker.machineconfig.yaml\nbutane sshd-master.yaml -o sshd-master.machineconfig.yaml\n</code></pre> Machine Config for worker node <pre><code># Generated by Butane; do not edit\napiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfig\nmetadata:\n  labels:\n    machineconfiguration.openshift.io/role: worker\n  name: 99-worker-sshd-custom\nspec:\n  config:\n    ignition:\n      version: 3.4.0\n    storage:\n      files:\n        - contents:\n            compression: gzip\n            source: data:;base64,H4sIAAAAAAAC/3yNzU6FMBCF9zwFD0BNOlrC3d0blsaHqGXoNOlfOsXw+KYqGli4m3znO2dmlwkL9xpZwiRMLUM7b3CcoMZfCpOwJtxTxshMTyaFw7jg7u0xc09BG8GkQTQF67n5lyoJ/6SgxrPbveL+8DYVVylwb7bygaCUvH0Zarx7995mUrEDmoW+i9FxzW3rgp6nlytSIIfFratDQeh90FHYkrYscDeko8WfR91nAAAA//8E0KQIPQEAAA==\n          mode: 420\n          overwrite: true\n          path: /etc/ssh/sshd_config.d/49-sshd-custom.conf\n</code></pre> Machine Config for worker node <pre><code>---\nvariant: openshift\nversion: 4.17.0\nmetadata:\n  name: 99-master-sshd-custom\n  labels:\n    machineconfiguration.openshift.io/role: master\nstorage:\n  files:\n    - path: /etc/ssh/sshd_config.d/49-sshd-custom.conf\n      mode: 0644\n      overwrite: true\n      contents:\n        inline: |\n          Ciphers aes128-ctr,aes192-ctr,aes256-ctr,aes128-gcm@openssh.com,aes256-gcm@openssh.com\n          MACs hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com,hmac-sha2-256,hmac-sha2-512\n          KexAlgorithms curve25519-sha256@libssh.org,ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group-exchange-sha256\n</code></pre>","tags":["MachineConfig","MCO","v4.17"]},{"location":"cluster-configuration/machine-config/#apply-changes-to-cluster","title":"Apply changes to cluster","text":"<pre><code>oc apply -f sshd-worker.machineconfig.yaml\noc apply -f sshd-master.machineconfig.yaml\n</code></pre> <p>All nodes (worker&amp;master) will be drained and rebooted.</p> <p>Watch rollout: <code>watch 'oc get mcp,nodes'</code></p>","tags":["MachineConfig","MCO","v4.17"]},{"location":"cluster-configuration/machine-config/kubelet-configs/","title":"Kubelet Configs","text":"<p>The following configuration for kubelet is applied after the cluster is deployed.</p>"},{"location":"cluster-configuration/machine-config/kubelet-configs/#example-kubelet-configuration","title":"Example Kubelet Configuration","text":"<pre><code>apiVersion: machineconfiguration.openshift.io/v1\nkind: KubeletConfig\nmetadata:\n  name: set-virt-values\nspec:\n  machineConfigPoolSelector:\n    matchLabels:\n      pools.operator.machineconfiguration.openshift.io/worker: \"\"\n  kubeletConfig:\n    autoSizingReserved: true              ### added\n    maxPods: 500                          ### added\n    nodeStatusMaxImages: -1               ### added\n    kubeAPIBurst: 200                     ### added\n    kubeAPIQPS: 100                       ### added\n    evictionSoft:                         ### added\n      memory.available: \"50Gi\"\n    evictionSoftGracePeriod:              ### added\n      memory.available: \"5m\"\n    evictionPressureTransitionPeriod: 0s  ### added\n</code></pre> <ul> <li> <p><code>kubeAPIBurst</code> and <code>kubeAPIQPS</code> - Adjusting these values up to <code>200</code> and <code>100</code> from the default of 100 and 50, respectively, accommodates bulk object creation on the nodes. The lower values are useful on clusters with smaller nodes to keep API server resource utilization reasonable, however with larger nodes this is not an issue.</p> </li> <li> <p><code>maxPods</code> - By default, OpenShift sets the maximum Pods per node to 250. This value affects not just the Pods running core OpenShift services and node functions but also virtual machines. For large virtualization nodes that can host many virtual machines, this value is likely too small. If you\u2019re using very large nodes and may have more than 500 VMs and Pods on a node, this value can be increased beyond <code>500</code>, however you will also need to adjust the size of the cluster network\u2019s host prefix when deploying the cluster.</p> </li> <li> <p><code>nodeStatusMaxImage</code> - The scheduler factors both the count of container images and which container images are on a host when deciding where to place a Pod or virtual machine. For large nodes with many different Pods and VMs, this can lead to unnecessary and undesired behavior. Disabling the <code>imageLocality</code> scheduler plugin by setting <code>nodeStatusMaxImage</code> to <code>-1</code> facilitates balanced scheduling across cluster nodes, avoiding scenarios where VMs are scheduled to the same host as a result of the image already being present vs factoring in other resource availability.</p> </li> <li> <p><code>autoSizingReserved</code> - parameter set to <code>true</code> to allow OpenShift Container Platform to automatically determine and allocate the system-reserved resources on the nodes associated with the specified label. To disable automatic allocation on those nodes, set this parameter to <code>false</code>.</p> </li> <li> <p>Source: Automatically allocating resources for nodes</p> </li> <li> <p>Configure CPU manager - to enable dedicated resources for virtual machines to be assigned. Without CPU manager, virtual machines using dedicated CPU scheduling, such as those configured with the cx instance type, cannot be scheduled.</p> </li> <li> <p>soft eviction - sets the upper boundary for memory utilization on the nodes before the virtual machines are attempted to be moved to other hosts in the cluster. This value should be set for a reasonable value according to the approximate maximum amount of memory utilization you want on the node, however if all nodes are exceeding this value then workload will not be rescheduled.</p> </li> </ul>"},{"location":"cluster-configuration/machine-config/machine-config-server/","title":"Machine Config Server","text":""},{"location":"cluster-configuration/machine-config/machine-config-server/#accessing-the-machineconfigserver-directly","title":"Accessing the MachineConfigServer directly","text":"<p><pre><code>curl -H \"Accept: application/vnd.coreos.ignition+json; version=3.2.0\" -k \\\n  https://api-int....:22623/config/worker\n</code></pre> Source</p>"},{"location":"cluster-configuration/machine-config/machine-config-server/#create-machineconfig","title":"Create MachineConfig","text":"<pre><code>oc create -f - &lt;&lt;EOF\napiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfig\nmetadata:\n  labels:\n    machineconfiguration.openshift.io/role: mypool\n  name: 00-mypool\nspec:\n  config:\n    ignition:\n      config: {}\n      security:\n        tls: {}\n      timeouts: {}\n      version: 2.2.0\n    networkd: {}\n    passwd: {}\n    storage: {}\n  fips: false\n  kernelArguments: null\n  osImageURL: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:db2b9ac6cd5ae6eb30b1b2c5f9739734edc7b628862072fb7399b4377684265b\nEOF\n</code></pre>"},{"location":"cluster-configuration/machine-config/machine-config-server/#create-machineconfigpool","title":"Create MachineConfigPool","text":"<pre><code>oc create -f - &lt;&lt;EOF\napiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfigPool\nmetadata:\n  name: mypool\nspec:\n  configuration:\n    name: rendered-mypool\n    source:\n    - apiVersion: machineconfiguration.openshift.io/v1\n      kind: MachineConfig\n      name: mypool\n  machineConfigSelector:\n    matchLabels:\n      machineconfiguration.openshift.io/role: mypool\n  nodeSelector:\n    matchLabels:\n      node-role.kubernetes.io/mypool: \"\"\n  paused: false\nEOF\n</code></pre>"},{"location":"cluster-configuration/machine-config/machine-config-server/#result","title":"Result","text":"<pre><code>root@homer:~ $ curl -i  -k https://192.168.51.1:22623/config/mypool\nHTTP/1.1 200 OK\nContent-Length: 8823\nContent-Type: application/json\nDate: Tue, 29 Oct 2019 15:55:35 GMT\n\n{\"ignition\":{\"config\":{.......\n</code></pre>"},{"location":"cluster-configuration/monitoring/","title":"Monitoring","text":"","tags":["monitoring"]},{"location":"cluster-configuration/monitoring/#content","title":"Content","text":"","tags":["monitoring"]},{"location":"cluster-configuration/monitoring/alertmanager/","title":"Alertmanager","text":"<p>Documentation: Sending notifications to external systems</p>"},{"location":"cluster-configuration/monitoring/alertmanager/#example-receiver","title":"Example receiver","text":"<ul> <li>Webhook: Debug receiver - log alerts to stdout</li> <li>Webhook: Telegram receiver</li> <li>Webhook: Microsoft Teams receiver</li> <li>E-Mail: Using labels to direct email notifications</li> </ul>"},{"location":"cluster-configuration/monitoring/alertmanager/#proxy-alertmanager","title":"Proxy &amp; Alertmanager","text":"<p>Warning</p> <p>Alertmanager do not pickup global proxy settings.</p> <p>alertmanager.yaml</p> <pre><code>\"global\":\n  \"resolve_timeout\": \"5m\"\n  slack_api_url: https://hooks.slack.com/services/xxxx/xxxx/xxxxx\n\"receivers\":\n  - name: slack\n    slack_configs:\n    - channel: '#ops'\n      http_config:\n        proxy_url: http://192.168.51.1:8888\n\n\"route\":\n  \"group_by\":\n  - \"job\"\n  \"group_interval\": \"5m\"\n  \"group_wait\": \"30s\"\n  \"receiver\": \"slack\"\n  \"repeat_interval\": \"12h\"\n  \"routes\":\n    - receiver: slack\n      match:\n        alertname: Watchdog\n</code></pre>"},{"location":"cluster-configuration/monitoring/alertmanager/#apply-config","title":"Apply config","text":"<pre><code>oc -n openshift-monitoring create secret generic alertmanager-main \\\n   --from-file=alertmanager.yaml=alertmanager.yaml \\\n   --dry-run -o=yaml |  oc -n openshift-monitoring replace secret \\\n   --filename=-\n</code></pre>"},{"location":"cluster-configuration/monitoring/alertmanager/debug-receiver/","title":"Debug receiver","text":"","tags":["alertmanager","prometheus","debug","monitoring"]},{"location":"cluster-configuration/monitoring/alertmanager/debug-receiver/#deployment","title":"Deployment","text":"OCdeployment.yaml <pre><code>oc apply -f https://examples.openshift.pub/pr-133/cluster-configuration/monitoring/alertmanager/debug-receiver/deployment.yaml\n</code></pre> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: debug-receiver\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: debug-receiver\n  template:\n    metadata:\n      labels:\n        app: debug-receiver\n    spec:\n      containers:\n      - image: registry.access.redhat.com/ubi8/python-38:latest\n        name: python\n        command:\n          - /bin/sh\n          - -c\n          - |\n            pip install pyserv\n            serv 9393\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: debug-receiver\n  name: debug-receiver\nspec:\n  ports:\n  - name: alert-debug-receiver\n    port: 9393\n    protocol: TCP\n    targetPort: 9393\n  selector:\n    app: debug-receiver\n  sessionAffinity: None\n  type: ClusterIP\n</code></pre>","tags":["alertmanager","prometheus","debug","monitoring"]},{"location":"cluster-configuration/monitoring/alertmanager/debug-receiver/#configure-alertmanager","title":"Configure alertmanager","text":"<p>Administrator -&gt; Cluster Settings -&gt; Global configuration -&gt; Alertmanager -&gt; Create Receiver</p> <p></p>","tags":["alertmanager","prometheus","debug","monitoring"]},{"location":"cluster-configuration/monitoring/alertmanager/github-receiver/","title":"Deploy GitHub receiver","text":"","tags":["alertmanager","prometheus","GitHub","monitoring"]},{"location":"cluster-configuration/monitoring/alertmanager/microsoft-teams-receiver/","title":"Microsoft Teams Configuration","text":"<ul> <li> <p>On Microsoft Teams, create new channel to the teams that should receive notifications</p> </li> <li> <p>Install \"Incoming Webhook\" connector by click the dots beside the channel and click on Connectors</p> </li> <li> <p>Configure the webhook, and notes the webhook URL </p> </li> </ul>","tags":["alertmanager","prometheus","microsoft-teams","monitoring"]},{"location":"cluster-configuration/monitoring/alertmanager/microsoft-teams-receiver/#deploy-microsoft-teams-receiver","title":"Deploy Microsoft Teams receiver","text":"","tags":["alertmanager","prometheus","microsoft-teams","monitoring"]},{"location":"cluster-configuration/monitoring/alertmanager/microsoft-teams-receiver/#deploye-prometheus-msteams","title":"Deploye prometheus-msteams","text":"<p>Create new project on OpenShift using prometheus-msteams</p> <pre><code>oc new-project prometheus-msteams\noc project prometheus-msteams\noc new-app --name=\"promteams\" \\\n    -e TEAMS_INCOMING_WEBHOOK_URL=\"https://teams-webhook-url\" \\\n    -e TEAMS_REQUEST_URI=alertmanager \\\n    -e HTTP_PROXY=\"http://x.x.x.x\" \\\n    -e HTTPS_PROXY=\"http://x.x.x.x\" \\\n    quay.io/prometheusmsteams/prometheus-msteams\n</code></pre>","tags":["alertmanager","prometheus","microsoft-teams","monitoring"]},{"location":"cluster-configuration/monitoring/alertmanager/microsoft-teams-receiver/#configure-alertmanager","title":"Configure Alertmanager","text":"<p>After deploying prometheus-msteams go to alert manager configuration, and create new webhook receiver with url: <pre><code>http://promteams.prometheus-msteams.svc.cluster.local:2000/alertmanager\n</code></pre></p>","tags":["alertmanager","prometheus","microsoft-teams","monitoring"]},{"location":"cluster-configuration/monitoring/alertmanager/telegram-receiver/","title":"Deploy Telegram receiver","text":"<p>Connected Prometheus alertmanager via webhook receiver to telegram.</p> <pre><code>oc new-project telegram\n# Build and deploy\noc new-app --name=telegram \\\n    https://github.com/openshift-examples/alertmanager-webhook-telegram.git\n</code></pre> <p>Add enviorment variables to deploymentconfig telegram</p> <pre><code>FLASK_ENV=development # If you like debug output ;-)\nAPP_FILE=flaskAlert.py\nTELEGRAM_BOTTOKEN=\"9999999999:AAAAAA-AAAAAA-AAAAA....\"\nTELEGRAM_CHATID=\"999999999\"\nBASIC_AUTH_USERNAME=\"aiPh1eHu\"\nBASIC_AUTH_PASSWORD=\"eoPhait8\"\n# In case of proxy env:\nHTTP_PROXY=\"http://192.168.51.1:8888\"\nHTTPS_PROXY=\"http://192.168.51.1:8888\"\n</code></pre> <p>Adjust alertmanager.yaml, example:</p> <p>alertmanager.yaml <pre><code>\"global\":\n  \"resolve_timeout\": \"5m\"\n\"receivers\":\n  - name: 'telegram-webhook'\n    webhook_configs:\n    - url: http://telegram.telegram.svc.cluster.local:8080/alert\n      send_resolved: true\n      http_config:\n        basic_auth:\n          username: 'aiPh1eHu'\n          password: 'eoPhait8'\n\"route\":\n  \"group_by\":\n  - \"job\"\n  \"group_interval\": \"5m\"\n  \"group_wait\": \"30s\"\n  \"receiver\": \"telegram-webhook\"\n  \"repeat_interval\": \"12h\"\n  \"routes\":\n    - receiver: telegram-webhook\n      match:\n        alertname: Watchdog\n</code></pre></p>","tags":["alertmanager","prometheus","Telegram","monitoring"]},{"location":"cluster-configuration/storage/","title":"Storage","text":"","tags":["storage"]},{"location":"cluster-configuration/storage/nfs/","title":"NFS Storage for dynamic provisioning and image registry","text":"","tags":["storage"]},{"location":"cluster-configuration/storage/nfs/#setup-nfs-server","title":"Setup NFS-Server","text":"<pre><code>yum -y install nfs-utils\nsystemctl enable --now rpcbind\n# RHEL 8: systemctl enable --now nfs-server\n# RHEL 7: systemctl enable --now nfs\nmkdir -p /srv/nfs-storage-{pv-infra-registry,pv-user-pvs}\nchmod 770 /srv/nfs-storage-{pv-infra-registry,pv-user-pvs}\n\necho \"\n/srv/nfs-storage-pv-infra-registry 192.168.51.0/24(rw,sync,no_root_squash)\n/srv/nfs-storage-pv-user-pvs 192.168.51.0/24(rw,sync,no_root_squash)\n\" &gt;&gt; /etc/exports\n\nexportfs -ra\n</code></pre>","tags":["storage"]},{"location":"cluster-configuration/storage/nfs/#configure-image-registry","title":"Configure image registry","text":"","tags":["storage"]},{"location":"cluster-configuration/storage/nfs/#create-persistentvolume-for-image-registry","title":"Create PersistentVolume for image registry","text":"<pre><code>oc create -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: nfs-registry-storage\nspec:\n  accessModes:\n  - ReadWriteMany\n  capacity:\n    storage: 100Gi\n  nfs:\n    path: \"/srv/nfs-storage-pv-infra-registry\"\n    server: \"192.168.51.1\"\n  persistentVolumeReclaimPolicy: Recycle\nEOF\n</code></pre>","tags":["storage"]},{"location":"cluster-configuration/storage/nfs/#create-persistentvolumeclaim-for-image-registry","title":"Create PersistentVolumeClaim for image registry","text":"<pre><code>oc create -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: registry-storage\n  namespace: openshift-image-registry\nspec:\n  accessModes:\n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 100Gi\nEOF\n</code></pre>","tags":["storage"]},{"location":"cluster-configuration/storage/nfs/#patch-image-registry","title":"Patch Image registry","text":"<pre><code>oc patch configs.imageregistry.operator.openshift.io cluster --type='json' -p='[{\"op\": \"remove\", \"path\": \"/spec/storage\" },{\"op\": \"add\", \"path\": \"/spec/storage\", \"value\": {\"pvc\":{\"claim\": \"registry-storage\"}}}]'\n</code></pre>","tags":["storage"]},{"location":"cluster-configuration/storage/nfs/#deploy-nfs-client-provisioner","title":"Deploy nfs-client-provisioner","text":"<p>Based on a retired project: https://github.com/kubernetes-retired/external-storage/tree/master/nfs-client</p> <pre><code>oc process -f  https://raw.githubusercontent.com/openshift-examples/external-storage-nfs-client/main/openshift-template-nfs-client-provisioner.yaml \\\n  -p NFS_SERVER=192.168.51.1 \\\n  -p NFS_PATH=/srv/nfs-storage-pv-user-pvs  | oc apply -f -\n</code></pre>","tags":["storage"]},{"location":"cluster-configuration/storage/nfs/#air-gapped-deployment","title":"Air-gapped deployment","text":"<pre><code># Part of general oc adm release mirror\nexport LOCAL_REPOSITORY=ocp4/openshift4\nexport LOCAL_REGISTRY=host.compute.local:5000\nexport LOCAL_SECRET_JSON=pullsecret.json\n\n# Mirror nfs-client-provisioner\noc image mirror -a ${LOCAL_SECRET_JSON} \\\n  quay.io/external_storage/nfs-client-provisioner:latest \\\n  ${LOCAL_REGISTRY}/${LOCAL_REPOSITORY}:nfs-client-provisioner-latest\n\n# Deployment\noc process -f https://raw.githubusercontent.com/openshift-examples/external-storage-nfs-client/main/openshift-template-nfs-client-provisioner.yaml \\\n  -p NFS_SERVER=192.168.51.1 \\\n  -p NFS_PATH=/var/lib/libvirt/images/air-gapped-pv-user-pvs \\\n  -p PROVISIONER_IMAGE=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY}:nfs-client-provisioner-latest | oc apply -f -\n</code></pre>","tags":["storage"]},{"location":"cluster-configuration/storage/csi-driver-nfs/","title":"CSI Driver NFS","text":"<p>Based on</p> <ul> <li>https://github.com/kubernetes-csi/csi-driver-nfs</li> <li>https://hackmd.io/@johnsimcall/BJeW2Y5mT</li> <li>https://hackmd.io/@kincl/csi-driver-nfs-with-console</li> </ul> Snapshots are very slow! <p>Snapshots are coping data via tar .. $source $targert and that is incredible slow. OpenShift Virtualization runs in timeout, for example, during VM cloning via WebUI. Possible solution, create an VM snapshot with an extralong timeout:</p> <pre><code>apiVersion: snapshot.kubevirt.io/v1beta1\nkind: VirtualMachineSnapshot\nmetadata:\n  name: snapshot-with-60-minute-timeout\nspec:\n  failureDeadline: 1h0m0s\n  source:\n    apiGroup: kubevirt.io\n    kind: VirtualMachine\n    name: rhel9-violet-halibut-12\n</code></pre>","tags":["nfs","csi","ocp-v","kubevirt"]},{"location":"cluster-configuration/storage/csi-driver-nfs/#deployment-via-helm","title":"Deployment via helm","text":"","tags":["nfs","csi","ocp-v","kubevirt"]},{"location":"cluster-configuration/storage/csi-driver-nfs/#prepare-namespace","title":"Prepare namespace","text":"<pre><code>export NAMESPACE=openshift-csi-driver-nfs\noc create namespace ${NAMESPACE}\noc adm policy add-scc-to-user -n  ${NAMESPACE}  privileged -z csi-nfs-controller-sa\noc adm policy add-scc-to-user -n  ${NAMESPACE}  privileged -z csi-nfs-node-sa\n</code></pre>","tags":["nfs","csi","ocp-v","kubevirt"]},{"location":"cluster-configuration/storage/csi-driver-nfs/#deploy","title":"Deploy","text":"<p>Download <code>values.yaml</code> and adjust the NFS-Server and Path in the last lines.</p> helmvalues.yaml <pre><code>curl -L -O  https://examples.openshift.pub/pr-133/cluster-configuration/storage/csi-driver-nfs/values.yaml\n</code></pre> <pre><code>---\ncontroller:\n  resources:\n    csiProvisioner:\n      limits:\n        memory: 1024Mi\n    csiSnapshotter:\n      limits:\n        memory: 1024Mi\n    livenessProbe:\n      limits:\n        memory: 1024Mi\n    nfs:\n      limits:\n        memory: 1024Mi\n\nexternalSnapshotter:\n  enabled: true\n  customResourceDefinitions:\n    enabled: false\n\n## StorageClass resource example:\nstorageClass:\n  create: true\n  name: nfs-csi\n  annotations:\n    storageclass.kubevirt.io/is-default-virt-class: \"true\"\n    storageclass.kubernetes.io/is-default-class: \"true\"\n  parameters:\n    server: 10.32.97.1\n    share: /coe_stormshift_ocp1\n    subDir: ${pvc.metadata.namespace}-${pvc.metadata.name}-${pv.metadata.name}\n</code></pre> <pre><code># Do use openshift/csi-driver-nfs because csi resizer is missing\nhelm repo add csi-driver-nfs https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/charts\n\n#  v0.0.0, because we want latest with csi-resizer\n\nhelm install csi-driver-nfs \\\n  csi-driver-nfs/csi-driver-nfs \\\n  --namespace openshift-csi-driver-nfs \\\n  --version v0.0.0 \\\n  --values values.yaml\n</code></pre>","tags":["nfs","csi","ocp-v","kubevirt"]},{"location":"cluster-configuration/storage/csi-driver-nfs/#create-volumesnapshotclass","title":"Create VolumeSnapshotClass","text":"<p>It's missing in the HelmChart: https://github.com/kubernetes-csi/csi-driver-nfs/issues/825</p> <pre><code>oc apply -f &lt;&lt;EOF\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotClass\nmetadata:\n  name: nfs-csi-snapclass\ndriver: nfs.csi.k8s.io\ndeletionPolicy: Delete\nEOF\n</code></pre>","tags":["nfs","csi","ocp-v","kubevirt"]},{"location":"cluster-installation/","title":"Cluster installation","text":""},{"location":"cluster-installation/#content","title":"Content","text":"<ul> <li> <p>Air-gapped</p> </li> <li> <p>SNO on OCP-V</p> </li> <li> <p>VMware/vSphere</p> </li> <li> <p>Hosted Control Plane</p> </li> <li> <p>Nvidia GPU</p> </li> <li> <p>Windows Container</p> </li> <li> <p>Adjust RHCOS.ISO</p> </li> </ul>"},{"location":"cluster-installation/adjust-rhcos.iso/","title":"How to adjust the an RHEL CoreOS ISO","text":"<p>Prerequisites</p> <ul> <li>Red Hat CoreOS ISO</li> <li>Latest coreos-installer</li> </ul>","tags":["rhcos","coreos"]},{"location":"cluster-installation/adjust-rhcos.iso/#prepare-auto-install-usb-sticks-including-worker-igntion","title":"Prepare auto-install USB-Sticks including worker igntion","text":"","tags":["rhcos","coreos"]},{"location":"cluster-installation/adjust-rhcos.iso/#get-worker-ignition-from-running-cluster","title":"Get worker ignition from running cluster","text":"<p>or use that one created from <code>openshift-install</code></p> <pre><code>oc get -n openshift-machine-api \\\n  secrets/worker-user-data \\\n  -o go-template=\"{{ .data.userData | base64decode }}\" \\\n  &gt; worker.ign\n</code></pre>","tags":["rhcos","coreos"]},{"location":"cluster-installation/adjust-rhcos.iso/#create-own-rhcos-iso","title":"Create own rhcos iso","text":"<pre><code>coreos-installer iso customize \\\n  --dest-device /dev/sda \\\n  --dest-ignition worker.ign \\\n  -o ready-to-install-at-sda.iso \\\n  rhcos-live.x86_64.iso\n</code></pre> <p>Option: double check ignition:</p> <pre><code>coreos-installer iso ignition show ready-to-install-at-sda.iso\n</code></pre>","tags":["rhcos","coreos"]},{"location":"cluster-installation/adjust-rhcos.iso/#prepare-usb-stick","title":"Prepare usb stick","text":"<pre><code>dd if=ready-to-install-at-sda.iso of=/dev/$USB_DISK status=progress\n</code></pre>","tags":["rhcos","coreos"]},{"location":"cluster-installation/air-gapped/","title":"Air-gapped installation","text":"","tags":["air-gapped","disconnected","restricted-network"]},{"location":"cluster-installation/air-gapped/#download-oc-client","title":"Download oc client","text":"<p>Download oc client from cloud.redhat.com or mirror.openshift.com</p>","tags":["air-gapped","disconnected","restricted-network"]},{"location":"cluster-installation/air-gapped/#create-mirror-registry","title":"Create mirror registry","text":"<p>This follows the official documentation: Creating a mirror registry for installation in a restricted network. It is just a short wrap up for me.</p>","tags":["air-gapped","disconnected","restricted-network"]},{"location":"cluster-installation/air-gapped/#install-prepare-image-registry","title":"Install &amp; prepare image registry","text":"<pre><code>yum -y install podman httpd-tools\n\nmkdir -p /var/lib/libvirt/images/mirror-registry/{auth,certs,data}\n\nopenssl req -newkey rsa:4096 -nodes -sha256 \\\n  -keyout /var/lib/libvirt/images/mirror-registry/certs/domain.key \\\n  -x509 -days 365 -subj \"/CN=host.compute.local\" \\\n  -out /var/lib/libvirt/images/mirror-registry/certs/domain.crt\n\ncp -v /var/lib/libvirt/images/mirror-registry/certs/domain.crt /etc/pki/ca-trust/source/anchors/\nupdate-ca-trust\n\nhtpasswd -bBc /var/lib/libvirt/images/mirror-registry/auth/htpasswd admin r3dh4t\\!1\n</code></pre> <p>Create internal registry service: <code>/etc/systemd/system/mirror-registry.service</code></p> <p>Change REGISTRY_HTTP_ADDR in case you use different network</p> <pre><code>cat - &gt; /etc/systemd/system/mirror-registry.service &lt;&lt;EOF\n[Unit]\nDescription=Mirror registry (mirror-registry)\nAfter=network.target\n\n[Service]\nType=simple\nTimeoutStartSec=5m\n\nExecStartPre=-/usr/bin/podman rm \"mirror-registry\"\nExecStartPre=/usr/bin/podman pull quay.io/redhat-emea-ssa-team/registry:2\nExecStart=/usr/bin/podman run --name mirror-registry --net host \\\n  -v /var/lib/libvirt/images/mirror-registry/data:/var/lib/registry:z \\\n  -v /var/lib/libvirt/images/mirror-registry/auth:/auth:z \\\n  -e \"REGISTRY_AUTH=htpasswd\" \\\n  -e \"REGISTRY_HTTP_ADDR=192.168.50.1:5000\" \\\n  -e \"REGISTRY_AUTH_HTPASSWD_REALM=registry-realm\" \\\n  -e \"REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd\" \\\n  -e \"REGISTRY_COMPATIBILITY_SCHEMA1_ENABLED=TRUE\" \\\n  -v /var/lib/libvirt/images/mirror-registry/certs:/certs:z \\\n  -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \\\n  -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \\\n  -e REGISTRY_COMPATIBILITY_SCHEMA1_ENABLED=true \\\n  quay.io/redhat-emea-ssa-team/registry:2\n\nExecReload=-/usr/bin/podman stop \"mirror-registry\"\nExecReload=-/usr/bin/podman rm \"mirror-registry\"\nExecStop=-/usr/bin/podman stop \"mirror-registry\"\nRestart=always\nRestartSec=30\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre> <p>Enable and start mirror registry <pre><code>systemctl enable --now mirror-registry.service\nsystemctl status mirror-registry.service\n</code></pre></p> <p>Configure firewall for Centos or RHEL <pre><code>firewall-cmd --zone=public --permanent --add-port=5000/tcp\nfirewall-cmd --reload\n</code></pre></p> <p>Check registry <pre><code>$ curl -u admin:r3dh4t\\!1 https://host.compute.local:5000/v2/_catalog\n{\"repositories\":[]}\n</code></pre></p> <p>Create mirror registry pullsecret <pre><code>podman login --authfile mirror-registry-pullsecret.json host.compute.local:5000\n</code></pre></p>","tags":["air-gapped","disconnected","restricted-network"]},{"location":"cluster-installation/air-gapped/#download-red-hat-pull-secret","title":"Download Red Hat pull secret","text":"<p>Download Red Hat pull secret and store it in <code>redhat-pullsecret.json</code></p>","tags":["air-gapped","disconnected","restricted-network"]},{"location":"cluster-installation/air-gapped/#mirror-images","title":"Mirror images","text":"<p>Merge  mirror-registry-pullsecret.json &amp; redhat-pullsecret.json <pre><code>jq -s '{\"auths\": ( .[0].auths + .[1].auths ) }' mirror-registry-pullsecret.json redhat-pullsecret.json &gt; pullsecret.json\n</code></pre></p> <p>Mirror images: <pre><code>export OCP_RELEASE=$(oc version -o json  --client | jq -r '.releaseClientVersion')\nexport LOCAL_REGISTRY='host.compute.local:5000'\nexport LOCAL_REPOSITORY='ocp4/openshift4'\nexport PRODUCT_REPO='openshift-release-dev'\nexport LOCAL_SECRET_JSON='pullsecret.json'\nexport RELEASE_NAME=\"ocp-release\"\nexport ARCHITECTURE=x86_64\n# export REMOVABLE_MEDIA_PATH=&lt;path&gt;\n\n# Try run:\n\noc adm -a ${LOCAL_SECRET_JSON} release mirror \\\n     --from=quay.io/${PRODUCT_REPO}/${RELEASE_NAME}:${OCP_RELEASE}-${ARCHITECTURE} \\\n     --to=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY} \\\n     --to-release-image=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY}:${OCP_RELEASE}-${ARCHITECTURE} --dry-run\n\noc adm -a ${LOCAL_SECRET_JSON} release mirror \\\n     --from=quay.io/${PRODUCT_REPO}/${RELEASE_NAME}:${OCP_RELEASE}-${ARCHITECTURE} \\\n     --to=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY} \\\n     --to-release-image=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY}:${OCP_RELEASE}-${ARCHITECTURE}\n</code></pre></p> <p>Save the output: <pre><code>info: Mirroring completed in 57.1s (81.95MB/s)\n\nSuccess\nUpdate image:  host.compute.local:5000/ocp4/openshift4:4.2.0\nMirror prefix: host.compute.local:5000/ocp4/openshift4\n\nTo use the new mirrored repository to install, add the following section to the install-config.yaml:\n\nimageContentSources:\n- mirrors:\n  - host.compute.local:5000/ocp4/openshift4\n  source: quay.io/openshift-release-dev/ocp-release\n- mirrors:\n  - host.compute.local:5000/ocp4/openshift4\n  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n\n\nTo use the new mirrored repository for upgrades, use the following to create an ImageContentSourcePolicy:\n\napiVersion: operator.openshift.io/v1alpha1\nkind: ImageContentSourcePolicy\nmetadata:\n  name: example\nspec:\n  repositoryDigestMirrors:\n  - mirrors:\n    - host.compute.local:5000/ocp4/openshift4\n    source: quay.io/openshift-release-dev/ocp-release\n  - mirrors:\n    - host.compute.local:5000/ocp4/openshift4\n    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n</code></pre></p> <p>Extract openshift-install command <pre><code>oc adm release extract -a pullsecret.json --command=openshift-install \"${LOCAL_REGISTRY}/${LOCAL_REPOSITORY}:${OCP_RELEASE}-${ARCHITECTURE}\"\n</code></pre></p> <p>Check openshift-install version: <pre><code>$ ./openshift-install version\n./openshift-install 4.5.2\nbuilt from commit 6336a4b3d696dd898eed192e4188edbac99e8c27\nrelease image host.compute.local:5000/ocp4/openshift4@sha256:8f923b7b8efdeac619eb0e7697106c1d17dd3d262c49d8742b38600417cf7d1d\n</code></pre></p>","tags":["air-gapped","disconnected","restricted-network"]},{"location":"cluster-installation/air-gapped/#ressources","title":"Ressources","text":"<ul> <li>Creating a mirror registry for installation in a restricted network</li> <li>Installing a cluster on bare metal in a restricted network</li> <li>https://github.com/openshift-telco/openshift4x-poc/blob/master/utils/Registry/local-registry.md</li> <li>https://github.com/ashcrow/filetranspiler</li> <li>https://github.com/dwojciec/OLM-disconnected</li> <li>https://github.com/operator-framework/operator-registry</li> <li>RH-INTERN: Disconnected \u201cAir-gapped\u201d Install Procedure</li> <li>RH-INTERN: OLM Disconnected Install</li> </ul>","tags":["air-gapped","disconnected","restricted-network"]},{"location":"cluster-installation/gpu/","title":"General GPU notes","text":"<ul> <li>Upstream version: https://github.com/NVIDIA/gpu-operator</li> <li>The official documentation can be found here: https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/overview.html</li> <li>The current supported feature set can be found here: https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/platform-support.html#operator-platform-support</li> </ul>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-debuging/","title":"Know-Issues / Debugging","text":"","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-debuging/#check-entitlement","title":"Check entitlement","text":"<pre><code>kubectl run check-entitlement  \\\n  --image=registry.access.redhat.com/ubi8  \\\n  --command -- /bin/sh -c 'sleep infinity'\n\nkubectl exec -ti check-entitlement -- bash\n\nrct cat-cert /etc/pki/entitlement-host/entitlement.pem | grep 'Label: rhocp'\n</code></pre> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    run: check-entitlement\n  name: check-entitlement\nspec:\n  containers:\n  - command:\n    - /bin/sh\n    - -c\n    - |\n      echo \"# Check available repos\"\n      rct cat-cert /etc/pki/entitlement-host/entitlement.pem | grep 'Label: rhocp-4.6'\n\n      echo \"# Check access to rhocp-4.6-for-rhel-8-x86_64-rpms\"\n      curl -I -XGET --silent \\\n        --cacert /etc/rhsm/ca/redhat-uep.pem \\\n        --key /etc/pki/entitlement-host/entitlement-key.pem \\\n        --cert /etc/pki/entitlement-host/entitlement.pem \\\n        https:/cdn.redhat.com/content/dist/layered/rhel8/x86_64/rhocp/4.6/os/repodata/repomd.xml\n\n    image: registry.access.redhat.com/ubi8\n    name: check-entitlement\n    resources: {}\n  dnsPolicy: ClusterFirst\n  restartPolicy: Never\nstatus: {}\nEOF\n</code></pre>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-debuging/#wait-of-pod-completion","title":"Wait of pod completion","text":"<pre><code>$ kubectl logs check-entitlement\n# Check available repos\n        Label: rhocp-4.6-for-rhel-8-x86_64-debug-rpms\n        Label: rhocp-4.6-for-rhel-8-x86_64-files\n        Label: rhocp-4.6-for-rhel-8-x86_64-rpms\n        Label: rhocp-4.6-for-rhel-8-x86_64-source-rpms\n# Check access to rhocp-4.6-for-rhel-8-x86_64-rpms\nHTTP/1.1 200 OK\nAccept-Ranges: bytes\nContent-Type: application/xml\nETag: \"82a31f982e86d3431e2216a8c55f066e:1612871206.488625\"\nLast-Modified: Tue, 09 Feb 2021 11:34:11 GMT\nServer: AkamaiNetStorage\nContent-Length: 4150\nDate: Tue, 09 Feb 2021 15:10:21 GMT\nX-Cache: TCP_HIT from a95-101-79-92.deploy.akamaitechnologies.com (AkamaiGHost/10.2.4-31895370) (-)\nConnection: keep-alive\nEJ-HOST: authorizer-prod-dc-us-west-19-mpfsw\nX-Akamai-Request-ID: 103c4687\n</code></pre>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-debuging/#imagepullbackoff-of-nvidia-driver-daemonset-pods","title":"ImagePullBackOff of nvidia-driver-daemonset-* pods","text":"<pre><code>$ oc describe pods -l app=nvidia-driver-daemonset  | grep image\n  Normal   Pulling         116s (x4 over 3m36s)  kubelet, compute-0  Pulling image \"nvidia/driver:440.64.00-rhel8.2\"\n  Warning  Failed          112s (x4 over 3m31s)  kubelet, compute-0  Failed to pull image \"nvidia/driver:440.64.00-rhel8.2\": rpc error: code = Unknown desc = Error reading manifest 440.64.00-rhel8.2 in docker.io/nvidia/driver: manifest unknown: manifest unknown\n  Normal   BackOff         86s (x6 over 3m31s)   kubelet, compute-0  Back-off pulling image \"nvidia/driver:440.64.00-rhel8.2\"\n</code></pre> <p>Double check if the image is available:</p> <pre><code>$ curl -s https://registry.hub.docker.com/v1/repositories/nvidia/driver/tags | jq -r ' .[] | .name' | grep rhel8\n418.87.01--rhel8\n418.87.01-4.18.0-80.11.2.el8_0.x86_64-rhel8\n418.87.01-rhel8\n440.33.01-1.0.0-custom-rhel8\n440.33.01-4.18.0-80.11.2.el8_0.x86_64-rhel8\n440.33.01-rhel8\n440.64.00-1.0.0-custom-rhel8\n440.64.00-1.0.0-rhel8\n</code></pre>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-debuging/#image-is-missing","title":"Image is missing","text":"<p>How the GPU operator build the Image tag: 1) All fields of <code>clusterpolicy.spec.driver</code> 2) OS Version of the node in case of OpenShift it use the node labels <code>feature.node.kubernetes.io/system-os_release.VERSION_ID</code> and <code>feature.node.kubernetes.io/system-os_release.ID</code> Source If you like to check the node labels please run: <code>oc get nodes -L feature.node.kubernetes.io/system-os_release.VERSION_ID -L feature.node.kubernetes.io/system-os_release.ID</code></p>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-debuging/#solution-use-a-local-image-copy-with-matching-tag","title":"Solution use a local image copy with matching tag","text":"<p>Create local image copy with matching tag:</p> <pre><code>oc -n gpu-operator-resources import-image \\\n  nvidia-driver:440.64.00-rhel8.2 \\\n  --from=docker.io/nvidia/driver:440.64.00-rhcos4.5 \\\n  --reference-policy=local --confirm\n</code></pre> <p>Update the clusterpolicy/cluster-policy  <code>oc edit clusterpolicy/cluster-policy</code> to</p> <pre><code>spec:\n...\n  driver:\n    image: nvidia-driver\n    repository: image-registry.openshift-image-registry.svc:5000/gpu-operator-resources\n    version: 440.64.00\n...\n</code></pre>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-debuging/#error-unable-to-find-a-match-kernel-headers-4180-193231el8_2x86_64-kernel-devel-4180-193231el8_2x86_64","title":"Error: Unable to find a match: kernel-headers-4.18.0-193.23.1.el8_2.x86_64 kernel-devel-4.18.0-193.23.1.el8_2.x86_64","text":"<p>The package kernel-headers-4.18.0-193.23.1.el8_2.x86_64 is only available in repo:</p> <ul> <li>rhocp-4.5-for-rhel-8-x86_64-rpms</li> <li>rhocp-4.3-for-rhel-8-x86_64-rpms</li> </ul> <p>Try to install by hand:</p> <pre><code>$ oc debug nvidia-driver-daemonset-95bfc\nStarting pod/nvidia-driver-daemonset-95bfc-debug, command was: nvidia-driver init\nPod IP: 10.131.0.24\nIf you don't see a command prompt, try pressing enter.\nsh-4.4# dnf -q -y install kernel-headers-4.18.0-193.23.1.el8_2.x86_64 kernel-devel-4.18.0-193.23.1.el8_2.x86_64\nError: Unable to find a match: kernel-headers-4.18.0-193.23.1.el8_2.x86_64 kernel-devel-4.18.0-193.23.1.el8_2.x86_64\nsh-4.4# dnf install --enablerepo=rhocp-4.5-for-rhel-8-x86_64-rpms -q -y kernel-headers-4.18.0-193.23.1.el8_2.x86_64 kernel-devel-4.18.0-193.23.1.el8_2.x86_6\nsh-4.4#\n</code></pre> <p>Adding <code>--enablerepo=rhocp-4.5-for-rhel-8-x86_64-rpms</code> solve the problem, let's patch the driver image.</p>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-debuging/#patching-driver-image","title":"Patching driver image","text":"<p>Fork the repo https://gitlab.com/nvidia/container-images/driver</p> <p>Warning</p> <p>This is the upstream version and will be different from the product. Might be hard to get it running.</p> <p>Adjust the script <code>rhel8/nvidia-driver</code> in your fork.</p> <p>Build the container image:</p> <pre><code>oc create -n gpu-operator-resources is/nvidia-driver\noc apply -n gpu-operator-resources -f - &lt;&lt;EOF\napiVersion: build.openshift.io/v1\nkind: BuildConfig\nmetadata:\n  name: nvidia-driver\n  labels:\n    name: nvidia-driver\nspec:\n  triggers:\n    - type: ConfigChange\n  source:\n    contextDir: \"rhel8/\"\n    type: Git\n    git:\n      uri: https://gitlab.com/rbohne/driver.git\n  strategy:\n    type: Docker\n    dockerStrategy:\n      env:\n      - name: DRIVER_VERSION\n        value: 440.64.00\n      buildArgs:\n      - name: DRIVER_VERSION\n        value: 440.64.00\n  output:\n    to:\n      kind: ImageStreamTag\n      name: 'nvidia-driver:440.64.00-rhel8.2'\nEOF\n</code></pre> <p>Update the clusterpolicy/cluster-policy  <code>oc edit clusterpolicy/cluster-policy</code> to</p> <pre><code>spec:\n...\n  driver:\n    image: nvidia-driver\n    repository: image-registry.openshift-image-registry.svc:5000/gpu-operator-resources\n    version: 440.64.00\n...\n</code></pre>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-debuging/#problems-with-ocp-4411","title":"Problems with OCP &gt;=4.4.11","text":"<ul> <li>Enable OpenShift Extended Update Support</li> </ul> <p>From nvidia-driver-daemonset, from (Server Version: 4.4.12, 4.18.0-147.20.1.el8_1.x86_64):</p> <pre><code>...\nInstalling Linux kernel headers...\n+ dnf -q -y install kernel-headers-4.18.0-147.20.1.el8_1.x86_64 kernel-devel-4.18.0-147.20.1.el8_1.x86_64\nError: Unable to find a match: kernel-headers-4.18.0-147.20.1.el8_1.x86_64 kernel-devel-4.18.0-147.20.1.el8_1.x86_64\n...\n</code></pre> OpenShift Version CoreOS Version Kernel Version Kernel Header available in repo <code>4.4.5</code> <code>44.81.202005180831-0</code> <code>4.18.0-147.8.1.el8_1.x86_64</code> rhel-8-for-x86_64-baseos-rpms,... <code>4.4.12</code> <code>44.81.202007070223-0</code> <code>4.18.0-147.20.1.el8_1.x86_64</code> rhel-8-for-x86_64-baseos-eus-rpms (8.1) <code>4.5.2</code> <code>45.82.202007141718-0</code> <code>4.18.0-193.13.2.el8_2.x86_64</code> rhel-8-for-x86_64-baseos-rpms,... <p>Get OS Version of OpenShift Release</p> <pre><code>$ oc adm release info 4.5.3 -o jsonpath=\"{.displayVersions.machine-os.Version}\"\n45.82.202007171855-0\n</code></pre> <p>Red Hat Internal Links: OpenShift Release page =&gt; 45.82.202007171855-0 =&gt; OS Content</p>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-debuging/#problems-with-openshift-45x","title":"Problems with OpenShift 4.5.x","text":"<p>NVidia does not provide a suitable CoreOS driver image.</p>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-debuging/#check-entitlements","title":"Check entitlements","text":"<pre><code># rct cat-cert export/entitlement_certificates/xxxxx.pem | grep rhel-8-for-x86_64-baseos-eus-rpms\n        Label: rhel-8-for-x86_64-baseos-eus-rpms\n\n# rct cat-cert export/entitlement_certificates/xxxxx.pem | grep rhocp-4.5-for-rhel-8-x86_64-rpms\n        Label: rhocp-4.5-for-rhel-8-x86_64-rpms\n\n# rct cat-cert export/entitlement_certificates/xxxxx.pem | grep SKU\n        SKU: SER0419\n</code></pre> <p>Source</p>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-aws/","title":"Install &amp; use GPU on AWS","text":"<p>Tested with OpenShift 4.4.5 at region <code>eu-central-1</code></p>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-aws/#install-openshift-4-on-aws","title":"Install OpenShift 4 on AWS","text":"<p>Follow the OpenShift documentation.</p> <p>Note</p> <p>Choose a region where GPUs are available.</p>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-aws/#create-machineset-with-an-gpu","title":"Create machineset with an GPU","text":"<p>You can create a new machineset as documented or copy an existing one.</p> <p>I use an existing one (required jq):</p>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-aws/#prepare-new-machineset-with-gpu","title":"Prepare new machineset with gpu","text":"<pre><code># Select the first machineset\nSOURCE_MACHINESET=$(oc get machineset -n openshift-machine-api -o name | head -n1)\n\n# Reformat with jq, for better diff result.\noc get -o json -n openshift-machine-api $SOURCE_MACHINESET  | jq -r &gt; /tmp/source-machineset.json\n\nOLD_MACHINESET_NAME=$(jq '.metadata.name' -r /tmp/source-machineset.json )\nNEW_MACHINESET_NAME=${OLD_MACHINESET_NAME/worker/worker-gpu}\n\n# Change instanceType and delete some stuff\njq -r '.spec.template.spec.providerSpec.value.instanceType = \"p3.2xlarge\"\n  | del(.metadata.selfLink)\n  | del(.metadata.uid)\n  | del(.metadata.creationTimestamp)\n  | del(.metadata.resourceVersion)\n  ' /tmp/source-machineset.json &gt; /tmp/gpu-machineset.json\n\n# Change machineset name\nsed -i \"s/$OLD_MACHINESET_NAME/$NEW_MACHINESET_NAME/g\" /tmp/gpu-machineset.json\n\n# Check changes via diff\ndiff -Nuar /tmp/source-machineset.json /tmp/gpu-machineset.json\n</code></pre>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-aws/#example-diff","title":"Example diff","text":"<pre><code>--- /tmp/source-machineset.json 2020-06-04 07:25:20.590127131 +0200\n+++ /tmp/gpu-machineset.json    2020-06-04 07:29:30.422321768 +0200\n@@ -2,23 +2,19 @@\n   \"apiVersion\": \"machine.openshift.io/v1beta1\",\n   \"kind\": \"MachineSet\",\n   \"metadata\": {\n-    \"creationTimestamp\": \"2020-06-04T04:36:57Z\",\n     \"generation\": 1,\n     \"labels\": {\n       \"machine.openshift.io/cluster-api-cluster\": \"demo-zwgq6\"\n     },\n-    \"name\": \"demo-zwgq6-worker-eu-central-1a\",\n-    \"namespace\": \"openshift-machine-api\",\n-    \"resourceVersion\": \"15003\",\n-    \"selfLink\": \"/apis/machine.openshift.io/v1beta1/namespaces/openshift-machine-api/machinesets/demo-zwgq6-worker-eu-central-1a\",\n-    \"uid\": \"168f3805-e1dc-4cc1-984a-a9000a5f037a\"\n+    \"name\": \"demo-zwgq6-worker-gpu-eu-central-1a\",\n+    \"namespace\": \"openshift-machine-api\"\n   },\n   \"spec\": {\n     \"replicas\": 1,\n     \"selector\": {\n       \"matchLabels\": {\n         \"machine.openshift.io/cluster-api-cluster\": \"demo-zwgq6\",\n-        \"machine.openshift.io/cluster-api-machineset\": \"demo-zwgq6-worker-eu-central-1a\"\n+        \"machine.openshift.io/cluster-api-machineset\": \"demo-zwgq6-worker-gpu-eu-central-1a\"\n       }\n     },\n     \"template\": {\n@@ -28,7 +24,7 @@\n           \"machine.openshift.io/cluster-api-cluster\": \"demo-zwgq6\",\n           \"machine.openshift.io/cluster-api-machine-role\": \"worker\",\n           \"machine.openshift.io/cluster-api-machine-type\": \"worker\",\n-          \"machine.openshift.io/cluster-api-machineset\": \"demo-zwgq6-worker-eu-central-1a\"\n+          \"machine.openshift.io/cluster-api-machineset\": \"demo-zwgq6-worker-gpu-eu-central-1a\"\n         }\n       },\n       \"spec\": {\n@@ -57,7 +53,7 @@\n             \"iamInstanceProfile\": {\n               \"id\": \"demo-zwgq6-worker-profile\"\n             },\n-            \"instanceType\": \"m4.large\",\n+            \"instanceType\": \"p3.2xlarge\",\n             \"kind\": \"AWSMachineProviderConfig\",\n             \"metadata\": {\n               \"creationTimestamp\": null\n</code></pre>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-aws/#create-machine-set","title":"Create machine set","text":"<pre><code>oc create -f /tmp/gpu-machineset.json\n</code></pre> <p>Wait until node is available...</p>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-aws/#entitle-your-openshift-cluster","title":"Entitle your openshift cluster","text":"<p>If not you run into: Bug 1835446 - Special resource operator gpu-driver-container pod error related to elfutils-libelf-devel</p> <p>These instructions assume you downloaded an entitlement encoded in base64 from access.redhat.com or extracted it from an existing node.</p> <p>In the following commands, the entitlement certificate is copied to nvidia.pem, but it can be copied to any accessible location. <pre><code># On RHEL8 machine pick entitlement from  /etc/pki/entitlement/\n# If you like to check the entitlement rct cat-cert /etc/pki/entitlement/xxx.pem\n\ncat /etc/pki/entitlement/xxxxxx*.pem &gt; nvidia.pem\n\ncurl -O  https://raw.githubusercontent.com/openshift-psap/blog-artifacts/master/how-to-use-entitled-builds-with-ubi/0003-cluster-wide-machineconfigs.yaml.template\n\nsed -i -f - 0003-cluster-wide-machineconfigs.yaml.template &lt;&lt; EOF\ns/BASE64_ENCODED_PEM_FILE/$(base64 -w0 nvidia.pem)/g\nEOF\n\noc apply -f 0003-cluster-wide-machineconfigs.yaml.template\n</code></pre> Based on nvidia docs</p>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-aws/#wait-for-machineconfigpool-is-updated","title":"Wait for machineconfigpool is updated","text":"<pre><code>oc wait --timeout=1800s --for=condition=Updated machineconfigpool/worker\n# or check via oc get\noc get machineconfigpool/worker\n</code></pre>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-aws/#install-nvidia-gpu-operator","title":"Install NVIDIA Gpu Operator","text":"<p>NVIDIA Documentation: OpenShift on NVIDIA GPU Accelerated Clusters</p>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-aws/#create-new-projectnamespace","title":"Create new project/namespace","text":"<pre><code>oc new-project gpu-operator-resources\n</code></pre>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-aws/#install-operators","title":"Install Operators","text":"<p>Actions:</p> <ul> <li>Install NVIDIA Operator (installs the Node Feature Discovery operator as a dependency.)</li> <li>Instantiate Node Feature Discovery Operator</li> </ul>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-aws/#instantiate-nvidia-gpu-operator","title":"Instantiate NVIDIA Gpu Operator","text":"<pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: nvidia.com/v1\nkind: ClusterPolicy\nmetadata:\n  name: cluster-policy\nspec:\n  dcgmExporter:\n    image: dcgm-exporter\n    repository: nvidia\n    version: 1.7.2-2.0.0-rc.11-ubi8\n  devicePlugin:\n    image: k8s-device-plugin\n    repository: nvidia\n    version: 1.0.0-beta6-ubi8\n  driver:\n    image: driver\n    repository: nvidia\n    version: 440.64.00\n  operator:\n    defaultRuntime: crio\n  toolkit:\n    image: container-toolkit\n    repository: nvidia\n    version: 1.0.2-ubi8\nEOF\n</code></pre>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-aws/#check-if-all-pods-are-running-or-completed","title":"Check if all pods are running or completed","text":"<pre><code>$ oc get pods\nNAME                                       READY   STATUS      RESTARTS   AGE\ngpu-operator-76d9bd6c65-7782f              1/1     Running     0          33m\nnfd-master-jkdmw                           1/1     Running     0          81m\nnfd-master-l82dp                           1/1     Running     0          81m\nnfd-master-sdggd                           1/1     Running     0          81m\nnfd-operator-684fcd5c8d-p7qcc              1/1     Running     0          82m\nnfd-worker-6btp5                           1/1     Running     0          81m\nnfd-worker-ch56g                           1/1     Running     0          81m\nnfd-worker-jc572                           1/1     Running     1          81m\nnfd-worker-llggg                           1/1     Running     0          81m\nnvidia-container-toolkit-daemonset-jbv2b   1/1     Running     0          82m\nnvidia-dcgm-exporter-mfnb9                 1/1     Running     0          2m23s\nnvidia-device-plugin-daemonset-wdfhq       1/1     Running     0          82m\nnvidia-device-plugin-validation            0/1     Completed   0          18m\nnvidia-driver-daemonset-2tzpb              1/1     Running     0          29m\nnvidia-driver-validation                   0/1     Completed   0          32m\n</code></pre> <p>Note</p> <p>Pod <code>nvidia-device-plugin-validation</code> stuck in Pending, problem was my gpu node had no capacity nvidia.com/gpu. A reboot of the node helped.</p> <p><code>oc debug node/...</code></p> <p><code>chroot /host</code></p> <p><code>reboot</code></p>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-aws/#run-test-workload","title":"Run test workload","text":"<pre><code>$ oc create -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nvidia-smi\nspec:\n  containers:\n  - image: nvidia/cuda\n    name: nvidia-smi\n    command: [ nvidia-smi ]\n    resources:\n      limits:\n        nvidia.com/gpu: 1\n      requests:\n        nvidia.com/gpu: 1\nEOF\n\n$ oc logs nvidia-smi\nThu Jun  4 07:24:52 2020\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n| N/A   32C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-prem/","title":"Install &amp; use GPU on-prem","text":"<p>Official solution: How to install the NVIDIA GPU Operator with OpenShift</p>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-prem/#entitle-your-openshift-cluster","title":"Entitle your openshift cluster","text":"<p>If not you run into: Bug 1835446 - Special resource operator gpu-driver-container pod error related to elfutils-libelf-devel</p> <p>These instructions assume you downloaded an entitlement encoded in base64 from access.redhat.com or extracted it from an existing node.</p> <p>In the following commands, the entitlement certificate is copied to nvidia.pem, but it can be copied to any accessible location. <pre><code># On RHEL8 machine pick entitlement from  /etc/pki/entitlement/\n# If you like to check the entitlement rct cat-cert /etc/pki/entitlement/xxx.pem\n\ncat /etc/pki/entitlement/xxxxxx*.pem &gt; nvidia.pem\n\ncurl -O  https://raw.githubusercontent.com/openshift-psap/blog-artifacts/master/how-to-use-entitled-builds-with-ubi/0003-cluster-wide-machineconfigs.yaml.template\n\nsed -i -f - 0003-cluster-wide-machineconfigs.yaml.template &lt;&lt; EOF\ns/BASE64_ENCODED_PEM_FILE/$(base64 -w0 nvidia.pem)/g\nEOF\n\noc apply -f 0003-cluster-wide-machineconfigs.yaml.template\n</code></pre> Based on nvidia docs</p>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-prem/#wait-for-machineconfigpool-is-updated","title":"Wait for machineconfigpool is updated","text":"<pre><code>oc wait --timeout=1800s --for=condition=Updated machineconfigpool/worker\n# or check via oc get\noc get machineconfigpool/worker\n</code></pre>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-prem/#install-nvidia-gpu-operator","title":"Install NVIDIA Gpu Operator","text":"<p>NVIDIA Documentation: OpenShift on NVIDIA GPU Accelerated Clusters</p>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-prem/#create-new-projectnamespace","title":"Create new project/namespace","text":"<pre><code>oc new-project gpu-operator-resources\n</code></pre>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-prem/#install-operators","title":"Install Operators","text":"<p>Actions:</p> <ul> <li>Install NVIDIA Operator (installs the Node Feature Discovery operator as a dependency.)</li> <li>Instantiate Node Feature Discovery Operator</li> </ul>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-prem/#instantiate-nvidia-gpu-operator","title":"Instantiate NVIDIA Gpu Operator","text":"<pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: nvidia.com/v1\nkind: ClusterPolicy\nmetadata:\n  name: cluster-policy\nspec:\n  dcgmExporter:\n    image: dcgm-exporter\n    repository: nvidia\n    version: 1.7.2-2.0.0-rc.11-ubi8\n  devicePlugin:\n    image: k8s-device-plugin\n    repository: nvidia\n    version: 1.0.0-beta6-ubi8\n  driver:\n    image: driver\n    repository: nvidia\n    version: 440.64.00\n  operator:\n    defaultRuntime: crio\n  toolkit:\n    image: container-toolkit\n    repository: nvidia\n    version: 1.0.2-ubi8\nEOF\n</code></pre>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-prem/#check-if-all-pods-are-running-or-completed","title":"Check if all pods are running or completed","text":"<pre><code>$ oc get pods\nNAME                                       READY   STATUS      RESTARTS   AGE\ngpu-operator-76d9bd6c65-7782f              1/1     Running     0          33m\nnfd-master-jkdmw                           1/1     Running     0          81m\nnfd-master-l82dp                           1/1     Running     0          81m\nnfd-master-sdggd                           1/1     Running     0          81m\nnfd-operator-684fcd5c8d-p7qcc              1/1     Running     0          82m\nnfd-worker-6btp5                           1/1     Running     0          81m\nnfd-worker-ch56g                           1/1     Running     0          81m\nnfd-worker-jc572                           1/1     Running     1          81m\nnfd-worker-llggg                           1/1     Running     0          81m\nnvidia-container-toolkit-daemonset-jbv2b   1/1     Running     0          82m\nnvidia-dcgm-exporter-mfnb9                 1/1     Running     0          2m23s\nnvidia-device-plugin-daemonset-wdfhq       1/1     Running     0          82m\nnvidia-device-plugin-validation            0/1     Completed   0          18m\nnvidia-driver-daemonset-2tzpb              1/1     Running     0          29m\nnvidia-driver-validation                   0/1     Completed   0          32m\n</code></pre> <p>Note</p> <p>Pod <code>nvidia-device-plugin-validation</code> stuck in Pending, problem was my gpu node had no capacity nvidia.com/gpu. A reboot of the node helped.</p> <p><code>oc debug node/...</code></p> <p><code>chroot /host</code></p> <p><code>reboot</code></p>","tags":["GPU"]},{"location":"cluster-installation/gpu/gpu-on-prem/#run-test-workload","title":"Run test workload","text":"<pre><code>$ oc create -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nvidia-smi\nspec:\n  containers:\n  - image: nvidia/cuda\n    name: nvidia-smi\n    command: [ nvidia-smi ]\n    resources:\n      limits:\n        nvidia.com/gpu: 1\n      requests:\n        nvidia.com/gpu: 1\nEOF\n\n$ oc logs nvidia-smi\nThu Jun  4 07:24:52 2020\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n| N/A   32C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>","tags":["GPU"]},{"location":"cluster-installation/hosted-control-plane/","title":"Hosted Control Plane","text":"<ul> <li>Upstream documation: https://hypershift-docs.netlify.app/</li> <li>Downstream documentation: https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/hosted_control_planes/index</li> </ul>","tags":["HostedControlPlane","hcp","hypershift"]},{"location":"cluster-installation/hosted-control-plane/#platform","title":"Platform","text":"","tags":["HostedControlPlane","hcp","hypershift"]},{"location":"cluster-installation/hosted-control-plane/#platform-kubevirt","title":"Platform - KubeVirt","text":"<p>Quick'n'dirty notes:</p> <pre><code>export PULL_SECRET=${HOME}/redhat-pullsecret-rh-ee-rbohne.json\nexport KUBEVIRT_CLUSTER_NAME=oat23\nexport TRUSTED_BUNDLE=${HOME}/Devel/gitlab.consulting.redhat.com/coe-lab/certificates/ca-bundle-v1.pem\n\nhcp create cluster \\\nkubevirt \\\n  --name $KUBEVIRT_CLUSTER_NAME \\\n  --namespace rbohne-hcp \\\n  --node-pool-replicas=2 \\\n  --memory '16Gi' \\\n  --cores '8' \\\n  --generate-ssh \\\n  --root-volume-size 120 \\\n  --root-volume-storage-class 'coe-netapp-nas' \\\n  --pull-secret $PULL_SECRET \\\n  --etcd-storage-class ocs-storagecluster-ceph-rbd \\\n  --control-plane-availability-policy HighlyAvailable \\\n  --additional-trust-bundle $TRUSTED_BUNDLE \\\n  --auto-repair \\\n  --release-image=quay.io/openshift-release-dev/ocp-release:4.14.1-x86_64\n  # --render\n  # Optional - add --render to show yaml\n</code></pre>","tags":["HostedControlPlane","hcp","hypershift"]},{"location":"cluster-installation/hosted-control-plane/#export-kubeconfig","title":"Export kubeconfig","text":"<pre><code>hcp create kubeconfig \\\n  --name $KUBEVIRT_CLUSTER_NAME \\\n  --namespace rbohne-hcp | sed \"s/admin/$KUBEVIRT_CLUSTER_NAME/\" &gt; ~/.kube/clusters/${KUBEVIRT_CLUSTER_NAME}\n</code></pre>","tags":["HostedControlPlane","hcp","hypershift"]},{"location":"cluster-installation/hosted-control-plane/#platform-none-baremetal","title":"Platform - None / BareMetal","text":"<p>Not tested, for a long time:</p> <pre><code>hcp create cluster \\\nnone \\\n  --expose-through-load-balancer \\\n  --name $KUBEVIRT_CLUSTER_NAME \\\n  --control-plane-availability-policy HighlyAvailable \\\n  --etcd-storage-class ocs-storagecluster-ceph-rbd \\\n  --release-image=quay.io/openshift-release-dev/ocp-release:4.12.1-x86_64 \\\n  --pull-secret $PULL_SECRET\n  # --render\n</code></pre>","tags":["HostedControlPlane","hcp","hypershift"]},{"location":"cluster-installation/hosted-control-plane/#loadbalacner-for-ingress","title":"Loadbalacner for ingress","text":"<p>Ingress is running on physical nodes, you have to provide an external load balancer.</p> <p>Here a container solution bases on openshift-4-loadbalancer</p> <ul> <li>Create new project</li> <li>Create service account <code>privileged</code> : <code>oc create sa privileged</code></li> <li>Grant scc <code>privileged</code> to service account <code>privileged</code></li> <li>Download and edit line 35 &amp; 37 with your BareMetal endpoints</li> </ul> Download, edit and applyopenshift-4-loadbalancer-deployment.yaml <pre><code>curl -L -O  https://examples.openshift.pub/pr-133/cluster-installation/hosted-control-plane/hosted-control-plane/openshift-4-loadbalancer-deployment.yaml\n$EDITOR openshift-4-loadbalancer-deployment.yaml\noc apply -f openshift-4-loadbalancer-deployment.yaml\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: openshift-4-loadbalancer\n    app.kubernetes.io/component: openshift-4-loadbalancer\n    app.kubernetes.io/instance: openshift-4-loadbalancer\n    app.kubernetes.io/name: openshift-4-loadbalancer\n    app.kubernetes.io/part-of: openshift-4-loadbalancer\n    app.openshift.io/runtime: haproxy\n  name: openshift-4-loadbalancer\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 2\n  selector:\n    matchLabels:\n      app: openshift-4-loadbalancer\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: openshift-4-loadbalancer\n        deployment: openshift-4-loadbalancer\n    spec:\n      containers:\n        - image: quay.io/redhat-emea-ssa-team/openshift-4-loadbalancer:latest\n          imagePullPolicy: Always\n          name: openshift-4-loadbalancer\n          env:\n            - name: INGRESS_HTTP\n              value: \"ucs56-0=10.32.96.56:80,ucs57-0=10.32.96.57:80\"\n            - name: INGRESS_HTTPS\n              value: \"ucs56-0=10.32.96.56:443,ucs57-0=10.32.96.57:443\"\n          ports:\n            - containerPort: 22623\n              protocol: TCP\n            - containerPort: 443\n              protocol: TCP\n            - containerPort: 6443\n              protocol: TCP\n            - containerPort: 80\n              protocol: TCP\n            - containerPort: 1984\n              protocol: TCP\n          resources: {}\n          securityContext:\n            privileged: true\n            runAsUser: 0\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n      dnsPolicy: ClusterFirst\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      securityContext: {}\n      serviceAccount: privileged\n      serviceAccountName: privileged\n      terminationGracePeriodSeconds: 30\n</code></pre> <ul> <li>Apply service type load balancer</li> </ul> Applyopenshift-4-loadbalancer-deployment.yaml <pre><code>oc apply -f  https://examples.openshift.pub/pr-133/cluster-installation/hosted-control-plane/hosted-control-plane/openshift-4-loadbalancer-service.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: openshift-4-loadbalancer\nspec:\n  type: LoadBalancer\n  selector:\n    app: openshift-4-loadbalancer\n    deployment: openshift-4-loadbalancer\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n      name: http\n    - protocol: TCP\n      port: 443\n      targetPort: 443\n      name: https\n    - protocol: TCP\n      port: 1984\n      targetPort: 1984\n      name: stats\n</code></pre>","tags":["HostedControlPlane","hcp","hypershift"]},{"location":"cluster-installation/hosted-control-plane/#trouble-shooting","title":"Trouble shooting","text":"<p>https://hypershift-docs.netlify.app/how-to/troubleshooting-general/</p> <pre><code>export KUBEVIRT_CLUSTER_NAME=lenggries3\nexport CLUSTERNS=\"rbohne-hcp\"\n\nmkdir clusterDump-${CLUSTERNS}-${KUBEVIRT_CLUSTER_NAME}\nhcp dump cluster \\\n    --name ${KUBEVIRT_CLUSTER_NAME} \\\n    --namespace ${CLUSTERNS} \\\n    --dump-guest-cluster \\\n    --artifact-dir clusterDump-${CLUSTERNS}-${KUBEVIRT_CLUSTER_NAME}\n</code></pre>","tags":["HostedControlPlane","hcp","hypershift"]},{"location":"cluster-installation/hosted-control-plane/kubevirt-networking/","title":"Hosted Control Plane - KubeVirt Networking","text":"","tags":["HostedControlPlane","hcp","hypershift","cnv","kubevirt","ocp-v"]},{"location":"cluster-installation/hosted-control-plane/kubevirt-networking/#introduction","title":"Introduction","text":"<p>This guide demonstrates how to configure advanced networking for Hosted Control Plane (HCP) clusters running on KubeVirt infrastructure. It covers the setup of VLAN-isolated networks, bridge interfaces, and MetalLB load balancing to provide networking for guest OpenShift clusters.</p>","tags":["HostedControlPlane","hcp","hypershift","cnv","kubevirt","ocp-v"]},{"location":"cluster-installation/hosted-control-plane/kubevirt-networking/#use-case","title":"Use Case","text":"<p>This configuration addresses scenarios where you need to:</p> <ul> <li>Isolate hosted clusters using dedicated VLANs for security and network segmentation</li> <li>Provide external connectivity for hosted cluster workloads through dedicated network interfaces</li> <li>Enable load balancing with MetalLB for ingress traffic to hosted clusters</li> <li>Support multiple hosted clusters on the same management cluster with proper network isolation</li> <li>Integrate with existing enterprise networks that require VLAN tagging and specific IP ranges</li> </ul> <p>This approach is particularly useful in enterprise environments where network isolation, compliance requirements, and integration with existing network infrastructure are critical for running multiple tenant clusters.</p> <p>Tested with:</p> Component Version OpenShift v4.18 OpenShift Virt v4.18 <p></p>","tags":["HostedControlPlane","hcp","hypershift","cnv","kubevirt","ocp-v"]},{"location":"cluster-installation/hosted-control-plane/kubevirt-networking/#test-cluster-information","title":"Test cluster information","text":"","tags":["HostedControlPlane","hcp","hypershift","cnv","kubevirt","ocp-v"]},{"location":"cluster-installation/hosted-control-plane/kubevirt-networking/#nodes","title":"Nodes","text":"NAME STATUS ROLES Second interface ucs55 Ready control-plane,master,virt-node,worker enp79s0f1 ucs56 Ready control-plane,master,virt-node,worker enp80s0f1 ucs57 Ready control-plane,master,virt-node,worker enp80s0f1","tags":["HostedControlPlane","hcp","hypershift","cnv","kubevirt","ocp-v"]},{"location":"cluster-installation/hosted-control-plane/kubevirt-networking/#cluster-network-configuration","title":"Cluster network configuration","text":"<pre><code>oc edit network.operator\n</code></pre> <p>Change/Add settings:</p> <pre><code>spec:\n  defaultNetwork:\n    ovnKubernetesConfig:\n      gatewayConfig:\n        ipForwarding: Global\n        routingViaHost: true\n</code></pre>","tags":["HostedControlPlane","hcp","hypershift","cnv","kubevirt","ocp-v"]},{"location":"cluster-installation/hosted-control-plane/kubevirt-networking/#prepare-node-interface","title":"Prepare node interface","text":"NodeNetworkConfigurationPolicy/coe-bridge-via-enp79s0f1 <pre><code>apiVersion: nmstate.io/v1\nkind: NodeNetworkConfigurationPolicy\nmetadata:\n  name: coe-bridge-via-enp79s0f1\nspec:\n  desiredState:\n    interfaces:\n      - name: enp79s0f1.2003\n        type: vlan\n        state: up\n        vlan:\n          base-iface: enp79s0f1\n          id: 2003\n        ipv4:\n          enabled: false\n        ipv6:\n          enabled: false\n      - bridge:\n          options:\n            stp:\n              enabled: false\n          port:\n            - name: enp79s0f1.2003\n        description: Linux Brige info COE Network via enp79s0f1.2003\n        ipv4:\n          enabled: true\n          dhcp: true\n        ipv6:\n          enabled: false\n        name: br-vlan-2003\n        state: up\n        type: linux-bridge\n      - bridge:\n          options:\n            stp:\n              enabled: false\n          port:\n            - name: enp79s0f1\n        description: Linux Brige info COE Network via enp79s0f1\n        ipv4:\n          enabled: false\n        name: coe-bridge\n        state: up\n        type: linux-bridge\n  nodeSelector:\n    coe.muc.redhat.com/second-nic: enp79s0f1\n</code></pre> NodeNetworkConfigurationPolicy/coe-bridge-via-enp80s0f1 <pre><code>apiVersion: nmstate.io/v1\nkind: NodeNetworkConfigurationPolicy\nmetadata:\n  name: coe-bridge-via-enp80s0f1\nspec:\n  desiredState:\n    interfaces:\n      - name: enp80s0f1.2003\n        type: vlan\n        state: up\n        vlan:\n          base-iface: enp80s0f1\n          id: 2003\n        ipv4:\n          enabled: false\n        ipv6:\n          enabled: false\n      - bridge:\n          options:\n            stp:\n              enabled: false\n          port:\n            - name: enp80s0f1.2003\n        description: Linux Brige info COE Network via enp80s0f1.2003\n        ipv4:\n          enabled: true\n          dhcp: true\n        ipv6:\n          enabled: false\n        name: br-vlan-2003\n        state: up\n        type: linux-bridge\n      - bridge:\n          options:\n            stp:\n              enabled: false\n          port:\n            - name: enp80s0f1\n        description: Linux Brige info COE Network via enp80s0f1\n        ipv4:\n          enabled: false\n        name: coe-bridge\n        state: up\n        type: linux-bridge\n  nodeSelector:\n    coe.muc.redhat.com/second-nic: enp80s0f1\n</code></pre> NetworkAttachmentDefinition/br-vlan-2003 <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  annotations:\n    k8s.v1.cni.cncf.io/resourceName: bridge.network.kubevirt.io/br-vlan-2003\n  name: br-vlan-2003\n  namespace: default\nspec:\n  config: |-\n    {\n        \"cniVersion\": \"0.3.1\",\n        \"name\": \"br-vlan-2003\",\n        \"type\": \"bridge\",\n        \"bridge\": \"br-vlan-2003\",\n        \"ipam\": {},\n        \"macspoofchk\": false,\n        \"preserveDefaultVlan\": false\n    }\n</code></pre>","tags":["HostedControlPlane","hcp","hypershift","cnv","kubevirt","ocp-v"]},{"location":"cluster-installation/hosted-control-plane/kubevirt-networking/#metallb","title":"MetalLB","text":"<ul> <li>Documetation, second interface</li> </ul> IPAddressPool <pre><code>apiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: coe-2003\n  namespace: metallb-system\nspec:\n  addresses:\n    - 192.168.203.10-192.168.203.19\n  autoAssign: true\n  avoidBuggyIPs: false\n</code></pre> <ul> <li>Optional add label selector to use the pool only hypershift/hcp</li> </ul> L2Advertisement <pre><code>apiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  annotations:\n  name: coe-lab\n  namespace: metallb-system\nspec:\n  ipAddressPools:\n    - coe-2003\n  interfaces:\n    - enp79s0f1.2003\n    - enp80s0f1.2003\n</code></pre> <ul> <li>Optional use a nodeSelector to run on specific nodes</li> <li>Optional add specific interface</li> </ul>","tags":["HostedControlPlane","hcp","hypershift","cnv","kubevirt","ocp-v"]},{"location":"cluster-installation/hosted-control-plane/kubevirt-networking/#start-hosted-cluster","title":"Start hosted cluster","text":"<pre><code>export PULL_SECRET=${HOME}/redhat-pullsecret-rh-ee-rbohne.json\nexport KUBEVIRT_CLUSTER_NAME=hcp-2003-2\nexport TRUSTED_BUNDLE=${HOME}/Devel/gitlab.consulting.redhat.com/coe-lab/certificates/ca-bundle-v2.pem\n\nhcp create cluster kubevirt \\\n  --name $KUBEVIRT_CLUSTER_NAME \\\n  --namespace clusters \\\n  --node-pool-replicas=2 \\\n  --memory '16Gi' \\\n  --cores '8' \\\n  --generate-ssh \\\n  --root-volume-size 120 \\\n  --root-volume-storage-class 'ocs-storagecluster-ceph-rbd-virtualization' \\\n  --pull-secret $PULL_SECRET \\\n  --etcd-storage-class ocs-storagecluster-ceph-rbd \\\n  --control-plane-availability-policy HighlyAvailable \\\n  --additional-trust-bundle $TRUSTED_BUNDLE \\\n  --release-image=quay.io/openshift-release-dev/ocp-release:4.18.13-x86_64 \\\n  --attach-default-network=false \\\n  --additional-network name:default/br-vlan-2003 \\\n  --external-dns-domain coe.muc.redhat.com\n</code></pre> Render yaml <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  creationTimestamp: null\n  name: clusters\nspec: {}\nstatus: {}\n---\napiVersion: v1\ndata:\n  ca-bundle.crt: |\n    xxx\nkind: ConfigMap\nmetadata:\n  creationTimestamp: null\n  name: user-ca-bundle\n  namespace: clusters\n---\napiVersion: hypershift.openshift.io/v1beta1\nkind: HostedCluster\nmetadata:\n  creationTimestamp: null\n  name: hcp-2003-2\n  namespace: clusters\nspec:\n  additionalTrustBundle:\n    name: user-ca-bundle\n  autoscaling: {}\n  configuration: {}\n  controllerAvailabilityPolicy: HighlyAvailable\n  dns:\n    baseDomain: \"\"\n  etcd:\n    managed:\n      storage:\n        persistentVolume:\n          size: 8Gi\n          storageClassName: ocs-storagecluster-ceph-rbd\n        type: PersistentVolume\n    managementType: Managed\n  fips: false\n  infraID: hcp-2003-2-t4x5c\n  networking:\n    clusterNetwork:\n      - cidr: 10.132.0.0/14\n    networkType: OVNKubernetes\n    serviceNetwork:\n      - cidr: 172.31.0.0/16\n  olmCatalogPlacement: management\n  platform:\n    kubevirt:\n      baseDomainPassthrough: true\n    type: KubeVirt\n  pullSecret:\n    name: hcp-2003-2-pull-secret\n  release:\n    image: quay.io/openshift-release-dev/ocp-release:4.18.13-x86_64\n  secretEncryption:\n    aescbc:\n      activeKey:\n        name: hcp-2003-2-etcd-encryption-key\n    type: aescbc\n  services:\n    - service: APIServer\n      servicePublishingStrategy:\n        type: Route\n    - service: Ignition\n      servicePublishingStrategy:\n        type: Route\n    - service: Konnectivity\n      servicePublishingStrategy:\n        type: Route\n    - service: OAuthServer\n      servicePublishingStrategy:\n        type: Route\n  sshKey:\n    name: hcp-2003-2-ssh-key\nstatus:\n  controlPlaneEndpoint:\n    host: \"\"\n    port: 0\n---\napiVersion: hypershift.openshift.io/v1beta1\nkind: NodePool\nmetadata:\n  creationTimestamp: null\n  name: hcp-2003-2\n  namespace: clusters\nspec:\n  arch: amd64\n  clusterName: hcp-2003-2\n  management:\n    autoRepair: false\n    upgradeType: Replace\n  nodeDrainTimeout: 0s\n  nodeVolumeDetachTimeout: 0s\n  platform:\n    kubevirt:\n      additionalNetworks:\n        - name: default/br-vlan-2003\n      attachDefaultNetwork: false\n      compute:\n        cores: 8\n        memory: 16Gi\n      networkInterfaceMultiqueue: Enable\n      rootVolume:\n        persistent:\n          size: 120Gi\n          storageClass: ocs-storagecluster-ceph-rbd-virtualization\n        type: Persistent\n    type: KubeVirt\n  release:\n    image: quay.io/openshift-release-dev/ocp-release:4.18.13-x86_64\n  replicas: 2\nstatus:\n  replicas: 0\n---\n</code></pre>","tags":["HostedControlPlane","hcp","hypershift","cnv","kubevirt","ocp-v"]},{"location":"cluster-installation/hosted-control-plane/kubevirt-networking/#configure-metallb-for-ingress-on-hosted-cluster","title":"Configure Metallb for Ingress on hosted cluster","text":"<p>Follow: Optional MetalLB Configuration Steps</p> Adjusted IPAddressPool <pre><code>apiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: metallb\n  namespace: metallb-system\nspec:\n  addresses:\n  - 192.168.203.31-192.168.203.35\n</code></pre>","tags":["HostedControlPlane","hcp","hypershift","cnv","kubevirt","ocp-v"]},{"location":"cluster-installation/hosted-control-plane/kubevirt-networking/#ingress-service","title":"Ingress Service","text":"<p>with RFE Enable preallocation of a predictable NodePort for Ingress it would be much easier.</p> Service/router-loadbalancer-default <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: router\n    ingresscontroller.operator.openshift.io/owning-ingresscontroller: default\n    router: router-loadbalancer-default\n  name: router-loadbalancer-default\n  namespace: openshift-ingress\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: https\n  selector:\n    ingresscontroller.operator.openshift.io/deployment-ingresscontroller: default\n  sessionAffinity: None\n  type: LoadBalancer\n</code></pre>","tags":["HostedControlPlane","hcp","hypershift","cnv","kubevirt","ocp-v"]},{"location":"cluster-installation/hosted-control-plane/kubevirt-networking/#dns","title":"DNS","text":"<p>Fetch information from hosted cluster:</p> <pre><code>[cloud-user@router ~]$ oc get svc -n openshift-ingress router-loadbalancer-default\nNAME                          TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)                      AGE\nrouter-loadbalancer-default   LoadBalancer   172.31.241.155   192.168.203.31   80:31572/TCP,443:30706/TCP   2m22s\n[cloud-user@router ~]$\n\n[cloud-user@router ~]$ oc get ingresscontroller -n openshift-ingress-operator default -o jsonpath=\"{.spec.domain}\";echo\napps.hcp1.apps.rhine.coe.muc.redhat.com\n[cloud-user@router ~]$\n</code></pre> <p>Create an <code>A</code> record based on the information:</p> <pre><code>*.apps.hcp1.apps.rhine.coe.muc.redhat.com. IN A 192.168.203.31\n</code></pre>","tags":["HostedControlPlane","hcp","hypershift","cnv","kubevirt","ocp-v"]},{"location":"cluster-installation/sno-on-ocpv/","title":"Single Node OpenShift on OpenShift Virtualization","text":"","tags":["ocp-v","kubevirt","cnv","sno","installation"]},{"location":"cluster-installation/sno-on-ocpv/#dns-records","title":"DNS Records","text":"<p>Create DNS Records to VM IP</p> <ul> <li>api-int.cluster-name.baseDomain</li> <li>api.cluster-name.baseDomain</li> <li>*.apps.cluster-name.baseDomain</li> </ul>","tags":["ocp-v","kubevirt","cnv","sno","installation"]},{"location":"cluster-installation/sno-on-ocpv/#installations","title":"Installations","text":"<p>Official documentation: https://docs.openshift.com/container-platform/latest/installing/installing_sno/install-sno-installing-sno.html</p> install-config.yamlDownload <pre><code>apiVersion: v1\nbaseDomain: coe.muc.redhat.com\ncompute:\n  - name: worker\n    replicas: 0\ncontrolPlane:\n  name: master\n  replicas: 1\nmetadata:\n  name: sno-on-ocpv\nnetworking:\n  clusterNetwork:\n    - cidr: 10.132.0.0/14\n      hostPrefix: 23\n  machineNetwork:\n    - cidr: 10.128.0.0/14\n  networkType: OVNKubernetes\n  serviceNetwork:\n    - 172.31.0.0/16\nplatform:\n  none: {}\nbootstrapInPlace:\n  installationDisk: /dev/vda\npullSecret: |\n  {\"auths\":{\"cloud.openshift.com\":'....\nsshKey: |\n  ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIEQM82o2imwpHyGVO7DxCNbdE0ZWnkp6oxdawb7/MOCT coe-muc\n  ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIAOfl+764UFbDkkxpsQYjET7ZAWoVApSf4I64L1KImoc rbohne@redhat.com\n</code></pre> <pre><code>curl -L -O https://examples.openshift.pub/pr-133/cluster-installation/sno-on-ocpv//install-config.yaml\n</code></pre>","tags":["ocp-v","kubevirt","cnv","sno","installation"]},{"location":"cluster-installation/sno-on-ocpv/#create-ignition-config-upload-it","title":"Create ignition config &amp; Upload it","text":"<pre><code>openshift-install --dir=ocp create single-node-ignition-config\n</code></pre>","tags":["ocp-v","kubevirt","cnv","sno","installation"]},{"location":"cluster-installation/sno-on-ocpv/#preare-installation-iso","title":"Preare installation ISO","text":"<pre><code>curl -L -O https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.14/4.14.0/rhcos-4.14.0-x86_64-live.x86_64.iso\n\nalias coreos-installer='podman run --privileged --pull always --rm -v /dev:/dev -v /run/udev:/run/udev -v $PWD:/data -w /data quay.io/coreos/coreos-installer:release'\n\ncoreos-installer iso ignition embed -fi ocp/bootstrap-in-place-for-live-iso.ign rhcos-4.14.0-x86_64-live.x86_64.iso\n\nvirtctl image-upload pvc live-iso-sno  \\\n  --size 2Gi --storage-class ocs-storagecluster-ceph-rbd \\\n  --access-mode ReadWriteOnce \\\n  --image-path rhcos-4.14.0-x86_64-live.x86_64.iso\n</code></pre>","tags":["ocp-v","kubevirt","cnv","sno","installation"]},{"location":"cluster-installation/sno-on-ocpv/#create-vm","title":"Create VM","text":"vm.yamloc apply <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  annotations:\n  labels:\n    app: sno1-rbohne-demo\n  name: sno1-rbohne-demo\nspec:\n  dataVolumeTemplates:\n    - metadata:\n        name: sno1-rbohne-demo-root\n      spec:\n        storage:\n          accessModes:\n            - ReadWriteMany\n          storageClassName: ocs-storagecluster-ceph-rbd-virtualization\n          resources:\n            requests:\n              storage: 120Gi\n        source:\n          blank: {}\n  running: true\n  template:\n    metadata:\n      labels:\n        kubevirt.io/domain: sno1-rbohne-demo\n    spec:\n      volumes:\n        - name: cdrom\n          persistentVolumeClaim:\n            claimName: live-iso-sno\n        - name: root\n          dataVolume:\n            name: sno1-rbohne-demo-root\n      networks:\n        - name: coe\n          multus:\n            networkName: coe-bridge\n      domain:\n        cpu:\n          cores: 8\n        memory:\n          guest: 64Gi\n        resources:\n          requests:\n            memory: 64Gi\n        devices:\n          disks:\n            - name: root\n              bootOrder: 1\n              disk:\n                bus: virtio\n            - name: cdrom\n              bootOrder: 2\n              cdrom:\n                bus: sata\n          interfaces:\n            - bridge: {}\n              model: virtio\n              name: coe\n</code></pre> <pre><code>oc apply -f https://examples.openshift.pub/pr-133/cluster-installation/sno-on-ocpv//vm.yaml\n</code></pre>","tags":["ocp-v","kubevirt","cnv","sno","installation"]},{"location":"cluster-installation/vmware/","title":"Cluster installation on VMware/vSphere","text":""},{"location":"cluster-installation/vmware/#permission-check","title":"Permission-check","text":"<p>Please check-out https://github.com/openshift-examples/vmware-permission-check</p>"},{"location":"cluster-installation/vmware/#content","title":"Content","text":""},{"location":"cluster-installation/vmware/ipi-proxy/","title":"vSphere IPI &amp; Proxy","text":"","tags":["Proxy","VMware","vSphere","IPI"]},{"location":"cluster-installation/vmware/ipi-proxy/#preperations","title":"Preperations","text":"<ul> <li>Download <code>oc</code> client - Red Hat Console</li> <li>Alternative source - Mirror OpenShift</li> <li>Download <code>openshift-installer</code> - Red Hat Console</li> <li> <p>Alternative source - Mirror OpenShift</p> </li> <li> <p>Install <code>oc</code> Openshift CLI</p> </li> <li>tar -zxvf openshift-client-mac.tar.gz</li> <li>chmod +x <code>oc</code></li> <li><code>sudo mv oc /usr/local/bin/oc</code></li> <li>Download and untar <code>openshift-installer</code></li> </ul> <p>Info</p> <p>vCenter must be reachable directly the connection does not go through the proxy.</p>","tags":["Proxy","VMware","vSphere","IPI"]},{"location":"cluster-installation/vmware/ipi-proxy/#install-vcenter-root-ca","title":"Install vCenter Root CA","text":"<pre><code>curl -k -L -O  https://vcenter/certs/download.zip\nunzip download.zip\ncp -v certs/lin/*  /etc/pki/ca-trust/source/anchors/\nupdate-ca-trust\n</code></pre>","tags":["Proxy","VMware","vSphere","IPI"]},{"location":"cluster-installation/vmware/ipi-proxy/#optional-reverse-proxy-for-vcenter-in-a-different-network","title":"Optional Reverse proxy for vCenter in a different Network","text":"<ul> <li>Install nginx</li> <li>Create certificates</li> </ul> <p>Nginx configuration:</p> <pre><code>http {\n  proxy_set_header            Host            $http_host;\n  proxy_set_header            X-Real-IP       $remote_addr;\n  proxy_set_header            X-Forwared-For  $proxy_add_x_forwarded_for;\n\n  upstream vcsa-443 {\n    server vcenter.mycomp.com:443;\n  }\n\n  server {\n    listen        80;\n    server_name   vcenter.openshift.pub;\n\n    location / {\n      allow all;\n      return 302 https://$server_name$request_uri;\n    }\n  }\n  server {\n    listen        443 ssl;\n    server_name   vcenter.openshift.pub;\n\n    ssl_certificate  /etc/nginx/cert-crt.pem;\n    ssl_certificate_key  /etc/nginx/cert-key.pem;\n    ssl_protocols  TLSv1 TLSv1.1 TLSv1.2;\n    ssl_ciphers    HIGH:!aNULL:!MD5;\n    keepalive_timeout 60;\n\n    location / {\n      allow all;\n      proxy_set_header Host $http_host;\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection \"upgrade\";\n      proxy_pass https://vcsa-443;\n    }\n  }\n}\n</code></pre>","tags":["Proxy","VMware","vSphere","IPI"]},{"location":"cluster-installation/vmware/ipi-proxy/#prepare-install-config","title":"Prepare Install Config","text":"<p>Via <code>openshift-install create install-config --dir=config</code></p> <p>Don't forget to add the proxy!</p>","tags":["Proxy","VMware","vSphere","IPI"]},{"location":"cluster-installation/vmware/ipi-proxy/#example-install-configyaml-dhcp","title":"Example install-config.yaml DHCP","text":"<pre><code>apiVersion: v1\nbaseDomain: openshift.pub\ncompute:\n- architecture: amd64\n  hyperthreading: Enabled\n  name: worker\n  platform: {}\n  replicas: 3\ncontrolPlane:\n  architecture: amd64\n  hyperthreading: Enabled\n  name: master\n  platform: {}\n  replicas: 3\nmetadata:\n  creationTimestamp: null\n  name: vde\nnetworking:\n  clusterNetwork:\n  - cidr: 10.128.0.0/14\n    hostPrefix: 23\n  machineNetwork:\n  - cidr: 10.0.0.0/16\n  networkType: OpenShiftSDN\n  serviceNetwork:\n  - 172.30.0.0/16\nplatform:\n  vsphere:\n    apiVIP: 172.16.0.15\n    cluster: vmware-cluster\n    datacenter: DC\n    defaultDatastore: datastore\n    ingressVIP: 172.16.0.16\n    network: VM Network\n    password: xxxxx\n    username: ocp-account\n    vCenter: vcenter.example.de\n    folder: /DC/vm/rbohne/                        &lt;===== Added\npublish: External\nproxy:                                            &lt;===== Added\n  httpProxy: http://172.16.0.1:3128/              &lt;===== Added # Sometimes also: http://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt;\n  httpsProxy: http://172.16.0.1:3128/             &lt;===== Added # Sometimes also: https://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt;\n  noProxy: 172.16.0.0/24,apps.vde.openshift.pub   &lt;===== Added # Must include &lt;Machine CIDR&gt; and &lt;vCenter IP&gt;\nsshKey: \".....\"                                   &lt;===== Added\npullSecret: '{\"auths\":{\"cloud.openshift.com\":....'\n</code></pre>","tags":["Proxy","VMware","vSphere","IPI"]},{"location":"cluster-installation/vmware/ipi-proxy/#example-install-configyaml-static-ips","title":"Example install-config.yaml Static IPs","text":"<pre><code>additionalTrustBundlePolicy: Always\napiVersion: v1\nbaseDomain: openshift.jarvis.lab\ncompute:\n- architecture: amd64\n  hyperthreading: Enabled\n  name: worker\n  platform: {}\n  replicas: 3\ncontrolPlane:\n  architecture: amd64\n  hyperthreading: Enabled\n  name: master\n  platform: {}\n  replicas: 3\nmetadata:\n  name: ocp-mk1\nnetworking:\n  clusterNetwork:\n  - cidr: 10.128.0.0/14\n    hostPrefix: 23\n  machineNetwork:\n  - cidr: 10.10.80.0/24   # same L2 as k8s-nodes\n  networkType: OVNKubernetes\n  serviceNetwork:\n  - 172.30.0.0/16\nplatform:\n  vsphere:\n    hosts:\n    - role: bootstrap\n      networkDevice:\n        ipAddrs:\n        - 10.10.80.99/24\n        gateway: 10.10.80.1\n        nameservers:\n        - 10.10.10.53\n    - role: control-plane\n      networkDevice:\n        ipAddrs:\n        - 10.10.80.151/24\n        gateway: 10.10.80.1\n        nameservers:\n        - 10.10.10.53\n    - role: control-plane\n      networkDevice:\n        ipAddrs:\n        - 10.10.80.152/24\n        gateway: 10.10.80.1\n        nameservers:\n        - 10.10.10.53\n    - role: control-plane\n      networkDevice:\n        ipAddrs:\n        - 10.10.80.153/24\n        gateway: 10.10.80.1\n        nameservers:\n        - 10.10.10.53\n    - role: compute\n      networkDevice:\n        ipAddrs:\n        - 10.10.80.154/24\n        gateway: 10.10.80.1\n        nameservers:\n        - 10.10.10.53\n    - role: compute\n      networkDevice:\n        ipAddrs:\n        - 10.10.80.155/24\n        gateway: 10.10.80.1\n        nameservers:\n        - 10.10.10.53\n    - role: compute\n      networkDevice:\n        ipAddrs:\n        - 10.10.80.156/24\n        gateway: 10.10.80.1\n        nameservers:\n        - 10.10.10.53\n    apiVIPs:\n    - 10.10.80.100\n    failureDomains:\n    - name: failure-domain\n      region: homelab-region\n      server: vcsa.jarvis.lab\n      topology:\n        computeCluster: /dc-1/host/cl-1\n        datacenter: dc-1\n        datastore: /dc-1/datastore/vsanDatastore\n        networks:\n        - 1080-wkld-net\n        resourcePool: /dc-1/host/cl-1/Resources/ocp-mk1\n        folder: /dc-1/vm/ocp-mk1\n      zone: zone1\n    ingressVIPs:\n    - 10.10.80.102\n    vcenters:\n    - datacenters:\n      - dc-1\n      password: ******\n      port: 443\n      server: vcsa.jarvis.lab\n      user: administrator@vsphere.local\npublish: External\nproxy:\n  httpProxy: http://&lt;proxy-ip&gt;:3128/ # Sometimes also: http://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt;\n  httpsProxy: http://&lt;proxy-ip&gt;:3128/ #Sometimes also: https://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt;\n  noProxy: &lt;Machine CIDR&gt;,&lt;vCenter IP&gt;,apps.openshift.jarvis.lab\npullSecret: '{\"auths\":{\"cloud.openshift.com\":...\nsshKey: |\n  ssh-rsa AAAAB3N...\n</code></pre>","tags":["Proxy","VMware","vSphere","IPI"]},{"location":"cluster-installation/vmware/ipi-proxy/#troubleshooting-the-installation","title":"Troubleshooting the Installation","text":"<p>The installation log messages are in $MY_CLUSTER/.openshift_install.log. If the installation fails, check the log for error messages and make changes to the environment accordingly. Then re-run the installation with the following command:</p> <p><code>tail -f $MY_CLUSTER/.openshift_install.log</code></p> <p><code>openshift-install wait-for install-complete</code></p>","tags":["Proxy","VMware","vSphere","IPI"]},{"location":"cluster-installation/vmware/ipi-proxy/#destroy-a-cluster","title":"Destroy a Cluster","text":"<p><code>openshift-install destroy cluster --dir=&lt;installation_directory&gt; --log-level=debug</code></p>","tags":["Proxy","VMware","vSphere","IPI"]},{"location":"cluster-installation/vmware/ipi-proxy/#explain-installconfigplatformvsphere","title":"Explain installconfig.platform.vsphere","text":"<pre><code>openshift-install explain installconfig.platform.vsphere\n\nKIND:     InstallConfig\nVERSION:  v1\n\nRESOURCE: &lt;object&gt;\n  VSphere is the configuration used when installing on vSphere.\n\nFIELDS:\n    apiVIP &lt;string&gt;\n      DeprecatedAPIVIP is the virtual IP address for the api endpoint Deprecated: Use APIVIPs\n\n    apiVIPs &lt;[]string&gt;\n      Format: ip\n      APIVIPs contains the VIP(s) for the api endpoint. In dual stack clusters it contains an IPv4 and IPv6 address, otherwise only one VIP\n\n    cluster &lt;string&gt;\n      Cluster is the name of the cluster virtual machines will be cloned into. Deprecated: Use FailureDomains.Topology.Cluster\n\n    clusterOSImage &lt;string&gt;\n      ClusterOSImage overrides the url provided in rhcos.json to download the RHCOS OVA\n\n    datacenter &lt;string&gt;\n      Datacenter is the name of the datacenter to use in the vCenter. Deprecated: Use FailureDomains.Topology.Datacenter\n\n    defaultDatastore &lt;string&gt;\n      DefaultDatastore is the default datastore to use for provisioning volumes. Deprecated: Use FailureDomains.Topology.Datastore\n\n    defaultMachinePlatform &lt;object&gt;\n      DefaultMachinePlatform is the default configuration used when installing on VSphere for machine pools which do not define their own platform configuration.\n\n    diskType &lt;string&gt;\n      Valid Values: \"\",\"thin\",\"thick\",\"eagerZeroedThick\"\n      DiskType is the name of the disk provisioning type, valid values are thin, thick, and eagerZeroedThick. When not specified, it will be set according to the default storage policy of vsphere.\n\n    failureDomains &lt;[]object&gt;\n      FailureDomains holds the VSpherePlatformFailureDomainSpec which contains the definition of region, zone and the vCenter topology. If this is omitted failure domains (regions and zones) will not be used.\n      FailureDomain holds the region and zone failure domain and the vCenter topology of that failure domain.\n\n    folder &lt;string&gt;\n      Folder is the absolute path of the folder that will be used and/or created for virtual machines. The absolute path is of the form /&lt;datacenter&gt;/vm/&lt;folder&gt;/&lt;subfolder&gt;. Deprecated: Use FailureDomains.Topology.Folder\n\n    hosts &lt;[]object&gt;\n      Hosts defines network configurations to be applied by the installer. Hosts is available in TechPreview.\n      Host defines host VMs to generate as part of the installation.\n\n    ingressVIP &lt;string&gt;\n      DeprecatedIngressVIP is the virtual IP address for ingress Deprecated: Use IngressVIPs\n\n    ingressVIPs &lt;[]string&gt;\n      Format: ip\n      IngressVIPs contains the VIP(s) for ingress. In dual stack clusters it contains an IPv4 and IPv6 address, otherwise only one VIP\n\n    loadBalancer &lt;object&gt;\n      LoadBalancer defines how the load balancer used by the cluster is configured. LoadBalancer is available in TechPreview.\n\n    network &lt;string&gt;\n      Network specifies the name of the network to be used by the cluster. Deprecated: Use FailureDomains.Topology.Network\n\n    password &lt;string&gt;\n      Password is the password for the user to use to connect to the vCenter. Deprecated: Use VCenters.Password\n\n    resourcePool &lt;string&gt;\n      ResourcePool is the absolute path of the resource pool where virtual machines will be created. The absolute path is of the form /&lt;datacenter&gt;/host/&lt;cluster&gt;/Resources/&lt;resourcepool&gt;. Deprecated: Use FailureDomains.Topology.ResourcePool\n\n    username &lt;string&gt;\n      Username is the name of the user to use to connect to the vCenter. Deprecated: Use VCenters.Username\n\n    vCenter &lt;string&gt;\n      VCenter is the domain name or IP address of the vCenter. Deprecated: Use VCenters.Server\n\n    vcenters &lt;[]object&gt;\n      VCenters holds the connection details for services to communicate with vCenter. Currently only a single vCenter is supported.\n      VCenter stores the vCenter connection fields https://github.com/kubernetes/cloud-provider-vsphere/blob/master/pkg/common/config/types_yaml.go\n</code></pre>","tags":["Proxy","VMware","vSphere","IPI"]},{"location":"cluster-installation/vmware/agent-base-non-integrated/","title":"Agent-based non-integrated installation on vSphere","text":"<p>https://docs.openshift.com/container-platform/4.13/installing/installing_with_agent_based_installer/preparing-to-install-with-agent-based-installer.html</p>","tags":["billy","Agent-based","non-integrated","vsphere","vmware"]},{"location":"cluster-installation/vmware/agent-base-non-integrated/#create-all-vms","title":"Create all VM's","text":"<p>For example with Govc:</p> <pre><code>govc vm.create \\\n  -on=false \\\n  -m 16384 \\\n  -c 8 \\\n  -net 'VM Network' \\\n  -disk 120GB \\\n  -ds ose3-vmware \\\n  -folder /Boston/vm/rbohne \\\n  -g rhel9_64Guest \\\n  cp-{1,2}\n..\n\nfor i in cp-{0,1,2} wp-{0,1}; do govc vm.change -vm /Boston/vm/rbohne/${i} -e disk.enableUUID=TRUE ;done\n\n# Just for information\n#for i in cp-{0,1,2} wp-{0,1}; do govc vm.power -off -force /Boston/vm/rbohne/${i} ;done\n#for i in cp-{0,1,2} wp-{0,1}; do govc vm.power -on  /Boston/vm/rbohne/${i} ;done\n</code></pre>","tags":["billy","Agent-based","non-integrated","vsphere","vmware"]},{"location":"cluster-installation/vmware/agent-base-non-integrated/#collect-mac-addresses","title":"Collect mac-addresses","text":"<pre><code>cp-0;00:50:56:89:5b:02\ncp-1;00:50:56:89:dd:e4\ncp-2;00:50:56:89:0d:fe\nwp-0;00:50:56:89:e0:2c\nwp-1;00:50:56:89:80:a3\n</code></pre>","tags":["billy","Agent-based","non-integrated","vsphere","vmware"]},{"location":"cluster-installation/vmware/agent-base-non-integrated/#get-rendezvousip","title":"Get rendezvousIP","text":"<p>Boot with RHEL/RHCOS Live ISO cp-0 and get IP from DHCP</p>","tags":["billy","Agent-based","non-integrated","vsphere","vmware"]},{"location":"cluster-installation/vmware/agent-base-non-integrated/#create-configuration","title":"Create configuration","text":"agent-config.yaml agent-config.yamlcurl <pre><code>apiVersion: v1alpha1\nkind: AgentConfig\nmetadata:\n  name: vmw1\nrendezvousIP: 10.28.115.123\nhosts:\n  - hostname: cp-0\n    role: master\n    interfaces:\n      - name: ens32\n        macAddress: 00:50:56:89:5b:02\n  - hostname: cp-1\n    role: master\n    interfaces:\n      - name: ens32\n        macAddress: 00:50:56:89:dd:e4\n  - hostname: cp-2\n    role: master\n    interfaces:\n      - name: ens32\n        macAddress: 00:50:56:89:0d:fe\n  - hostname: wp-0\n    role: worker\n    interfaces:\n      - name: ens32\n        macAddress: 00:50:56:89:e0:2c\n  - hostname: wp-1\n    role: worker\n    interfaces:\n      - name: ens32\n        macAddress: 00:50:56:89:80:a3\n</code></pre> <pre><code>curl -L -O https://examples.openshift.pub/pr-133/cluster-installation/vmware/agent-base-non-integrated//agent-config.yaml\n</code></pre> install-config.yaml install-config.yamlcurl <pre><code>apiVersion: v1\nbaseDomain: rbohne.e2e.bos.redhat.com\ncompute:\n  - architecture: amd64\n    hyperthreading: Enabled\n    name: worker\n    platform: {}\n    replicas: 2\ncontrolPlane:\n  architecture: amd64\n  hyperthreading: Enabled\n  name: master\n  platform: {}\n  replicas: 3\nmetadata:\n  creationTimestamp: null\n  name: vmw1\nnetworking:\n  clusterNetwork:\n    - cidr: 10.128.0.0/14\n      hostPrefix: 23\n  machineNetwork:\n    - cidr: 10.0.0.0/16\n  networkType: OVNKubernetes\n  serviceNetwork:\n    - 172.30.0.0/16\nplatform:\n  baremetal:\n    apiVIPs: '10.19.114.149'\n    ingressVIP: '10.19.114.150'\npublish: External\npullSecret: '{\"auths\":{\"cl...'\nsshKey: |\n  ssh-ed25519 AAAA..\n</code></pre> <pre><code>curl -L -O https://examples.openshift.pub/pr-133/cluster-installation/vmware/agent-base-non-integrated//install-config.yaml\n</code></pre>","tags":["billy","Agent-based","non-integrated","vsphere","vmware"]},{"location":"cluster-installation/vmware/agent-base-non-integrated/#create-iso","title":"Create iso","text":"<pre><code>openshift-install --dir vmw1/ agent create image\n</code></pre>","tags":["billy","Agent-based","non-integrated","vsphere","vmware"]},{"location":"cluster-installation/vmware/agent-base-non-integrated/#start-installation","title":"Start installation","text":"<p>Attach the agent iso to all VM's and boot it.</p> <p>Watch the output of cp-0 as rendezvous host and the others.</p>","tags":["billy","Agent-based","non-integrated","vsphere","vmware"]},{"location":"cluster-installation/vmware/example/","title":"Example installation at my VMware Lab","text":"","tags":["VMware","UPI","vSphere"]},{"location":"cluster-installation/vmware/example/#architecture","title":"Architecture","text":"","tags":["VMware","UPI","vSphere"]},{"location":"cluster-installation/vmware/example/#setup-lb-virtual-machine","title":"Setup LB virtual machine","text":"","tags":["VMware","UPI","vSphere"]},{"location":"cluster-installation/vmware/example/#install-rhel-8","title":"Install RHEL 8","text":"","tags":["VMware","UPI","vSphere"]},{"location":"cluster-installation/vmware/example/#enable-repos","title":"Enable repos:","text":"<pre><code>subscription-manager repos \\\n    --enable=\"rhel-8-for-x86_64-baseos-rpms\" \\\n    --enable=\"rhel-8-for-x86_64-appstream-rpms\"\n</code></pre>","tags":["VMware","UPI","vSphere"]},{"location":"cluster-installation/vmware/example/#install-tmux","title":"Install tmux","text":"<pre><code>yum install tmux -y\n</code></pre> <p>Start tmux session ;-)</p>","tags":["VMware","UPI","vSphere"]},{"location":"cluster-installation/vmware/example/#update","title":"Update","text":"<pre><code>yum update -y\n</code></pre>","tags":["VMware","UPI","vSphere"]},{"location":"cluster-installation/vmware/example/#install-setup-dnsmasq","title":"Install &amp; Setup dnsmasq","text":"<pre><code>yum install -y dnsmasq\n\ncat &gt; /etc/dnsmasq.d/openshift-4.conf &lt;&lt;EOF\nstrict-order\ndomain=rbohne.lab.example.com\nexpand-hosts\nserver=10.19.143.247\n# except-interface=lo\n# bind-dynamic\ninterface=ens224\naddress=/apps.rbohne.lab.example.com/192.168.100.148\naddn-hosts=/etc/dnsmasq-openshift-4.addnhosts\nEOF\n\ncat &gt; /etc/dnsmasq-openshift-4.addnhosts &lt;&lt;EOF\n192.168.100.148 api.rbohne.lab.example.com api-int.rbohne.lab.example.com\n192.168.100.149 master-0.rbohne.lab.example.com etcd-0.rbohne.lab.example.com\n192.168.100.150 master-1.rbohne.lab.example.com etcd-1.rbohne.lab.example.com\n192.168.100.151 master-2.rbohne.lab.example.com etcd-2.rbohne.lab.example.com\n192.168.100.152 bootstrap.rbohne.lab.example.com\n192.168.100.153 worker-1.rbohne.lab.example.com\n192.168.100.154 worker-2.rbohne.lab.example.com\n192.168.100.155 worker-3.rbohne.lab.example.com\nEOF\n\n\nsystemctl enable --now dnsmasq\nsystemctl status dnsmasq\n\nfirewall-cmd --zone=public --permanent --add-service=dns\nfirewall-cmd --reload\n</code></pre> <p>Testing with: <pre><code>dig wild.apps.apps.rbohne.lab.example.com @192.168.100.148\n</code></pre></p>","tags":["VMware","UPI","vSphere"]},{"location":"cluster-installation/vmware/example/#install-setup-load-balancer","title":"Install &amp; setup load balancer","text":"<pre><code>yum install -y podman\n\npodman pull quay.io/redhat-emea-ssa-team/openshift-4-loadbalancer\n\ncat &gt; /etc/systemd/system/openshift-4-loadbalancer.service &lt;&lt;EOF\n[Unit]\nDescription=OpenShift 4 LoadBalancer CLUSTER\nAfter=network.target\n\n[Service]\nType=simple\nTimeoutStartSec=5m\n\nExecStartPre=-/usr/bin/podman rm \"openshift-4-loadbalancer\"\nExecStartPre=/usr/bin/podman pull quay.io/redhat-emea-ssa-team/openshift-4-loadbalancer\nExecStart=/usr/bin/podman run --name openshift-4-loadbalancer --net host \\\n  -e API=bootstrap=192.168.100.152:6443,master-0=192.168.100.149:6443,master-1=192.168.100.150:6443,master-2=192.168.100.151:6443 \\\n  -e API_LISTEN=192.168.100.148:6443 \\\n  -e INGRESS_HTTP=bootstrap=192.168.100.152:80,master-0=192.168.100.149:80,master-1=192.168.100.150:80,master-2=192.168.100.151:80,worker-1=192.168.100.153:80,worker-2=192.168.100.154:80,worker-3=192.168.100.155:80 \\\n  -e INGRESS_HTTP_LISTEN=192.168.100.148:80 \\\n  -e INGRESS_HTTPS=bootstrap=192.168.100.152:443,master-0=192.168.100.149:443,master-1=192.168.100.150:443,master-2=192.168.100.151:443,worker-1=192.168.100.153:443,worker-2=192.168.100.154:443,worker-3=192.168.100.155:443 \\\n  -e INGRESS_HTTPS_LISTEN=192.168.100.148:443 \\\n  -e MACHINE_CONFIG_SERVER=bootstrap=192.168.100.152:22623,master-0=192.168.100.149:22623,master-1=192.168.100.150:22623,master-2=192.168.100.151:22623 \\\n  -e MACHINE_CONFIG_SERVER_LISTEN=192.168.100.148:22623 \\\n  -e STATS_LISTEN=192.168.100.148:1984 \\\n  -e STATS_ADMIN_PASSWORD=aengeo4oodoidaiP \\\n  quay.io/redhat-emea-ssa-team/openshift-4-loadbalancer\n\nExecReload=-/usr/bin/podman stop \"openshift-4-loadbalancer\"\nExecReload=-/usr/bin/podman rm \"openshift-4-loadbalancer\"\nExecStop=-/usr/bin/podman stop \"openshift-4-loadbalancer\"\nRestart=always\nRestartSec=30\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsystemctl daemon-reload\nsystemctl enable --now openshift-4-loadbalancer.service\nsystemctl status openshift-4-loadbalancer.service\n\nfirewall-cmd --zone=public --permanent --add-port=80/tcp\nfirewall-cmd --zone=public --permanent --add-port=443/tcp\nfirewall-cmd --zone=public --permanent --add-port=6443/tcp\nfirewall-cmd --zone=public --permanent --add-port=22623/tcp\nfirewall-cmd --zone=public --permanent --add-port=1984/tcp\nfirewall-cmd --reload\n</code></pre>","tags":["VMware","UPI","vSphere"]},{"location":"cluster-installation/vmware/example/#install-openshift-install-oc-kubectl-govc","title":"Install openshift-install, oc, kubectl, govc","text":"<pre><code>curl -L -O https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/stable-4.6/openshift-client-linux-4.6.48.tar.gz\ncurl -L -O https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/stable-4.6/openshift-install-linux-4.6.48.tar.gz\ncurl -L -O https://github.com/vmware/govmomi/releases/download/v0.27.2/govc_Linux_x86_64.tar.gz\n\ntar -xzvf openshift-client-linux-4.6.48.tar.gz -C /usr/local/bin/ kubectl oc\ntar -xzvf openshift-install-linux-4.6.48.tar.gz -C /usr/local/bin/ openshift-install\ntar -xzvf govc_Linux_x86_64.tar.gz -C /usr/local/bin/ govc\n</code></pre>","tags":["VMware","UPI","vSphere"]},{"location":"cluster-installation/vmware/example/#create-ssh-key","title":"Create ssh key","text":"<p><code>ssh-keygen</code></p>","tags":["VMware","UPI","vSphere"]},{"location":"cluster-installation/vmware/example/#download-pull-secret-from-cloudredhatcom","title":"Download pull secret from cloud.redhat.com","text":"<p>Store it in <code>~/redhat-pullsecret.json</code></p>","tags":["VMware","UPI","vSphere"]},{"location":"cluster-installation/vmware/example/#install-configure-govc","title":"Install configure govc","text":"<p>Add GOVC env to bashrc: <pre><code>export GOVC_URL='vcenterip'\nexport GOVC_USERNAME='user'\nexport GOVC_PASSWORD='pass'\nexport GOVC_INSECURE=1\nexport GOVC_DATACENTER=\"DC\"\nexport GOVC_DATASTORE=\"ds\"\n</code></pre></p>","tags":["VMware","UPI","vSphere"]},{"location":"cluster-installation/vmware/example/#create-rhcos-vm-template","title":"Create rhcos vm template","text":"<p>Create tempalte vm from OVA <code>rhcos-4.6.47</code></p> <p>Configure the template</p> InfoScreenshots <ul> <li>CPU, RAM, Disk</li> <li>BIOS Boot</li> <li>Disk.EnableUUID</li> <li>Latency high</li> </ul> <p></p> <p></p> <p></p> <p></p>","tags":["VMware","UPI","vSphere"]},{"location":"cluster-installation/vmware/example/#install-openshift","title":"Install openshift","text":"","tags":["VMware","UPI","vSphere"]},{"location":"cluster-installation/vmware/example/#create-install-configyaml","title":"Create install-config.yaml","text":"<pre><code>cat &gt; install-config.yaml &lt;&lt;EOF\napiVersion: v1\nbaseDomain: lab.example.com\ncompute:\n- hyperthreading: Enabled\n  name: worker\n  replicas: 0\ncontrolPlane:\n  hyperthreading: Enabled\n  name: master\n  replicas: 3\nmetadata:\n  name: rbohne\nplatform:\n  none: {}\nfips: false\npullSecret: '$( cat ~/redhat-pullsecret.json )'\nsshKey: '$( cat ~/.ssh/id_rsa.pub )'\nEOF\n</code></pre> <p>In case you want integrated: <pre><code>platform:\n  vsphere:\n    vcenter: ${GOVC_URL}\n    username: ${GOVC_USERNAME}\n    password: ${GOVC_PASSWORD}\n    datacenter: ${GOVC_DATACENTER}\n    defaultDatastore: ${GOVC_DATASTORE}\n</code></pre></p>","tags":["VMware","UPI","vSphere"]},{"location":"cluster-installation/vmware/example/#running-installation","title":"Running installation","text":"<pre><code>mkdir rbohne\ncp -v install-config.yaml rbohne/\n\nopenshift-install create manifests --dir=rbohne/\nrm -rf rbohne/openshift/99_openshift-cluster-api_master-machines-*.yaml\nrm -rf rbohne/openshift/99_openshift-cluster-api_worker-machineset-0.yaml\n\n# Disable Scheduler if you like...\n\nopenshift-install create ignition-configs --dir=rbohne/\n</code></pre> <p>Deploy VM's</p> <p>Prepare ignition files:</p> <pre><code>cat rbohne/bootstrap.ign | base64 -w0 &gt; rbohne/bootstrap.ign.base64\ncat rbohne/master.ign | base64 -w0 &gt; rbohne/master.ign.base64\ncat rbohne/worker.ign | base64 -w0 &gt; rbohne/worker.ign.base64\n</code></pre> By handShort script <pre><code>NODE=\"bootstrap.rbohne.lab.example.com\"\n# IP::GATEWAY:NETMASK:HOSTNAME:INTERFACE:none\nIP_CFG=\"ip='192.168.100.152::192.168.100.1:255.255.255.0:$NODE:ens192:none nameserver=192.168.100.148'\"\nIGN=\"$(pwd)/rbohne/bootstrap.ign.base64\"\nCLUSTERNAME=rbohne\n\ngovc vm.clone -vm \"/${GOVC_DATACENTER}/vm/${CLUSTERNAME}/rhcos-4.6.47\"  \\\n  -annotation=$ignition \\\n  -c=4 \\\n  -m=16384 \\\n  -net 'DC-Provisioning' \\\n  -on=false \\\n  -folder=${CLUSTERNAME} \\\n  -ds=\"${GOVC_DATASTORE}\" \\\n  $NODE\n\ngovc vm.change -vm=\"/${GOVC_DATACENTER}/vm/${CLUSTERNAME}/$NODE\" \\\n  -e=\"guestinfo.afterburn.initrd.network-kargs=$IP_CFG\" \\\n  -e=\"guestinfo.ipxe.ignition=$ignition\" \\\n  -e=\"guestinfo.ignition.config.data.encoding=base64\" \\\n  -f=\"guestinfo.ignition.config.data=${IGN}\"\n\ngovc vm.power -on=true \"/${GOVC_DATACENTER}/vm/${CLUSTERNAME}/$NODE\"\n</code></pre> <p>Info</p> <p>Repeat for other VM's masters and computer</p> <p>Download</p> <pre><code>#!/usr/bin/env bash\n\n# How to use the script: copy it and adjust what you have to adjust!\n#   For example: ip adresses ;-)\n\nset -euo pipefail\nset -x\n\n# Moved into ~/.basrc ;-)\n# export GOVC_URL='vcenter'\n# export GOVC_USERNAME='user'\n# export GOVC_PASSWORD='pwd'\n# export GOVC_INSECURE=1\n# export GOVC_DATACENTER=\"DC\"\n# export GOVC_DATASTORE=\"datastore\"\n\nbasedomain=\"lab.example.com\"\nclustername=\"rbohne\"\n\nnodes=(\n    \"bootstrap.${clustername}.${basedomain}\"\n    \"master-0.${clustername}.${basedomain}\"\n    \"master-1.${clustername}.${basedomain}\"\n    \"master-2.${clustername}.${basedomain}\"\n    \"worker-1.${clustername}.${basedomain}\"\n    \"worker-2.${clustername}.${basedomain}\"\n    \"worker-3.${clustername}.${basedomain}\"\n)\n\nignitions=(\n    \"$(pwd)/rbohne/bootstrap.ign.base64\"\n    \"$(pwd)/rbohne/master.ign.base64\"\n    \"$(pwd)/rbohne/master.ign.base64\"\n    \"$(pwd)/rbohne/master.ign.base64\"\n    \"$(pwd)/rbohne/worker.ign.base64\"\n    \"$(pwd)/rbohne/worker.ign.base64\"\n    \"$(pwd)/rbohne/worker.ign.base64\"\n);\n\nips=(\n    \"ip=192.168.100.152::192.168.100.148:255.255.255.0:bootstrap.${clustername}.${basedomain}:ens192:none nameserver=192.168.100.148\"\n    \"ip=192.168.100.149::192.168.100.148:255.255.255.0:master-0.${clustername}.${basedomain}:ens192:none nameserver=192.168.100.148\"\n    \"ip=192.168.100.150::192.168.100.148:255.255.255.0:master-1.${clustername}.${basedomain}:ens192:none nameserver=192.168.100.148\"\n    \"ip=192.168.100.151::192.168.100.148:255.255.255.0:master-2.${clustername}.${basedomain}:ens192:none nameserver=192.168.100.148\"\n    \"ip=192.168.100.153::192.168.100.148:255.255.255.0:worker-1.${clustername}.${basedomain}:ens192:none nameserver=192.168.100.148\"\n    \"ip=192.168.100.154::192.168.100.148:255.255.255.0:worker-2.${clustername}.${basedomain}:ens192:none nameserver=192.168.100.148\"\n    \"ip=192.168.100.155::192.168.100.148:255.255.255.0:worker-3.${clustername}.${basedomain}:ens192:none nameserver=192.168.100.148\"\n)\n\n# function govc {\n#     echo \"Dummy function\"\n# }\n\n# Setup vm's\nfor (( i=0; i&lt; ${#nodes[@]} ; i++ )) ; do\n    node=${nodes[$i]}\n    ip=${ips[$i]}\n    ignition=${ignitions[$i]}\n\n    echo \"Setup $node -&gt; $ip\";\n\n    # If you want to setup mac adress\n    # mac_adresse=${mac_adresses[$i]}\n    # -net.address ${mac_adresse} \\\n\n    govc vm.clone -vm \"/${GOVC_DATACENTER}/vm/${clustername}/rhcos-4.6.47\"  \\\n      -annotation=$ignition \\\n      -c=4 \\\n      -m=16384 \\\n      -net 'DC-Provisioning' \\\n      -on=false \\\n      -folder=${clustername} \\\n      -ds=\"${GOVC_DATASTORE}\" \\\n      $node\n\n    govc vm.change -vm=\"/${GOVC_DATACENTER}/vm/${clustername}/$node\" \\\n      -e=\"guestinfo.afterburn.initrd.network-kargs=$ip\" \\\n      -e=\"guestinfo.ignition.config.data.encoding=base64\" \\\n      -f=\"guestinfo.ignition.config.data=${ignition}\"\n\ndone;\n\n# Start vm's\nfor node in ${nodes[@]} ; do\n    echo \"# Start $node\";\n    govc vm.power -on=true $node\ndone;\n</code></pre> <pre><code>openshift-install wait-for bootstrap-complete --dir=rbohne\n\n# Power off bootstrap\ngovc vm.power -off bootstrap.rbohne.lab.example.com\n\n# Watch for pending csr and approve it for nodes\n#    two for each node\noc get csr | awk '/Pending/ { print $1 }' | xargs oc adm certificate approve\n\n\nopenshift-install wait-for install-complete --dir=rbohne\n</code></pre>","tags":["VMware","UPI","vSphere"]},{"location":"cluster-installation/windows-container/VMware-ipi/","title":"Windows Container auf VMware IPI","text":"<p>Doc Bugs:   * https://bugzilla.redhat.com/show_bug.cgi?id=1947052   * https://bugzilla.redhat.com/show_bug.cgi?id=1943587</p> <p>High level steps:</p> <ol> <li>Install OpenShift 4.7+ with <code>OVNKubernetes</code> SDN and <code>hybridOverlayConfig</code></li> <li> <p>Prepare a Windows golden image      Perfect time is during cluster installation ;-)</p> </li> <li> <p>Expose DNS Record api-int... for Windows Machines</p> </li> <li> <p>Install Windows Machine Config Operator (WMCO)</p> </li> </ol> <ol> <li>Configure private key (public installed in golden image)</li> </ol> <ol> <li>Create MachineSet</li> </ol>"},{"location":"cluster-installation/windows-container/VMware-ipi/#cluster-installation","title":"Cluster installation","text":""},{"location":"cluster-installation/windows-container/VMware-ipi/#create-install-configyaml","title":"Create install-config.yaml","text":"<pre><code>openshift-install create install-config --dir=cluster\n</code></pre>"},{"location":"cluster-installation/windows-container/VMware-ipi/#adjust-install-configyaml","title":"Adjust install-config.yaml","text":"<pre><code>cp -v cluster/install-config.yaml cluster/install-config-plain.yaml\nsed -i 's/OpenShiftSDN/OVNKubernetes/' cluster/install-config.yaml\n</code></pre>"},{"location":"cluster-installation/windows-container/VMware-ipi/#create-manifests","title":"Create manifests","text":"<pre><code>openshift-install create manifests --dir=cluster/\n</code></pre>"},{"location":"cluster-installation/windows-container/VMware-ipi/#configure-hypride-overlay","title":"Configure hypride overlay","text":"<p>Note</p> <p>Import on vSphere is the <code>hybridOverlayVXLANPort</code> because of Pod-to-pod connectivity between hosts is broken on my Kubernetes cluster running on vSphere</p> <pre><code>cat &gt;  cluster/manifests/cluster-network-03-config.yml &lt;&lt; EOF\napiVersion: operator.openshift.io/v1\nkind: Network\nmetadata:\n  creationTimestamp: null\n  name: cluster\nspec:\n  clusterNetwork:\n  - cidr: 10.128.0.0/14\n    hostPrefix: 23\n  externalIP:\n    policy: {}\n  networkType: OVNKubernetes\n  serviceNetwork:\n  - 172.30.0.0/16\n  defaultNetwork:\n    type: OVNKubernetes\n    ovnKubernetesConfig:\n      hybridOverlayConfig:\n        hybridClusterNetwork:\n        - cidr: 10.132.0.0/14\n          hostPrefix: 23\n        # Not supported with Windows 2019 LTSC\n        hybridOverlayVXLANPort: 9898\nstatus: {}\nEOF\n</code></pre>"},{"location":"cluster-installation/windows-container/VMware-ipi/#install-cluster","title":"Install cluster","text":"<pre><code>openshift-install create cluster --dir=cluster/\n</code></pre>"},{"location":"cluster-installation/windows-container/VMware-ipi/#prepare-windows-golden-image","title":"Prepare Windows golden image","text":"<p>Perfect time during OpenShift 4 installation :-)</p>"},{"location":"cluster-installation/windows-container/VMware-ipi/#get-a-windows-1909-iso-install-a-vm","title":"Get a Windows 1909 ISO &amp; Install a VM","text":"<p>Maybe 2019 works too</p> <p>Note</p> <ul> <li>Windows Server 2019 =&gt; LTSC</li> <li>Windows Server 1909 =&gt; SAC</li> </ul>"},{"location":"cluster-installation/windows-container/VMware-ipi/#update-windows","title":"Update Windows","text":"<p>Remote Desktop</p> <p>PS C:\\Users\\Administrator&gt; Set-ItemProperty -Path 'HKLM:\\System\\CurrentControlSet\\Control\\Terminal Server' -name \"fDenyTSConnections\" -value 0 PS C:\\Users\\Administrator&gt; Enable-NetFirewallRule -DisplayGroup \"Remote Desktop\"</p> <p>GUI or CLI <pre><code>Install-PackageProvider -Name NuGet -MinimumVersion 2.8.5.201 -Force\nSet-PSRepository PSGallery -InstallationPolicy Trusted\nInstall-Module PSWindowsUpdate\nGet-WindowsUpdate\nInstall-WindowsUpdate -AcceptAll -Install -IgnoreReboot\n</code></pre> Source: win-updates.ps1</p>"},{"location":"cluster-installation/windows-container/VMware-ipi/#disable-ipv6","title":"Disable IPv6","text":"<pre><code>&gt; Get-NetAdapterBinding\n\n&gt; Disable-NetAdapterBinding -Name &lt;Name&gt; -ComponentID ms_tcpip6\n</code></pre>"},{"location":"cluster-installation/windows-container/VMware-ipi/#install-vmware-tools","title":"Install VMware Tools","text":"<p>Business as usual :-)</p> <p>cmd: <code>d:\\setup64 /s /v \"/qb REBOOT=R\"</code></p>"},{"location":"cluster-installation/windows-container/VMware-ipi/#configure-vmware-tools","title":"Configure VMware Tools","text":"<pre><code>\"exclude-nics=\" | Set-Content -Path 'C:\\ProgramData\\VMware\\VMware Tools\\tools.conf'\n</code></pre>"},{"location":"cluster-installation/windows-container/VMware-ipi/#install-all-windows-updates","title":"Install all Windows Updates","text":""},{"location":"cluster-installation/windows-container/VMware-ipi/#install-openssh","title":"Install OpenSSH","text":"<pre><code>Add-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0\n\nSet-Service -Name ssh-agent -StartupType 'Automatic'\nSet-Service -Name sshd -StartupType 'Automatic'\nStart-Service ssh-agent\nStart-Service sshd\n\n$pubKeyConf = (Get-Content -path C:\\ProgramData\\ssh\\sshd_config) -replace '#PubkeyAuthentication yes','PubkeyAuthentication yes'\n$pubKeyConf | Set-Content -Path C:\\ProgramData\\ssh\\sshd_config\n$passwordConf = (Get-Content -path C:\\ProgramData\\ssh\\sshd_config) -replace '#PasswordAuthentication yes','PasswordAuthentication yes'\n$passwordConf | Set-Content -Path C:\\ProgramData\\ssh\\sshd_config\n\nRestart-Service sshd\n</code></pre>"},{"location":"cluster-installation/windows-container/VMware-ipi/#setup-public-key","title":"Setup Public Key","text":"<pre><code>\"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDHHYQEdg8wxKijObWr4fM69/zaZv/ll7mR2ua0o2qxJMauiFMWpcD24Liihy20mOYCJEfGU3F+sAUeXL5vdSnxf21jt1bk04KAAKhLF0KlNN7eGVcG9cLR2+inugNIUIArVNNPKC4+bJkRmKS9XMByHDC20X82dTEETTtwS57au0GeuzeKK9tNOrZRABiBX3bTplyESx3KzSNoY9IhKYL58Z6RF7bus3dVtYWHpFw1FYl9E1eSnwKjN4fmAqILQe6CaJESoDRgMnOmMFFrDfgGHtw8tkhEera7iDI2ZBShccxaFLauCj5r8RtBS8NlvVJumVMsVooCHf6CVxCK0xFv17wbpW5+E8MNU9qkPVBznEed4jPyYbzEhmEHtXsgh75Y6Xp2Ycegxzl6Y1rxmSillki472kHZftihEWIGVyG3QW7vYdjemkJKDiYnqMYur6HgXEJc7L5h/rg1N7IDAOzRHGPmDbgj7QYkSPHL9CkpA/Y2CX48JtLVaLGywZO9X8= rbohne@stormshiftdeploy.coe.muc.redhat.com\" | Set-Content -Path 'C:\\ProgramData\\ssh\\administrators_authorized_keys'\n\n# Fix permission\n$acl = Get-Acl C:\\ProgramData\\ssh\\administrators_authorized_keys\n$acl.SetAccessRuleProtection($true, $false)\n$administratorsRule = New-Object system.security.accesscontrol.filesystemaccessrule(\"Administrators\",\"FullControl\",\"Allow\")\n$systemRule = New-Object system.security.accesscontrol.filesystemaccessrule(\"SYSTEM\",\"FullControl\",\"Allow\")\n$acl.SetAccessRule($administratorsRule)\n$acl.SetAccessRule($systemRule)\n$acl | Set-Acl\n</code></pre>"},{"location":"cluster-installation/windows-container/VMware-ipi/#allow-incoming-connection-for-container-logs","title":"Allow incoming connection for container logs:","text":"<pre><code>$firewallRuleName = \"ContainerLogsPort\"\n$containerLogsPort = \"10250\"\nNew-NetFirewallRule -DisplayName $firewallRuleName -Direction Inbound -Action Allow -Protocol TCP -LocalPort $containerLogsPort -EdgeTraversalPolicy Allow\n</code></pre>"},{"location":"cluster-installation/windows-container/VMware-ipi/#install-container-runtime","title":"Install container runtime","text":"<pre><code>Install-PackageProvider -Name NuGet -MinimumVersion 2.8.5.201 -Force\nSet-PSRepository PSGallery -InstallationPolicy Trusted\nInstall-Module -Name DockerMsftProvider -Repository PSGallery -Force\nInstall-Package -Name docker -ProviderName DockerMsftProvider -Force\nRestart-Computer -Force\n</code></pre>"},{"location":"cluster-installation/windows-container/VMware-ipi/#pre-pull-images","title":"Pre pull images","text":"<p>TBD</p>"},{"location":"cluster-installation/windows-container/VMware-ipi/#optional-clone-vm","title":"Optional: Clone VM","text":"<p>After sysprep you can not modifi the golden image anymore. I recommend to clone the VM and run the sysprep in the clone. If you want to change the golden image, you follow the process: 1) Made changes you want 2) Clone the VM 3) Run sysprep in the clone</p>"},{"location":"cluster-installation/windows-container/VMware-ipi/#sysprep-to-have-a-propper-template","title":"Sysprep to have a propper Template","text":""},{"location":"cluster-installation/windows-container/VMware-ipi/#prepare-unattendxml","title":"Prepare unattend.xml","text":""},{"location":"cluster-installation/windows-container/VMware-ipi/#run-sysprep-tool","title":"Run Sysprep tool","text":"<pre><code>cd 'C:\\Windows\\System32\\Sysprep\\'\n.\\sysprep.exe /generalize /oobe /shutdown /unattend:C:\\Users\\Administrator\\unattend-1909.xml\n</code></pre>"},{"location":"cluster-installation/windows-container/VMware-ipi/#resources-links","title":"Resources &amp; Links","text":"<ul> <li>How to Check your PowerShell Version</li> <li>Windows Server-Wartungskan\u00e4le: LTSC und SAC</li> <li>https://docs.microsoft.com/de-de/virtualization/windowscontainers/manage-docker/configure-docker-daemon#clean-up-docker-data-and-system-components</li> <li>KMS-Clientsetupschl\u00fcssel</li> <li>https://docs.microsoft.com/de-de/windows-server/networking/sdn/technologies/hyper-v-network-virtualization/whats-new-hyperv-network-virtualization-windows-server</li> <li>https://www.software-express.de/info/windows-server2019-ltsc-sac/</li> <li>https://www.microsoft.com/de-de/evalcenter/evaluate-windows-server-2019</li> <li>https://docs.microsoft.com/en-us/windows/release-health/release-information</li> </ul>"},{"location":"cluster-installation/windows-container/VMware-ipi/#windows-sdn-debugging","title":"Windows SDN Debugging","text":"<ul> <li>https://github.com/microsoft/SDN/blob/master/Kubernetes/windows/debug/Debug.md</li> </ul>"},{"location":"cluster-installation/windows-container/VMware-ipi/#windows-2019-failed","title":"Windows 2019 - Failed","text":"<p>WMCO Pod: <pre><code>2021-04-01T11:27:59.081Z    ERROR   controller-runtime.controller   Reconciler error    {\"controller\": \"windowsmachine-controller\", \"request\": \"openshift-machine-api/win-hx4pn\", \"error\": \"failed to configure Windows VM 42037020-b6d0-1820-df2e-bb90efdaa952: configuring node network failed: error waiting for k8s.ovn.org/hybrid-overlay-distributed-router-gateway-mac node annotation for win-hx4pn: timeout waiting for k8s.ovn.org/hybrid-overlay-distributed-router-gateway-mac node annotation: timed out waiting for the condition\", \"errorVerbose\": \"timed out waiting for the condition\\ntimeout waiting for k8s.ovn.org/hybrid-overlay-distributed-router-gateway-mac node annotation\\ngithub.com/openshift/windows-machine-config-operator/pkg/controller/windowsmachine/nodeconfig.(*nodeConfig).waitForNodeAnnotation\\n\\t/remote-source/build/windows-machine-config-operator/pkg/controller/windowsmachine/nodeconfig/nodeconfig.go:264\\ngithub.com/openshift/w\n</code></pre></p> <p>Missing VXLAN Port in networkdetailed created via <code>.\\collectlogs.ps1</code></p> <pre><code>PS C:\\Users\\Administrator&gt; Get-ComputerInfo | select WindowsProductName, WindowsVersion, OsHardwareAbstractionLayer\n\nWindowsProductName                      WindowsVersion OsHardwareAbstractionLayer\n------------------                      -------------- --------------------------\nWindows Server 2019 Standard Evaluation 1809           10.0.17763.737\n\n\nPS C:\\Users\\Administrator&gt; (Get-ItemProperty \"HKLM:\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\").BuildLabEx\n17763.1.amd64fre.rs5_release.180914-1434\n\nPS C:\\Users\\Administrator&gt; $PSVersionTable.PSVersion\n\nMajor  Minor  Build  Revision\n-----  -----  -----  --------\n5      1      17763  592\n\nNach Updates:\n\nPS C:\\Users\\Administrator&gt; Get-ComputerInfo | select WindowsProductName, WindowsVersion, OsHardwareAbstractionLayer\n\nWindowsProductName                      WindowsVersion OsHardwareAbstractionLayer\n------------------                      -------------- --------------------------\nWindows Server 2019 Standard Evaluation 1809           10.0.17763.1790\n\n\nPS C:\\Users\\Administrator&gt; Get-ComputerInfo | select WindowsProductName, WindowsVersion, OsHardwareAbstractionLayer\n\nWindowsProductName                      WindowsVersion OsHardwareAbstractionLayer\n------------------                      -------------- --------------------------\nWindows Server 2019 Standard Evaluation 1809           10.0.17763.1790\n\n\nPS C:\\Users\\Administrator&gt; $PSVersionTable.PSVersion\n\nMajor  Minor  Build  Revision\n-----  -----  -----  --------\n5      1      17763  1490\n\n--------\n\n\n\nPS C:\\Users\\Administrator&gt; Get-ComputerInfo | select WindowsProductName, WindowsVersion, OsHardwareAbstractionLayer\n\nWindowsProductName                      WindowsVersion OsHardwareAbstractionLayer\n------------------                      -------------- --------------------------\nWindows Server 2019 Standard Evaluation 1809           10.0.17763.1852\n\n\n\nPS C:\\Users\\Administrator&gt; (Get-ItemProperty \"HKLM:\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\").BuildLabEx\n17763.1.amd64fre.rs5_release.180914-1434\nPS C:\\Users\\Administrator&gt; $PSVersionTable.PSVersion\n\nMajor  Minor  Build  Revision\n-----  -----  -----  --------\n5      1      17763  1852\n</code></pre>"},{"location":"cluster-installation/windows-container/dev-prev/","title":"Windows Container - DevPreview","text":"![](windows-container.gif)  <p>Checkout the Video from Christian Hernandez</p>"},{"location":"cluster-installation/windows-container/dev-prev/#installation","title":"Installation","text":""},{"location":"cluster-installation/windows-container/dev-prev/#prerequisite","title":"Prerequisite","text":"<ul> <li>Create a AWS key pairs with name windows-ssh-key</li> </ul> <pre><code>cd ~/directory-to-store-cluster-data\ndocker run -ti -v ~/.aws/:/root/.aws:z -v $(pwd)/:/work:z quay.io/openshift-examples/windows-container-install-helper:latest\ncd /work\n# Run script, it's not perfect just for me to spinup a OpenShift 4 cluster with a windows worker\naws-create-cluster.sh\n</code></pre>"},{"location":"cluster-installation/windows-container/dev-prev/#demo-applications","title":"Demo applications","text":""},{"location":"cluster-installation/windows-container/dev-prev/#run-powershellexe-webserver","title":"Run powershell.exe webserver","text":"<p>Note</p> <p>the image size is 2GB! It take some time to pull the image</p> <pre><code>oc new-project windows-container\noc label namespace windows-container \"openshift.io/run-level=1\"\n\noc create -f https://gist.githubusercontent.com/suhanime/683ee7b5a2f55c11e3a26a4223170582/raw/d893db98944bf615fccfe73e6e4fb19549a362a5/WinWebServer.yaml\n</code></pre>"},{"location":"cluster-installation/windows-container/dev-prev/#sample-apsnet","title":"Sample APS.NET","text":"<pre><code>oc new-project windows-container\noc label namespace windows-container \"openshift.io/run-level=1\"\n</code></pre> <p>Note</p> <p>In order to deploy into a different namespace SCC must be disabled in that namespace. This should never be used in production, and any namespace that this has been done to should not be used to run Linux pods.</p> <pre><code>oc create -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: sample-aspnetapp\n  name: sample-aspnetapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sample-aspnetapp\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: sample-aspnetapp\n    spec:\n      containers:\n      - image: mcr.microsoft.com/dotnet/framework/samples:aspnetapp\n        imagePullPolicy: IfNotPresent\n        name: sample-aspnetapp\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        resources: {}\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n      dnsPolicy: ClusterFirst\n      nodeSelector:\n        beta.kubernetes.io/os: windows\n      tolerations:\n      - key: os\n        value: Windows\nEOF\n\noc create -f - &lt;&lt;EOF\napiVersion: v1\nkind: Service\nmetadata:\n  name: sample-aspnetapp\n  labels:\n    app: sample-aspnetapp\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 80\n    targetPort: 80\n  selector:\n    app: sample-aspnetapp\n  type: LoadBalancer\nEOF\n\noc expose service/sample-aspnetapp\n</code></pre>"},{"location":"cluster-installation/windows-container/on-prem/","title":"On-Prem Windows Container installation","text":"<p>Warning</p> <p>Work in progress, this is no finish yet! And will not work!!!</p>"},{"location":"cluster-installation/windows-container/on-prem/#requirements","title":"Requirements:","text":"<p>Download:</p> <ul> <li>Windows 2019 Server The English Version!</li> <li>On KVM: Latest VirtIO driver</li> </ul>"},{"location":"cluster-installation/windows-container/on-prem/#windows-installation","title":"Windows installation","text":"<p>Note</p> <p>Windows Container need enabled Hyper-V, that means you have to enable nested virtualization. KVM on Host (L0) and Windows 2019 + Hyper-V as VM (L1) will fail. Windows crash/stuck at boot after Hyper-V activation</p>"},{"location":"cluster-installation/windows-container/on-prem/#installation-on-kvm-will-fail-because-if-nested-hyper-v","title":"Installation on KVM (will fail because if nested Hyper-V!)","text":"<p>List of  Disk is empty, please install VirtIO driver:</p> <p></p> <p></p> <p></p>"},{"location":"cluster-installation/windows-container/on-prem/#install-vm-guest-tools-virtiovmware","title":"Install VM guest tools (VirtIO/VMware)","text":""},{"location":"cluster-installation/windows-container/on-prem/#virtio","title":"VirtIO","text":"<p>Important: Use latest upstream because of:</p> <p></p>"},{"location":"cluster-installation/windows-container/on-prem/#enable-remote-desktop-optional","title":"Enable Remote Desktop (Optional)","text":""},{"location":"cluster-installation/windows-container/on-prem/#enable-hyper-v","title":"Enable Hyper-V","text":"<ul> <li>1 </li> <li>2 </li> <li>3 </li> <li>4 </li> <li>5 </li> <li>6 </li> <li>7 </li> <li>8 </li> <li>9 </li> <li>10 </li> <li>11 </li> <li>12 </li> <li>13 </li> <li>14 </li> </ul>"},{"location":"cluster-installation/windows-container/on-prem/#disable-ipv6","title":"Disable IPv6","text":""},{"location":"cluster-installation/windows-container/on-prem/#deactivate-firewall","title":"Deactivate firewall","text":""},{"location":"cluster-installation/windows-container/on-prem/#install-docker","title":"Install Docker","text":"<p>Official Windows Documentation: Get started: Prep Windows for containers</p> <p></p> <p>PowerShell <pre><code>Install-Module -Name DockerMsftProvider -Repository PSGallery -Force\n\nInstall-Package -Name docker -ProviderName DockerMsftProvider\n\nRestart-Computer -Force\n</code></pre></p>"},{"location":"cluster-installation/windows-container/on-prem/#enable-remote-managment","title":"Enable Remote Managment","text":"<pre><code>winrm quickconfig\nwinrm set winrm/config/client/auth '@{Basic=\"true\"}'\nwinrm set winrm/config/service/auth '@{Basic=\"true\"}'\nwinrm set winrm/config/service '@{AllowUnencrypted=\"true\"}'\n</code></pre>"},{"location":"cluster-installation/windows-container/on-prem/#join-windows-to-ocp-cluster","title":"Join Windows to OCP Cluster","text":"<pre><code>ansible-playbook -i ${CLUSTER_CONFIG}/inventory.ini /windows-machine-config-bootstrapper/tools/ansible/tasks/wsu/main.yaml -vv\n\nansible win -i ${CLUSTER_CONFIG}/inventory.ini -m win_ping -v\n</code></pre>"},{"location":"cluster-installation/windows-container/on-prem/#resources","title":"Resources","text":"<ul> <li>https://github.com/ovn-org/ovn-kubernetes/issues/683</li> <li>https://docs.microsoft.com/de-de/virtualization/windowscontainers/kubernetes/common-problems</li> <li>https://github.com/ansible/ansible/blob/devel/examples/scripts/ConfigureRemotingForAnsible.ps1</li> <li>https://docs.microsoft.com/en-us/virtualization/windowscontainers/container-networking/architecture</li> <li>https://docs.microsoft.com/en-us/windows-server/virtualization/hyper-v/get-started/create-a-virtual-switch-for-hyper-v-virtual-machines</li> <li>https://docs.google.com/presentation/d/1YofaUnlkBzFfeG9VIvzuBX5fC6C3w9M1bjTpbskw6RY/edit#slide=id.g7261e2d0f6_2_2732</li> </ul>"},{"location":"cluster-lifecycle/","title":"Cluster lifecycle","text":""},{"location":"cluster-lifecycle/#content","title":"Content","text":"<ul> <li> <p>Over-the-Air Upgrades</p> </li> <li> <p>Add node</p> </li> <li> <p>Monitoring</p> </li> <li> <p>cloud.redhat.com</p> </li> <li> <p>Backup</p> </li> <li> <p>Storage migration</p> </li> <li> <p>Restore &amp; Recovery</p> </li> <li> <p>Shutdown</p> </li> </ul>"},{"location":"cluster-lifecycle/cloud-redhat-com/","title":"cloud.redhat.com","text":"<p>Usefull CLI to manage cloud.redhat.com openshift clusters: https://github.com/openshift-online/uhc-cli</p> <ul> <li>API behind description https://api.openshift.com/</li> <li>Get a API token: https://cloud.redhat.com/openshift/token</li> </ul>"},{"location":"cluster-lifecycle/cloud-redhat-com/#get-a-api-token","title":"Get a API token","text":"<p>https://cloud.redhat.com/openshift/token</p>"},{"location":"cluster-lifecycle/cloud-redhat-com/#how-to-work-with-curl","title":"How to work with curl","text":"<p>Store bearer token in <code>$TOKEN</code>:</p> <pre><code>OFFLINE_ACCESS_TOKEN=\"\\\n[..snipped..]\n\"\nexport TOKEN=$(curl \\\n--silent \\\n--data-urlencode \"grant_type=refresh_token\" \\\n--data-urlencode \"client_id=cloud-services\" \\\n--data-urlencode \"refresh_token=${OFFLINE_ACCESS_TOKEN}\" \\\nhttps://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token | \\\njq -r .access_token)\n</code></pre>"},{"location":"cluster-lifecycle/cloud-redhat-com/#list-clusters","title":"List clusters","text":"<pre><code>curl -X GET \"https://api.openshift.com/api/clusters_mgmt/v1/clusters\" -H \"accept: application/json\"  -H \"Authorization: Bearer $TOKEN\"\n</code></pre> <p>CSV:</p> <pre><code>curl -s -X GET \\\n  \"https://api.openshift.com/api/clusters_mgmt/v1/clusters\" \\\n  -H \"accept: application/json\"  \\\n  -H \"Authorization: Bearer $TOKEN\"  \\\n  | jq -r ' .items[] | [.creation_timestamp,.name,.id,.state] |@csv'\n</code></pre>"},{"location":"cluster-lifecycle/cloud-redhat-com/#delete-a-cluster","title":"Delete a cluster","text":"<pre><code>curl -X DELETE \"https://api.openshift.com/api/clusters_mgmt/v1/clusters/$CLUSTER-ID$?deprovision=false\" -H \"accept: application/json\" -H \"Authorization: Bearer $TOKEN\"\n</code></pre>"},{"location":"cluster-lifecycle/cloud-redhat-com/#delete-all-clusters","title":"Delete all clusters:","text":"<pre><code>curl -s -X GET \"https://api.openshift.com/api/clusters_mgmt/v1/clusters\" \\\n    -H \"accept: application/json\" \\\n    -H \"Authorization: Bearer $TOKEN\" \\\n    | jq -r ' .items[] | .id' \\\n    | xargs -n1 -I{} curl -X DELETE \"https://api.openshift.com/api/clusters_mgmt/v1/clusters/{}?deprovision=false\" \\\n    -H \"accept: application/json\" \\\n    -H \"Authorization: Bearer $TOKEN\"\n</code></pre>"},{"location":"cluster-lifecycle/cloud-redhat-com/#delete-several-clusters","title":"Delete several clusters:","text":"<pre><code>curl -s -X GET \"https://api.openshift.com/api/clusters_mgmt/v1/clusters\" -H \"accept: application/json\"  -H \"Authorization: Bearer $TOKEN\"  | jq -r ' .items[] | [.creation_timestamp,.id]|@csv' | grep '\"2019-07-'| tr -d '\"' | while read line ; do curl -v -X DELETE \"https://api.openshift.com/api/clusters_mgmt/v1/clusters/${line#*,}?deprovision=false\" -H \"accept: application/json\" -H \"Authorization: Bearer $TOKEN\" ; done\n</code></pre>"},{"location":"cluster-lifecycle/oat-upgrades/","title":"Over-the-Air Upgrades","text":"","tags":["Over-the-Air","OTA"]},{"location":"cluster-lifecycle/oat-upgrades/#channel-overview","title":"Channel overview","text":"<ul> <li>Red Hat OpenShift Container Platform Update Graph</li> <li>https://openshift-channels.robszumski.com</li> </ul>","tags":["Over-the-Air","OTA"]},{"location":"cluster-lifecycle/restore/","title":"Some information about restore","text":"","tags":["restore","kubeconfig"]},{"location":"cluster-lifecycle/restore/#restore-kubeconfig","title":"Restore kubeconfig","text":"<pre><code>cat .openshift_install_state.json \\\n    | jq '.[\"*kubeconfig.AdminClient\"].File.Data' -r \\\n    | base64 -d &gt; kubeconfig\n</code></pre>","tags":["restore","kubeconfig"]},{"location":"cluster-lifecycle/restore/#restore-kube-scheduler","title":"Restore kube-scheduler","text":"<p>Or follow the KCS: Kube-scheduler is not scheduling pods due to client certificate not renewed automatically in OpenShift 4</p> <p>If pods stucks in Pending, might a problem with kube scheduler.</p> <p>Problem expired cert: <pre><code>$ kubectl describe secret kube-scheduler-client-cert-key -n openshift-kube-scheduler\nName:         kube-scheduler-client-cert-key\nNamespace:    openshift-kube-scheduler\nLabels:       auth.openshift.io/managed-certificate-type=target\nAnnotations:  auth.openshift.io/certificate-issuer: kube-control-plane-signer\n              auth.openshift.io/certificate-not-after: 2021-01-30T19:47:44Z\n              auth.openshift.io/certificate-not-before: 2020-12-31T19:47:43Z\n\nType:  kubernetes.io/tls\n\nData\n====\ntls.key:  1675 bytes\ntls.crt:  1168 bytes\n\n$ ssh -l core &lt;master-0&gt;\n[core@master-0 ~]$ sudo su - \nLast login: Tue Feb  2 14:20:52 UTC 2021 on pts/0\n[root@master-0 ~]# openssl x509 -noout -dates -in /etc/kubernetes/static-pod-resources/kube-scheduler-certs/secrets/kube-scheduler-client-cert-key/tls.crt \nnotBefore=Dec 31 19:47:43 2020 GMT\nnotAfter=Jan 30 19:47:44 2021 GMT\n</code></pre></p>","tags":["restore","kubeconfig"]},{"location":"cluster-lifecycle/restore/#check-client-cert-sync-on-master-nodes","title":"Check client cert sync on master nodes","text":"<pre><code>[root@master-0 ~]# crictl logs --tail 10 $(crictl ps --name kube-scheduler-cert-syncer -q) \nI0202 14:28:08.544428       1 certsync_controller.go:65] Syncing configmaps: []\nI0202 14:28:08.544441       1 certsync_controller.go:162] Syncing secrets: [{kube-scheduler-client-cert-key false}]\nI0202 14:38:08.500345       1 certsync_controller.go:65] Syncing configmaps: []\nI0202 14:38:08.500375       1 certsync_controller.go:162] Syncing secrets: [{kube-scheduler-client-cert-key false}]\nI0202 14:38:08.502818       1 certsync_controller.go:65] Syncing configmaps: []\nI0202 14:38:08.502845       1 certsync_controller.go:162] Syncing secrets: [{kube-scheduler-client-cert-key false}]\nI0202 14:38:08.544282       1 certsync_controller.go:65] Syncing configmaps: []\nI0202 14:38:08.544328       1 certsync_controller.go:162] Syncing secrets: [{kube-scheduler-client-cert-key false}]\nI0202 14:38:08.544698       1 certsync_controller.go:65] Syncing configmaps: []\nI0202 14:38:08.544716       1 certsync_controller.go:162] Syncing secrets: [{kube-scheduler-client-cert-key false}]\n[root@master-0 ~]# date\nTue Feb  2 14:41:11 UTC 2021\n[root@master-0 ~]# \n</code></pre> <p>=&gt; Sync every 10 minutes</p>","tags":["restore","kubeconfig"]},{"location":"cluster-lifecycle/restore/#manuel-renewal","title":"Manuel renewal","text":"<pre><code>mkdir kube-control-plane-signer \ncd $_\nkubectl get secrets -n openshift-kube-apiserver-operator \\\n  kube-control-plane-signer \\\n  -o jsonpath=\"{.data.tls\\.crt}\" \\\n  | base64 -d &gt; kube-control-plane-signer.crt\n\nkubectl get secrets -n openshift-kube-apiserver-operator \\\n  kube-control-plane-signer \\\n  -o jsonpath=\"{.data.tls\\.key}\" \\\n  | base64 -d &gt; kube-control-plane-signer.key\n\nkubectl get secret -n openshift-kube-scheduler \\\n  kube-scheduler-client-cert-key \\\n  -o jsonpath=\"{.data.tls\\.key}\" \\\n  | base64 -d &gt; openshift-kube-scheduler.key\n\n# Create new certificate request\nopenssl req -new \\\n  -key openshift-kube-scheduler.key \\\n  -out openshift-kube-scheduler.csr \\\n  -subj \"/CN=system:kube-scheduler\"\n\n# Sign certificate request - only one day to enforce renewal by OpenShift\nopenssl x509 -req \\\n  -in openshift-kube-scheduler.csr \\\n  -CA kube-control-plane-signer.crt \\\n  -CAkey kube-control-plane-signer.key \\\n  -CAcreateserial \\\n  -out openshift-kube-scheduler.crt \\\n  -days 1\n\n# Update secret\nkubectl create secret tls kube-scheduler-client-cert-key  \\\n  --namespace openshift-kube-scheduler \\\n  --save-config --dry-run=client \\\n  --key=openshift-kube-scheduler.key \\\n  --cert=openshift-kube-scheduler.crt \\\n  -o yaml | kubectl apply -f -\n</code></pre> <p>Check <code>kube-scheduler-cert-syncer</code> and <code>kube-scheduler</code>: </p> <pre><code>[root@master-0 ~]# crictl logs --tail 1 $(crictl ps --name kube-scheduler-cert-syncer -q) \nI0202 14:56:04.698961       1 event.go:282] Event(v1.ObjectReference{Kind:\"Pod\", Namespace:\"openshift-kube-scheduler\", Name:\"openshift-kube-scheduler-master-0\", UID:\"\", APIVersion:\"v1\", ResourceVersion:\"\", FieldPath:\"\"}): type: 'Normal' reason: 'CertificateUpdated' Wrote updated secret: openshift-kube-scheduler/kube-scheduler-client-cert-key\n[root@master-0 ~]# \n\n[root@master-0 ~]# crictl logs --tail 1 $(crictl ps --name kube-scheduler -q) \nI0202 14:56:04.698961       1 event.go:282] Event(v1.ObjectReference{Kind:\"Pod\", Namespace:\"openshift-kube-scheduler\", Name:\"openshift-kube-scheduler-master-0\", UID:\"\", APIVersion:\"v1\", ResourceVersion:\"\", FieldPath:\"\"}): type: 'Normal' reason: 'CertificateUpdated' Wrote updated secret: openshift-kube-scheduler/kube-scheduler-client-cert-key\n[root@master-0 ~]# \n</code></pre>","tags":["restore","kubeconfig"]},{"location":"cluster-lifecycle/shutdown/","title":"Shutdown","text":"","tags":["shutdown","restart","lifecycle"]},{"location":"cluster-lifecycle/shutdown/#some-information-about-shutting-down-a-cluster","title":"Some information about shutting down a cluster","text":"<p>Guide to gracefully shut down your cluster.</p> <p>Warning</p> <p>Important It is important to take an etcd backup before performing this procedure so that your cluster can be restored if you encounter any issues when restarting the cluster.</p> <p>For example, the following conditions can cause the restarted cluster to malfunction:</p> <ul> <li>etcd data corruption during shutdown</li> <li>Node failure due to hardware</li> <li>Network connectivity issues</li> </ul>","tags":["shutdown","restart","lifecycle"]},{"location":"cluster-lifecycle/shutdown/#certificate-expiration","title":"Certificate Expiration","text":"<p>Check the expiration date of the cluster certificates.</p> CommandExample output <pre><code>oc -n openshift-kube-apiserver-operator \\\n  get secret kube-apiserver-to-kubelet-signer \\\n  -o jsonpath='{.metadata.annotations.auth\\.openshift\\.io/certificate-not-after}'\n</code></pre> <pre><code>oc -n openshift-kube-apiserver-operator \\\n  get secret kube-apiserver-to-kubelet-signer \\\n  -o jsonpath='{.metadata.annotations.auth\\.openshift\\.io/certificate-not-after}'\n2025-11-20T12:30:34Z\n</code></pre> <p>Info</p> <p>To ensure that the cluster can restart gracefully, plan to restart it on or before the specified date. As the cluster restarts, the process might require you to manually approve the pending certificate signing requests (CSRs) to recover kubelet certificates.</p>","tags":["shutdown","restart","lifecycle"]},{"location":"cluster-lifecycle/shutdown/#cordon-and-evacuate-the-nodes","title":"Cordon and evacuate the Nodes","text":"<p>Mark all the nodes in the cluster as unschedulable.</p> <pre><code>oc get nodes -o name | xargs oc adm cordon\n</code></pre> <p>Evacuate the pods using the following method:</p> <pre><code>for node in $(oc get nodes -l node-role.kubernetes.io/worker -o name); do\n  echo ${node} ;\n  oc adm drain ${node} \\\n    --delete-emptydir-data \\\n    --ignore-daemonsets=true \\\n    --timeout=15s --force\ndone\n</code></pre>","tags":["shutdown","restart","lifecycle"]},{"location":"cluster-lifecycle/shutdown/#shutdown-the-nodes","title":"Shutdown the Nodes","text":"<p>Shut down all of the nodes in the cluster.</p> <pre><code>for node in $(oc get nodes -o name); do\n  oc debug ${node} -- chroot /host shutdown -h 1;\ndone\n</code></pre> <p>Note</p> <p>It is not necessary to drain control plane nodes of the standard pods that ship with OpenShift Container Platform prior to shutdown. Cluster administrators are responsible for ensuring a clean restart of their own workloads after the cluster is restarted. If you drained control plane nodes prior to shutdown because of custom workloads, you must mark the control plane nodes as schedulable before the cluster will be functional again after restart.</p>","tags":["shutdown","restart","lifecycle"]},{"location":"cluster-lifecycle/shutdown/#shutting-down-openshift-with-ceph-openshift-data-foundations-simplified","title":"Shutting down OpenShift with Ceph / OpenShift Data Foundations (simplified)","text":"<p>Source (origin): Shutting down OpenShift with Ceph / OpenShift Data Foundations (simplified)</p> <p>Steps:</p> <ol> <li>Setup some connection variables</li> <li>Mark the OpenShift nodes as unschedulable which prevents Pods &amp; VMs from restarting</li> <li>Stop everything using ODF a. VMs - gracefully shutdown with oc delete VirtualMachineInstance/... b. Monitoring - graceful shutdown with oc delete Pod/... c. other apps</li> <li> <p>Shutdown the nodes (remaining ODF processes, kube-apiserver, etcd, and other OCP processes)</p> </li> <li> <p>Shutdown ODF first</p> </li> <li>Shutdown OCP last</li> </ol> <p>The script below tries to gracefully shutdown a 3-node / \"compact\" OCP cluster.</p> <pre><code>#!/bin/bash\n\nset -euo pipefail\n\nCLUSTER_NAME=\"tacos\"\nOC_BIN=/usr/local/bin/oc\nKUBECONFIG=/home/jcall/ocp-tacos/kubeconfig\nOC_CMD=\"$OC_BIN --kubeconfig=$KUBECONFIG\"\n\n\n# check if we're connected to the correct cluster\nCONNECTED_CLUSTER=$($OC_CMD whoami --show-console)\nif [[ $CONNECTED_CLUSTER =~ $CLUSTER_NAME ]]; then\n  CERT_EXPIRE=$($OC_CMD -n openshift-kube-apiserver-operator get secret/kube-apiserver-to-kubelet-signer -o jsonpath='{.metadata.annotations.auth\\.openshift\\.io/certificate-not-after}')\n  echo \"Please restart the cluster before the certificates expire: $(date -d $CERT_EXPIRE)\"\nelse\n  echo \"Error: Not connected to $CLUSTER_NAME\"\n  echo \"found $CONNECTED_CLUSTER instead!\"\n  exit 1\nfi\n\necho \"Marking all nodes as Unschedulable\"\nfor i in $($OC_CMD get nodes -o name); do\n  $OC_CMD adm cordon $i\ndone\n\necho ; echo \"Find all Pods and VMs using ODF storage\"\nexport STORAGECLASSES=$(oc get sc -o json | jq -j '.items | .[] | select(.provisioner|endswith(\"csi.ceph.com\")) | .metadata.name')\nexport PVCS=$(oc get pvc -A -o json | jq -j --arg sc \"$STORAGECLASSES\" '.items | .[] | select(.spec.storageClassName | inside($sc) )| .metadata.name')\nexport PODS=$(oc get pods -A -o json | jq -r --arg pvc \"$PVCS\" '.items | .[] | select(.spec.volumes | try(.[]) | try(.persistentVolumeClaim) | select(.claimName | inside($pvc))) | .metadata.namespace+\" \"+.metadata.name' | sort | uniq | tee /dev/tty)\n\necho ; echo \"Enter to continue to delete the above pods, or press Ctrl + C to abort\"\nread yes\n\necho ; echo \"Sending graceful shutdown command to Pods and VMs\"\necho \"$PODS\" | while read line; do\n  oc delete pod -n $line\ndone\n\n# nothing should be using ODF at this point, move on to shutting down the nodes\necho ; echo \"Telling the nodes to shutdown/halt in 5 minutes...\"\nfor node in $($OC_CMD get nodes -o name); do\n  oc debug $node -- chroot /host shutdown -h 5\ndone\n\necho ; echo \"Please remember to \\\"uncordon\\\" the nodes when the cluster is restarted!\"\necho \"For example, using this loop command:\"\necho \"for i in $(oc get nodes -o name); do oc adm uncordon $i; done\"\n</code></pre>","tags":["shutdown","restart","lifecycle"]},{"location":"cluster-lifecycle/shutdown/#sources","title":"Sources","text":"<ul> <li>Docs - Shutting down the cluster gracefully</li> <li>John Call via HackMD - Shutting down OpenShift with Ceph / OpenShift Data Foundations (simplified)</li> </ul>","tags":["shutdown","restart","lifecycle"]},{"location":"cluster-lifecycle/storage-migration/","title":"Storage Migration for container workload","text":"<ul> <li>Install Migration Toolkit for Containers Operator</li> </ul> <p>Versions:</p> Component Version OpenShift 4.17.7 Migration Toolkit for Containers Operator 1.8.5","tags":["pvc","storage","MTC","v4.17"]},{"location":"cluster-lifecycle/storage-migration/#created-migplan","title":"Created MigPlan","text":"<p>By yaml because WebUI do not list namespaces with <code>openshift-*</code></p> <pre><code>apiVersion: migration.openshift.io/v1alpha1\nkind: MigPlan\nmetadata:\n  annotations:\n    migration.openshift.io/selected-migplan-type: scc\n  name: image-registry\n  namespace: openshift-migration\nspec:\n  destMigClusterRef:\n    name: host\n    namespace: openshift-migration\n  liveMigrate: false\n  namespaces:\n    - openshift-image-registry\n  persistentVolumes:\n    - capacity: 100Gi\n      name: pvc-8059a107-3874-4ac3-b4a2-a747539fb712\n      proposedCapacity: 100Gi\n      pvc:\n        accessModes:\n          - ReadWriteMany\n        hasReference: true\n        name: 'registry-storage-netapp-nas'\n        namespace: openshift-image-registry\n        volumeMode: Filesystem\n      selection:\n        action: copy\n        copyMethod: filesystem\n        storageClass: coe-netapp-nas\n        verify: true\n      storageClass: ocs-storagecluster-cephfs\n      supported:\n        actions:\n          - skip\n          - copy\n        copyMethods:\n          - filesystem\n          - block\n          - snapshot\n  srcMigClusterRef:\n    name: host\n    namespace: openshift-migration\n</code></pre>","tags":["pvc","storage","MTC","v4.17"]},{"location":"cluster-lifecycle/storage-migration/#test-pod","title":"Test Pod","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  generateName: tools-\n  labels:\n    app: tools\nspec:\n  containers:\n    - name: tools\n      image: registry.redhat.io/rhel9/support-tools:9.5\n      command:\n        - \"/bin/sh\"\n        - \"-c\"\n        - \"sleep infinity\"\n      volumeMounts:\n        - mountPath: /src\n          name: src\n        - mountPath: /dst\n          name: dst\n  volumes:\n    - name: src\n      persistentVolumeClaim:\n        claimName: registry-storage-ocs\n    - name: dst\n      persistentVolumeClaim:\n        claimName: registry-storage-ocs-mig-7jrm\n</code></pre> <pre><code>oc rsh $(oc wait --for=condition=Ready pod -l app=tools -o name )\n</code></pre>","tags":["pvc","storage","MTC","v4.17"]},{"location":"cluster-lifecycle/add-node/","title":"Add Node to an existing cluster","text":"<p>Tested with OpenShift 4.17</p> <p>Doc bug to improve RH Documentation: OSDOCS-13020</p>","tags":["node","v4.17","control-plane"]},{"location":"cluster-lifecycle/add-node/#documentation","title":"Documentation","text":"Documetation Notes Adding worker nodes to an on-premise cluster <li>Supports only to add worker nodes</li> Single Node documentation: 10.1.3. Adding worker nodes using the Assisted Installer API <li>Ignored, because all my clusters are install without assisted isntaller (SaaS)</li> Single Node documentation: 10.1.4. Adding worker nodes to single-node OpenShift clusters manually <li>This works with all cluster types, doesn't matter which cluster size or installation type</li>","tags":["node","v4.17","control-plane"]},{"location":"cluster-lifecycle/add-node/#how-to-get-rhel-coreos-boot-image","title":"How to get RHEL CoreOS boot image","text":"","tags":["node","v4.17","control-plane"]},{"location":"cluster-lifecycle/add-node/#download-generic-version-from-red-hat-resources","title":"Download generic Version from Red Hat resources","text":"<ul> <li>Download from console.redhat.com for latest version</li> <li>To download a specific one: https://mirror.openshift.com/pub/openshift-v4/<ul> <li>x86_64 =&gt; https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/</li> <li>arm64 =&gt; https://mirror.openshift.com/pub/openshift-v4/arm64/dependencies/rhcos/</li> <li>...</li> </ul> </li> </ul>","tags":["node","v4.17","control-plane"]},{"location":"cluster-lifecycle/add-node/#download-cluster-specific-one-from-red-hat-resources-recommended","title":"Download cluster specific one from Red Hat resources (recommended)","text":"","tags":["node","v4.17","control-plane"]},{"location":"cluster-lifecycle/add-node/#via-oc","title":"via oc","text":"<pre><code>% oc -n openshift-machine-config-operator \\\n    get configmap/coreos-bootimages \\\n    -o jsonpath='{.data.stream}' \\\n    | jq -r '.architectures.x86_64.artifacts.metal.formats.iso.disk.location'\nhttps://rhcos.mirror.openshift.com/art/storage/prod/streams/4.17-9.4/builds/417.94.202410090854-0/x86_64/rhcos-417.94.202410090854-0-live.x86_64.iso\n\ncurl -L -O ...\n</code></pre>","tags":["node","v4.17","control-plane"]},{"location":"cluster-lifecycle/add-node/#via-openshift-install","title":"via openshift-install","text":"<pre><code>% openshift-install coreos print-stream-json \\\n  | jq -r '.architectures.x86_64.artifacts.metal.formats.iso.disk.location'\nhttps://rhcos.mirror.openshift.com/art/storage/prod/streams/4.17-9.4/builds/417.94.202410090854-0/x86_64/rhcos-417.94.202410090854-0-live.x86_64.iso\n</code></pre>","tags":["node","v4.17","control-plane"]},{"location":"cluster-lifecycle/add-node/#add-control-plane-node","title":"Add control-plane node","text":"<p>Node overview</p> Node IP Mac cp-1 (0) 10.32.105.69 0E:C0:EF:20:69:45 cp-2 (1) 10.32.105.70 0E:C0:EF:20:69:46 cp-3 (2) 10.32.105.71 0E:C0:EF:20:69:47 cp-4 (4) 10.32.105.72 0E:C0:EF:20:69:48 cp-5 (5) 10.32.105.73 0E:C0:EF:20:69:49","tags":["node","v4.17","control-plane"]},{"location":"cluster-lifecycle/add-node/#configure-dhcp-dns","title":"Configure DHCP &amp; DNS","text":"<p>DHCP</p> <pre><code>host ocp1-cp-4 {\n  hardware ethernet 0E:C0:EF:20:69:48;\n  fixed-address 10.32.105.72;\n  option host-name \"ocp1-cp-4\";\n  option domain-name \"stormshift.coe.muc.redhat.com\";\n}\n</code></pre> <p>DNS</p> <pre><code>72.105.32.10.in-addr.arpa. 120  IN      PTR     ocp1-cp-4.stormshift.coe.muc.redhat.com.\nocp1-cp-4.stormshift.coe.muc.redhat.com. 60 IN A 10.32.105.72\n</code></pre>","tags":["node","v4.17","control-plane"]},{"location":"cluster-lifecycle/add-node/#at-target-cluster-stormshift-ocp1","title":"At target cluster (stormshift-ocp1)","text":"<p>Get RHCOS and Download it</p> <pre><code>% oc -n openshift-machine-config-operator     get configmap/coreos-bootimages     -o jsonpath='{.data.stream}'     | jq -r '.architectures.x86_64.artifacts.metal.formats.iso.disk.location'\nhttps://rhcos.mirror.openshift.com/art/storage/prod/streams/4.17-9.4/builds/417.94.202410090854-0/x86_64/rhcos-417.94.202410090854-0-live.x86_64.iso\ncurl -L -O https://rhcos.mirror.openshift.com/art/storage/prod/streams/4.17-9.4/builds/417.94.202410090854-0/x86_64/rhcos-417.94.202410090854-0-live.x86_64.iso\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 1187M  100 1187M    0     0  30.4M      0  0:00:39  0:00:39 --:--:-- 32.9M\n</code></pre> <p>Extract ignition and put it into a Webserver</p> <p>Control plane node ignition:</p> <pre><code>oc extract -n openshift-machine-api \\\n  secret/master-user-data-managed \\\n  --keys=userData --to=- &gt; cp.ign\n</code></pre> <p>Worker node ignition:</p> <pre><code>oc extract -n openshift-machine-api \\\n  secret/worker-user-data-managed \\\n  --keys=userData --to=- &gt; worker.ign\n</code></pre>","tags":["node","v4.17","control-plane"]},{"location":"cluster-lifecycle/add-node/#at-hosting-cluster-isar","title":"At hosting cluster (ISAR)","text":"","tags":["node","v4.17","control-plane"]},{"location":"cluster-lifecycle/add-node/#upload-iso","title":"Upload ISO","text":"<pre><code>% oc project stormshift-ocp1-infra\nNow using project \"stormshift-ocp1-infra\" on server \"https://api.isar.coe.muc.redhat.com:6443\".\n% virtctl image-upload dv rhcos-417-94-202410090854-0-live --size=2Gi --storage-class coe-netapp-nas --image-path rhcos-417.94.202410090854-0-live.x86_64.iso\nPVC stormshift-ocp1-infra/rhcos-417-94-202410090854-0-live not found\nDataVolume stormshift-ocp1-infra/rhcos-417-94-202410090854-0-live created\nWaiting for PVC rhcos-417-94-202410090854-0-live upload pod to be ready...\nPod now ready\nUploading data to https://cdi-uploadproxy-openshift-cnv.apps.isar.coe.muc.redhat.com\n\n 1.16 GiB / 1.16 GiB [===================================================================================================================================] 100.00% 11s\n\nUploading data completed successfully, waiting for processing to complete, you can hit ctrl-c without interrupting the progress\nProcessing completed successfully\nUploading rhcos-417.94.202410090854-0-live.x86_64.iso completed successfully\n</code></pre>","tags":["node","v4.17","control-plane"]},{"location":"cluster-lifecycle/add-node/#create-vm","title":"Create VM","text":"<pre><code>---\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: ocp1-cp-4\n  namespace: stormshift-ocp1-infra\nspec:\n  dataVolumeTemplates:\n    - metadata:\n        creationTimestamp: null\n        name: ocp1-cp-4-root\n      spec:\n        source:\n          blank: {}\n        storage:\n          accessModes:\n            - ReadWriteMany\n          resources:\n            requests:\n              storage: 120Gi\n          storageClassName: coe-netapp-san\n  running: true\n  template:\n    metadata:\n      creationTimestamp: null\n    spec:\n      architecture: amd64\n      domain:\n        cpu:\n          cores: 8\n        devices:\n          disks:\n            - bootOrder: 1\n              disk:\n                bus: virtio\n              name: root\n            - bootOrder: 2\n              cdrom:\n                bus: sata\n              name: cdrom\n          interfaces:\n            - bridge: {}\n              macAddress: '0E:C0:EF:20:69:48'\n              model: virtio\n              name: coe\n        machine:\n          type: pc-q35-rhel9.4.0\n        memory:\n          guest: 16Gi\n        resources:\n          limits:\n            memory: 16706Mi\n          requests:\n            memory: 16Gi\n      networks:\n        - multus:\n            networkName: coe-bridge\n          name: coe\n      volumes:\n        - name: cdrom\n          persistentVolumeClaim:\n            claimName: rhcos-417-94-202410090854-0-live\n        - dataVolume:\n            name: ocp1-cp-4-root\n          name: root\n</code></pre> <ul> <li>ToDo: Serial consol does not work</li> </ul>","tags":["node","v4.17","control-plane"]},{"location":"cluster-lifecycle/add-node/#install-coreos-via-console","title":"Install coreos via Console","text":"<pre><code>curl -L -O http://10.32.96.31/stormshift-ocp1-cp.ign\nsudo coreos-installer install -i stormshift-ocp1-cp.ign /dev/vda\nsudo reboot\n</code></pre>","tags":["node","v4.17","control-plane"]},{"location":"cluster-lifecycle/add-node/#wait-for-and-approve-csr","title":"Wait for and approve CSR","text":"<pre><code>oc get csr | awk '/Pending/ { print $1 }' | xargs oc adm certificate approve\n</code></pre>","tags":["node","v4.17","control-plane"]},{"location":"cluster-lifecycle/add-node/#in-case-of-control-plane-node","title":"In case of control-plane node","text":"<p>Let's create BareMetalHost object and MachineObject</p> <pre><code>---\napiVersion: metal3.io/v1alpha1\nkind: BareMetalHost\nmetadata:\n  name: ocp1-cp-4\n  namespace: openshift-machine-api\nspec:\n  automatedCleaningMode: metadata\n  bootMACAddress: 0E:C0:EF:20:69:48\n  bootMode: legacy\n  customDeploy:\n    method: install_coreos\n  externallyProvisioned: true\n  online: true\n  userData:\n    name: master-user-data-managed\n    namespace: openshift-machine-api\n</code></pre> <pre><code>---\napiVersion: machine.openshift.io/v1beta1\nkind: Machine\nmetadata:\n  annotations:\n    machine.openshift.io/instance-state: externally provisioned\n    metal3.io/BareMetalHost: openshift-machine-api/ocp1-cp-4\n  labels:\n    machine.openshift.io/cluster-api-cluster: ocp1-nlxjs\n    machine.openshift.io/cluster-api-machine-role: master\n    machine.openshift.io/cluster-api-machine-type: master\n  name: ocp1-cp-4\n  namespace: openshift-machine-api\nspec:\n  metadata: {}\n  providerSpec:\n    value:\n      apiVersion: baremetal.cluster.k8s.io/v1alpha1\n      customDeploy:\n        method: install_coreos\n      hostSelector: {}\n      image:\n        checksum: \"\"\n        url: \"\"\n      kind: BareMetalMachineProviderSpec\n      metadata:\n        creationTimestamp: null\n      userData:\n        name: master-user-data-managed\n</code></pre>","tags":["node","v4.17","control-plane"]},{"location":"cluster-lifecycle/add-node/#patch-baremetalhost-status","title":"Patch BareMetalHost status","text":"<p>Open API proxy in on terminal</p> <pre><code>oc proxy\n</code></pre> <p>Patch object in another terminal</p> <pre><code>export HOST_PROXY_API_PATH=\"http://127.0.0.1:8001/apis/metal3.io/v1alpha1/namespaces/openshift-machine-api/baremetalhosts\"\n\nread -r -d '' host_patch &lt;&lt; EOF\n{\n  \"status\": {\n    \"hardware\": {\n      \"nics\": [\n        {\n          \"ip\": \"10.32.105.72\",\n          \"mac\": \"0E:C0:EF:20:69:48\"\n        }\n      ]\n    }\n  }\n}\nEOF\n\ncurl -vv \\\n     -X PATCH \\\n     \"${HOST_PROXY_API_PATH}/ocp1-cp-4/status\" \\\n     -H \"Content-type: application/merge-patch+json\" \\\n     -d \"${host_patch}\"\n</code></pre>","tags":["node","v4.17","control-plane"]},{"location":"cluster-lifecycle/add-node/#add-worker-node","title":"Add worker node","text":"<ul> <li>I added two interfaces to the VM for bonding tests</li> <li>nodes-config.yaml does not match to vm example!!</li> </ul> nodes-config.yaml for a BareMetal node <pre><code>hosts:\n  - hostname: inf49\n    rootDeviceHints:\n      deviceName: /dev/sda\n    interfaces:\n      - macAddress: b4:99:ba:b4:49:d2\n        name: enp3s0f0\n      - macAddress: 00:1b:21:b5:6a:20\n        name: ens2f0\n      - macAddress: 00:1b:21:b5:6a:21\n        name: ens2f1\n    networkConfig:\n      interfaces:\n        - name: enp3s0f0\n          type: ethernet\n          ipv6:\n            enabled: false\n          ipv4:\n            enabled: false\n        - name: bond0.32\n          type: vlan\n          state: up\n          ipv4:\n            enabled: true\n            dhcp: true\n          ipv6:\n            enabled: false\n          vlan:\n            base-iface: bond0\n            id: 32\n        - name: bond0\n          type: bond\n          state: up\n          link-aggregation:\n            mode: active-backup\n            options:\n              primary: ens2f0\n              miimon: '140'\n            port:\n              - ens2f0\n              - ens2f1\n</code></pre> <p>Create &amp; upload iso:</p> <pre><code>oc adm node-image create nodes-config.yaml\nvirtctl image-upload dv  extra-worker-1-iso  --size=2Gi --storage-class coe-netapp-nas --image-path node.x86_64.iso\n</code></pre> Example VM definition extra-worker-1.yaml <pre><code>---\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: extra-worker-1\n  namespace: stormshift-ocp1-infra\nspec:\n  dataVolumeTemplates:\n    - metadata:\n        creationTimestamp: null\n        name: extra-worker-1-root\n      spec:\n        source:\n          blank: {}\n        storage:\n          accessModes:\n            - ReadWriteMany\n          resources:\n            requests:\n              storage: 120Gi\n          storageClassName: coe-netapp-san\n  running: true\n  template:\n    metadata:\n      creationTimestamp: null\n    spec:\n      architecture: amd64\n      domain:\n        cpu:\n          cores: 8\n        devices:\n          disks:\n            - bootOrder: 1\n              disk:\n                bus: virtio\n              name: root\n            - bootOrder: 2\n              cdrom:\n                bus: sata\n              name: cdrom\n          interfaces:\n            - bridge: {}\n              macAddress: '0E:C0:EF:20:69:4B'\n              model: virtio\n              name: coe\n            - bridge: {}\n              macAddress: '0E:C0:EF:20:69:4C'\n              model: virtio\n              name: coe2\n        machine:\n          type: pc-q35-rhel9.4.0\n        memory:\n          guest: 16Gi\n        resources:\n          limits:\n            memory: 16706Mi\n          requests:\n            memory: 16Gi\n      networks:\n        - multus:\n            networkName: coe-bridge\n          name: coe\n        - multus:\n            networkName: coe-bridge\n          name: coe2\n      volumes:\n        - name: cdrom\n          persistentVolumeClaim:\n            claimName: extra-worker-1-iso\n        - dataVolume:\n            name: extra-worker-1-root\n          name: root\n</code></pre>","tags":["node","v4.17","control-plane"]},{"location":"cluster-lifecycle/backup/etcd/","title":"ETCD Backup","text":"<p>Official documentation: https://docs.openshift.com/container-platform/latest/backup_and_restore/backing-up-etcd.html</p>","tags":["backup","etcd"]},{"location":"cluster-lifecycle/backup/etcd/#via-cronjob-scheduled-job-prefered","title":"via Cronjob / scheduled job \u2b50prefered\u2b50","text":"<p>https://github.com/samuelvl/ocp4-upi-baremetal-lab/tree/master/day-two/50-etcd-backup</p>","tags":["backup","etcd"]},{"location":"cluster-lifecycle/backup/etcd/#with-acm","title":"With ACM","text":"<p>https://github.com/open-cluster-management/policy-collection/blob/master/community/CM-Configuration-Management/policy-etcd-backup.yaml</p>","tags":["backup","etcd"]},{"location":"cluster-lifecycle/backup/etcd/#try-to-run-in-a-pod","title":"Try to run in a Pod","text":"<ul> <li>Create pvc with name <code>etcd-backup</code></li> </ul> <p>Note</p> <p>Inline bash to get the etcd image, etcd image will change after a cluster upgrade.</p> <p>Skip podman and umount, because only needed to extract etcd client from image.</p> <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    run: etcd-backup\n  name: etcd-backup\nspec:\n  nodeSelector:\n    node-role.kubernetes.io/master: ''\n  containers:\n  - command:\n    - /bin/sh\n    - -c\n    - |\n      function umount() { :; }\n      export -f umount\n      function podman() { :; }\n      export -f podman\n\n      /host/usr/local/bin/cluster-backup.sh /etcd-backup/\n\n    image: $(oc get pod -n openshift-etcd -l app=etcd -o jsonpath=\"{.items[0].spec.containers[0].image}\")\n    name: etcd-backup\n    securityContext:\n      privileged: true\n      runAsUser: 0\n    resources: {}\n    volumeMounts:\n      - name: etcd-backup\n        mountPath: /etcd-backup\n      - name: host\n        mountPath: /host\n      - name: host-etc-kubernetes\n        mountPath: /etc/kubernetes\n  volumes:\n    - name: etcd-backup\n      persistentVolumeClaim:\n        claimName: etcd-backup\n    - name: host\n      hostPath:\n        path: /\n        type: Directory\n    - name: host-etc-kubernetes\n      hostPath:\n        path: /etc/kubernetes\n        type: Directory\n  dnsPolicy: ClusterFirst\n  restartPolicy: Always\nstatus: {}\nEOF\n</code></pre> <p>Output: <pre><code>$ oc logs etcd-backup\netcdctl version: 3.4.9\nAPI version: 3.4\nfound latest kube-apiserver-pod: /etc/kubernetes/static-pod-resources/kube-apiserver-pod-9\nfound latest kube-controller-manager-pod: /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-4\nfound latest kube-scheduler-pod: /etc/kubernetes/static-pod-resources/kube-scheduler-pod-5\nfound latest etcd-pod: /etc/kubernetes/static-pod-resources/etcd-pod-3\n{\"level\":\"info\",\"ts\":1612345971.3303144,\"caller\":\"snapshot/v3_snapshot.go:119\",\"msg\":\"created temporary db file\",\"path\":\"/etcd-backup//snapshot_2021-02-03_095251.db.part\"}\n{\"level\":\"info\",\"ts\":\"2021-02-03T09:52:51.340Z\",\"caller\":\"clientv3/maintenance.go:200\",\"msg\":\"opened snapshot stream; downloading\"}\n{\"level\":\"info\",\"ts\":1612345971.3406882,\"caller\":\"snapshot/v3_snapshot.go:127\",\"msg\":\"fetching snapshot\",\"endpoint\":\"https://192.168.52.12:2379\"}\n{\"level\":\"info\",\"ts\":\"2021-02-03T09:52:52.975Z\",\"caller\":\"clientv3/maintenance.go:208\",\"msg\":\"completed snapshot read; closing\"}\n{\"level\":\"info\",\"ts\":1612345973.1738656,\"caller\":\"snapshot/v3_snapshot.go:142\",\"msg\":\"fetched snapshot\",\"endpoint\":\"https://192.168.52.12:2379\",\"size\":\"108 MB\",\"took\":1.84332374}\n{\"level\":\"info\",\"ts\":1612345973.174559,\"caller\":\"snapshot/v3_snapshot.go:152\",\"msg\":\"saved\",\"path\":\"/etcd-backup//snapshot_2021-02-03_095251.db\"}\nSnapshot saved at /etcd-backup//snapshot_2021-02-03_095251.db\nsnapshot db and kube resources are successfully saved to /etcd-backup/\n</code></pre></p>","tags":["backup","etcd"]},{"location":"cluster-lifecycle/monitoring/metrics/","title":"Metrics","text":""},{"location":"cluster-lifecycle/monitoring/metrics/#list-metrics-of-a-node-or-pod","title":"List metrics of a node or pod,...","text":"<pre><code>$ oc adm top pod\nNAME                              CPU(cores)   MEMORY(bytes)\nrouter-default-7469c545dd-d795m   2m           31Mi\nrouter-default-7469c545dd-q9zrz   2m           32Mi\n\n$ oc adm top node\nNAME        CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%\ncompute-0   131m         8%     920Mi           13%\ncompute-1   139m         9%     981Mi           14%\ncompute-2   251m         16%    2135Mi          31%\nmaster-0    794m         22%    3311Mi          22%\nmaster-1    1199m        34%    4556Mi          30%\nmaster-2    885m         25%    4669Mi          31%\n</code></pre>"},{"location":"cluster-lifecycle/monitoring/metrics/#fetch-metrics-api-inside-a-pod","title":"Fetch Metrics API inside a POD","text":"<pre><code>oc create serviceaccount viewer\noc policy add-role-to-user view -z viewer\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: cli\n  name: cli\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cli\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: cli\n    spec:\n      serviceAccount: viewer\n      serviceAccountName: viewer\n      containers:\n      - image: image-registry.openshift-image-registry.svc:5000/openshift/cli:latest\n        imagePullPolicy: Always\n        name: cli\n        # command: [\"/usr/bin/sleep\",\"infinity\"]\n        command:\n          - /usr/bin/sh\n          - -c\n          - |\n            while true; do\n              date;\n              echo \"====== OC ======\"\n              oc adm top pod\n              echo \"====== cURL =====\"\n              curl -s --cacert /run/secrets/kubernetes.io/serviceaccount/ca.crt \\\n                --header \"Authorization: Bearer $(cat /run/secrets/kubernetes.io/serviceaccount/token)\" \\\n                https://kubernetes.default.svc.cluster.local/apis/metrics.k8s.io/v1beta1/namespaces/$KUBERNETES_NAMESPACE/pods ;\n              sleep 5;\n            done\n        # https://access.redhat.com/solutions/5175931\n        env:\n          - name: HOME\n            value: /tmp\n          - name: KUBERNETES_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n</code></pre>"},{"location":"control-plane/","title":"Control plane","text":"","tags":["control-plane"]},{"location":"control-plane/#content","title":"Content","text":"<ul> <li>Restoring etcd quorum</li> </ul>","tags":["control-plane"]},{"location":"control-plane/lost-quorum/","title":"Control plane node failure test","text":"<p>Let's test how to restore a control-plane from a lost quorum. Losing the majority of control plane nodes leads to a quorum loss</p> <p>Tested with OpenShift Cluster Version 4.17.0.</p> <p>With OCPSTRAT-539 there will be a improvement of the process. Hopefully, land in 4.18!</p> <p>Useful etcd commands</p> <ul> <li>Member list</li> </ul> <pre><code>etcdctl member list -w table\n</code></pre> <ul> <li>Endpoint status</li> </ul> <pre><code>etcdctl endpoint status --cluster -w table\n</code></pre> <ul> <li>Endpoint health</li> </ul> <pre><code>etcdctl endpoint health --cluster -w table\n</code></pre>","tags":["etcd","control-plane","v4.17"]},{"location":"control-plane/lost-quorum/#control-plane-overview","title":"Control plane overview","text":"<p>Date:</p> <ul> <li>UTC: <code>2025-01-08 13:51:23 +0000</code></li> <li>CET: <code>2025-01-08 14:51:35 +0100</code></li> </ul> Node IP Mac Leader API VIP cp-1 (0) 10.32.105.69 0E:C0:EF:20:69:45 \u26aa\ufe0f \u26aa\ufe0f cp-2 (1) 10.32.105.70 0E:C0:EF:20:69:46 \u2705 \u26aa\ufe0f cp-3 (2) 10.32.105.71 0E:C0:EF:20:69:47 \u26aa\ufe0f \u2705 <p>Test Workload</p> Time API(ping/https) WebUI(ping/https) App (ping/https) VM(ping/https) <code>2025-01-08 13:51:23 +0000</code> \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 etcdctl endpoint status --cluster -w table <pre><code>sh-5.1#   etcdctl endpoint status --cluster -w table\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n| https://10.32.105.70:2379 | 4fb023e0ba979504 |  3.5.14 |  133 MB |     false |      false |        21 |     803932 |             803932 |        |\n| https://10.32.105.69:2379 | 8f13951319793d10 |  3.5.14 |  132 MB |      true |      false |        21 |     803932 |             803932 |        |\n| https://10.32.105.71:2379 | ced393e846a090ee |  3.5.14 |  135 MB |     false |      false |        21 |     803932 |             803932 |        |\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n</code></pre>","tags":["etcd","control-plane","v4.17"]},{"location":"control-plane/lost-quorum/#one-control-plane-node-stopped","title":"One control plane node stopped","text":"<p>Date CET: <code>2025-01-08 15:06:37 +0100</code></p> <pre><code>isar # cdate; virtctl stop -n stormshift-ocp1-infra ocp1-cp-1\n2025-01-08 15:06:37 +0100\nVM ocp1-cp-2 was scheduled to stop\nisar #\n</code></pre> Node IP Mac Leader API VIP cp-1 (0) 10.32.105.69 0E:C0:EF:20:69:45 \u26aa\ufe0f \u26aa\ufe0f cp-2 (1) 10.32.105.70 0E:C0:EF:20:69:46 \u2705 \u2705 cp-3 (2) 10.32.105.71 0E:C0:EF:20:69:47 \ud83d\udd34 \ud83d\udd34 <p>Test Workload</p> Time API(ping/https) WebUI(ping/https) App (ping/https) VM(ping/https) <code>2025-01-08 13:51:23 +0000</code> \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 etcdctl endpoint status --cluster -w table <pre><code>sh-5.1# etcdctl endpoint status --cluster -w table\n{\"level\":\"warn\",\"ts\":\"2025-01-08T14:09:50.533659Z\",\"logger\":\"etcd-client\",\"caller\":\"v3@v3.5.14/retry_interceptor.go:63\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc000354000/10.32.105.69:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \\\"transport: Error while dialing: dial tcp 10.32.105.71:2379: connect: no route to host\\\"\"}\nFailed to get the status of endpoint https://10.32.105.71:2379 (context deadline exceeded)\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n| https://10.32.105.70:2379 | 4fb023e0ba979504 |  3.5.14 |  134 MB |     false |      false |        21 |     817949 |             817949 |        |\n| https://10.32.105.69:2379 | 8f13951319793d10 |  3.5.14 |  134 MB |      true |      false |        21 |     817949 |             817949 |        |\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\nsh-5.1#\n</code></pre> etcdctl endpoint health  -w table <pre><code>sh-5.1# etcdctl endpoint health --cluster -w table\n{\"level\":\"warn\",\"ts\":\"2025-01-08T14:11:01.430788Z\",\"logger\":\"etcd-client\",\"caller\":\"v3@v3.5.14/retry_interceptor.go:63\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc0003e0000/10.32.105.69:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = context deadline exceeded\"}\nError: failed to fetch endpoints from etcd cluster member list: context deadline exceeded\nsh-5.1# etcdctl endpoint health  -w table\n{\"level\":\"warn\",\"ts\":\"2025-01-08T14:11:19.330933Z\",\"logger\":\"client\",\"caller\":\"v3@v3.5.14/retry_interceptor.go:63\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc000018000/10.32.105.71:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = context deadline exceeded\"}\n+---------------------------+--------+--------------+---------------------------+\n|         ENDPOINT          | HEALTH |     TOOK     |           ERROR           |\n+---------------------------+--------+--------------+---------------------------+\n| https://10.32.105.70:2379 |   true | 1.086873919s |                           |\n| https://10.32.105.69:2379 |   true | 1.176393623s |                           |\n| https://10.32.105.71:2379 |  false | 5.002713425s | context deadline exceeded |\n+---------------------------+--------+--------------+---------------------------+\nError: unhealthy cluster\nsh-5.1#\n</code></pre> oc get nodes (stormshift-ocp1) <pre><code>stormshift-ocp1 # oc get nodes\nNAME            STATUS     ROLES                  AGE   VERSION\nocp1-cp-1       Ready      control-plane,master   20h   v1.30.4\nocp1-cp-2       Ready      control-plane,master   21h   v1.30.4\nocp1-cp-3       NotReady   control-plane,master   21h   v1.30.4\nocp1-worker-1   Ready      worker                 19h   v1.30.4\nocp1-worker-2   Ready      worker                 18h   v1.30.4\nocp1-worker-3   Ready      worker                 18h   v1.30.4\nstormshift-ocp1 #\n</code></pre>","tags":["etcd","control-plane","v4.17"]},{"location":"control-plane/lost-quorum/#two-control-plane-nodes-stopped","title":"Two control plane nodes stopped","text":"<p>Date CET: <code>2025-01-08 15:16:06 +0100</code></p> <pre><code>isar # cdate; virtctl stop -n stormshift-ocp1-infra ocp1-cp-1\n2025-01-08 15:16:06 +0100\nVM ocp1-cp-1 was scheduled to stop\nisar #\n</code></pre> Node IP Mac Leader API VIP cp-1 (0) 10.32.105.69 0E:C0:EF:20:69:45 \u26aa\ufe0f \u26aa\ufe0f cp-2 (1) 10.32.105.70 0E:C0:EF:20:69:46 \ud83d\udd34 \ud83d\udd34 cp-3 (2) 10.32.105.71 0E:C0:EF:20:69:47 \ud83d\udd34 \ud83d\udd34 <p>Test Workload</p> Time API(ping/https) WebUI(ping/https) App (ping/https) VM(ping/https) <code>2025-01-08 13:51:23 +0000</code> \ud83d\udfe2 \ud83d\udd34 \ud83d\udfe2 \ud83d\udfe2* \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 <ul> <li>OpenShift Web console is available. But the React application cannot connect to Kubernetes/OpenShift API any more.</li> <li>Control plane is read-only == offline / not available</li> <li>Workload is still running as expected</li> </ul> etcdctl endpoint status --cluster -w table <p>etcd pods are not available via API anymore!</p> <pre><code>% ssh -l core -i ~/.ssh/coe-muc 10.32.105.69\n[core@ocp1-cp-1 ~]$ sudo su -\n[root@ocp1-cp-1 ~]# crictl ps --name \"^etcd$\"\nCONTAINER           IMAGE                                                              CREATED             STATE               NAME                ATTEMPT             POD ID              POD\n12b84d7a637bd       3e2541880f59e43ba27714033241c45e0be80f0950a8d6e2fe5cfe5df86a800a   21 hours ago        Running             etcd                0                   43d64c15c722f       etcd-ocp1-cp-1\n[root@ocp1-cp-1 ~]# crictl exec -ti 12b84d7a637bd bash\n[root@ocp1-cp-1 /]# etcdctl endpoint status -w table\n{\"level\":\"warn\",\"ts\":\"2025-01-08T14:24:29.584514Z\",\"logger\":\"etcd-client\",\"caller\":\"v3@v3.5.14/retry_interceptor.go:63\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc000028000/10.32.105.69:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \\\"transport: Error while dialing: dial tcp 10.32.105.70:2379: connect: no route to host\\\"\"}\nFailed to get the status of endpoint https://10.32.105.70:2379 (context deadline exceeded)\n{\"level\":\"warn\",\"ts\":\"2025-01-08T14:24:34.585233Z\",\"logger\":\"etcd-client\",\"caller\":\"v3@v3.5.14/retry_interceptor.go:63\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc000028000/10.32.105.69:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \\\"transport: Error while dialing: dial tcp 10.32.105.71:2379: connect: no route to host\\\"\"}\nFailed to get the status of endpoint https://10.32.105.71:2379 (context deadline exceeded)\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+-----------------------+\n|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX |        ERRORS         |\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+-----------------------+\n| https://10.32.105.69:2379 | 8f13951319793d10 |  3.5.14 |  135 MB |     false |      false |        21 |     822965 |             822965 | etcdserver: no leader |\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+-----------------------+\n</code></pre> etcdctl endpoint health  -w table <pre><code>[root@ocp1-cp-1 /]# etcdctl endpoint health  -w table\n{\"level\":\"warn\",\"ts\":\"2025-01-08T14:25:14.567156Z\",\"logger\":\"client\",\"caller\":\"v3@v3.5.14/retry_interceptor.go:63\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc00037e000/10.32.105.69:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = context deadline exceeded\"}\n{\"level\":\"warn\",\"ts\":\"2025-01-08T14:25:14.567096Z\",\"logger\":\"client\",\"caller\":\"v3@v3.5.14/retry_interceptor.go:63\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc000018000/10.32.105.70:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \\\"transport: Error while dialing: dial tcp 10.32.105.70:2379: connect: no route to host\\\"\"}\n{\"level\":\"warn\",\"ts\":\"2025-01-08T14:25:14.567096Z\",\"logger\":\"client\",\"caller\":\"v3@v3.5.14/retry_interceptor.go:63\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc0000de000/10.32.105.71:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \\\"transport: Error while dialing: dial tcp 10.32.105.71:2379: connect: no route to host\\\"\"}\n+---------------------------+--------+--------------+---------------------------+\n|         ENDPOINT          | HEALTH |     TOOK     |           ERROR           |\n+---------------------------+--------+--------------+---------------------------+\n| https://10.32.105.69:2379 |  false | 5.000376526s | context deadline exceeded |\n| https://10.32.105.70:2379 |  false | 5.000426348s | context deadline exceeded |\n| https://10.32.105.71:2379 |  false | 5.000456502s | context deadline exceeded |\n+---------------------------+--------+--------------+---------------------------+\nError: unhealthy cluster\n[root@ocp1-cp-1 /]#\n</code></pre> oc get nodes (stormshift-ocp1) <pre><code>stormshift-ocp1 # oc get nodes\nE0108 15:18:32.888771   15036 memcache.go:265] couldn't get current server API group list: Get \"https://api.ocp1.stormshift.coe.muc.redhat.com:6443/api?timeout=32s\": EOF - error from a previous attempt: read tcp 10.45.224.32:49580-&gt;10.32.105.64:6443: read: connection reset by peer\nE0108 15:18:43.957199   15036 memcache.go:265] couldn't get current server API group list: Get \"https://api.ocp1.stormshift.coe.muc.redhat.com:6443/api?timeout=32s\": EOF\n</code></pre>","tags":["etcd","control-plane","v4.17"]},{"location":"control-plane/lost-quorum/#start-the-recovery-process","title":"Start the recovery process","text":"<p>Documentation: 5.3.2. Restoring to a previous cluster state</p>","tags":["etcd","control-plane","v4.17"]},{"location":"control-plane/lost-quorum/#run-etcd-backup","title":"Run etcd Backup","text":"<p>Documentation: 5.1.1. Backing up etcd data</p> <p>You have to run the backup script wiht <code>--force</code> because API is not available.</p> /usr/local/bin/cluster-backup.sh <pre><code>[root@ocp1-cp-1 ~]# date +\"%F %T %z\"\n2025-01-08 14:31:59 +0000\n[root@ocp1-cp-1 ~]# /usr/local/bin/cluster-backup.sh /home/core/assets/backup\nCertificate /etc/kubernetes/static-pod-certs/configmaps/etcd-all-bundles/server-ca-bundle.crt is missing. Checking in different directory\nCertificate /etc/kubernetes/static-pod-resources/etcd-certs/configmaps/etcd-all-bundles/server-ca-bundle.crt found!\nError from server (Timeout): the server was unable to return a response in the time allotted, but may still be processing the request (get clusteroperators.config.openshift.io kube-apiserver)\nCould not find the status of the kube-apiserver. Check if the API server is running. Pass the --force flag to skip checks.\n[root@ocp1-cp-1 ~]# date +\"%F %T %z\"\n2025-01-08 14:33:17 +0000\n[root@ocp1-cp-1 ~]# /usr/local/bin/cluster-backup.sh --force /home/core/assets/backup\nCertificate /etc/kubernetes/static-pod-certs/configmaps/etcd-all-bundles/server-ca-bundle.crt is missing. Checking in different directory\nCertificate /etc/kubernetes/static-pod-resources/etcd-certs/configmaps/etcd-all-bundles/server-ca-bundle.crt found!\nfound latest kube-apiserver: /etc/kubernetes/static-pod-resources/kube-apiserver-pod-12\nfound latest kube-controller-manager: /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-10\nfound latest kube-scheduler: /etc/kubernetes/static-pod-resources/kube-scheduler-pod-9\nfound latest etcd: /etc/kubernetes/static-pod-resources/etcd-pod-10\nd3051a7ced55db5947f809d531d6821f310f422ad19eca1c5f87c83cd553e0c2\netcdctl version: 3.5.14\nAPI version: 3.5\n{\"level\":\"info\",\"ts\":\"2025-01-08T14:33:24.839593Z\",\"caller\":\"snapshot/v3_snapshot.go:65\",\"msg\":\"created temporary db file\",\"path\":\"/home/core/assets/backup/snapshot_2025-01-08_143323__POSSIBLY_DIRTY__.db.part\"}\n{\"level\":\"info\",\"ts\":\"2025-01-08T14:33:24.852422Z\",\"logger\":\"client\",\"caller\":\"v3@v3.5.14/maintenance.go:212\",\"msg\":\"opened snapshot stream; downloading\"}\n{\"level\":\"info\",\"ts\":\"2025-01-08T14:33:24.8525Z\",\"caller\":\"snapshot/v3_snapshot.go:73\",\"msg\":\"fetching snapshot\",\"endpoint\":\"https://10.32.105.69:2379\"}\n{\"level\":\"info\",\"ts\":\"2025-01-08T14:33:26.493909Z\",\"logger\":\"client\",\"caller\":\"v3@v3.5.14/maintenance.go:220\",\"msg\":\"completed snapshot read; closing\"}\n{\"level\":\"info\",\"ts\":\"2025-01-08T14:33:26.990149Z\",\"caller\":\"snapshot/v3_snapshot.go:88\",\"msg\":\"fetched snapshot\",\"endpoint\":\"https://10.32.105.69:2379\",\"size\":\"135 MB\",\"took\":\"2 seconds ago\"}\n{\"level\":\"info\",\"ts\":\"2025-01-08T14:33:26.990347Z\",\"caller\":\"snapshot/v3_snapshot.go:97\",\"msg\":\"saved\",\"path\":\"/home/core/assets/backup/snapshot_2025-01-08_143323__POSSIBLY_DIRTY__.db\"}\nSnapshot saved at /home/core/assets/backup/snapshot_2025-01-08_143323__POSSIBLY_DIRTY__.db\n{\"hash\":2206292178,\"revision\":739064,\"totalKey\":15086,\"totalSize\":134733824}\nsnapshot db and kube resources are successfully saved to /home/core/assets/backup\n[root@ocp1-cp-1 ~]#\n</code></pre>","tags":["etcd","control-plane","v4.17"]},{"location":"control-plane/lost-quorum/#continue","title":"Continue","text":"<p>Important points:</p> <ul> <li>A healthy control plane host to use as the recovery host.</li> </ul> Point 6. check keepalived <pre><code>[root@ocp1-cp-1 ~]# date +\"%F %T %z\"\n2025-01-08 14:39:01 +0000\n[root@ocp1-cp-1 ~]# ip -o address | grep 10.32.105.64\n5: br-ex    inet 10.32.105.64/32 scope global vip\\       valid_lft forever preferred_lft forever\n[root@ocp1-cp-1 ~]#\n</code></pre> Point 8. cluster-restore.sh <pre><code>[root@ocp1-cp-1 ~]# date +\"%F %T %z\"\n2025-01-08 14:40:33 +0000\n[root@ocp1-cp-1 ~]# /usr/local/bin/cluster-restore.sh /home/core/assets/backup\n1d3f2a4ed0f1f076267b6a2d51962bdb80f252bd9878ce8236f3148ab9bb2d61\netcdctl version: 3.5.14\nAPI version: 3.5\n{\"hash\":2206292178,\"revision\":739064,\"totalKey\":15086,\"totalSize\":134733824}\n...stopping kube-apiserver-pod.yaml\n...stopping kube-controller-manager-pod.yaml\n...stopping kube-scheduler-pod.yaml\nWaiting for container kube-controller-manager to stop\n.complete\nWaiting for container kube-apiserver to stop\n................................................................................................................................complete\nWaiting for container kube-scheduler to stop\ncomplete\n...stopping etcd-pod.yaml\nWaiting for container etcd to stop\n.complete\nWaiting for container etcdctl to stop\n............................complete\nWaiting for container etcd-metrics to stop\ncomplete\nWaiting for container etcd-readyz to stop\ncomplete\nMoving etcd data-dir /var/lib/etcd/member to /var/lib/etcd-backup\nstarting restore-etcd static pod\nstarting kube-apiserver-pod.yaml\nstatic-pod-resources/kube-apiserver-pod-12/kube-apiserver-pod.yaml\nstarting kube-controller-manager-pod.yaml\nstatic-pod-resources/kube-controller-manager-pod-10/kube-controller-manager-pod.yaml\nstarting kube-scheduler-pod.yaml\nstatic-pod-resources/kube-scheduler-pod-9/kube-scheduler-pod.yaml\n[root@ocp1-cp-1 ~]#\n</code></pre> <p>According to monitoring, API was online at <code>2025-01-08 15:44:01</code></p> oc get nodes (stormshift-ocp1) <pre><code>stormshift-ocp1 # oc get nodes\nNAME            STATUS     ROLES                  AGE   VERSION\nocp1-cp-1       Ready      control-plane,master   21h   v1.30.4\nocp1-cp-2       NotReady   control-plane,master   21h   v1.30.4\nocp1-cp-3       NotReady   control-plane,master   21h   v1.30.4\nocp1-worker-1   Ready      worker                 19h   v1.30.4\nocp1-worker-2   Ready      worker                 19h   v1.30.4\nocp1-worker-3   Ready      worker                 19h   v1.30.4\nstormshift-ocp1 #\n</code></pre> etcdctl endpoint status... <pre><code>sh-5.1# env | grep ETCDCTL\nETCDCTL_ENDPOINTS=https://10.32.105.69:2379,https://10.32.105.70:2379,https://10.32.105.71:2379\nETCDCTL_KEY=/etc/kubernetes/static-pod-certs/secrets/etcd-all-certs/etcd-peer-ocp1-cp-1.key\nETCDCTL_API=3\nETCDCTL_CACERT=/etc/kubernetes/static-pod-certs/configmaps/etcd-all-bundles/server-ca-bundle.crt\nETCDCTL_CERT=/etc/kubernetes/static-pod-certs/secrets/etcd-all-certs/etcd-peer-ocp1-cp-1.crt\nsh-5.1#\nsh-5.1# etcdctl endpoint status  -w table\n{\"level\":\"warn\",\"ts\":\"2025-01-08T14:48:49.981146Z\",\"logger\":\"etcd-client\",\"caller\":\"v3@v3.5.14/retry_interceptor.go:63\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc000026000/10.32.105.69:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \\\"transport: Error while dialing: dial tcp 10.32.105.70:2379: connect: no route to host\\\"\"}\nFailed to get the status of endpoint https://10.32.105.70:2379 (context deadline exceeded)\n{\"level\":\"warn\",\"ts\":\"2025-01-08T14:48:54.982148Z\",\"logger\":\"etcd-client\",\"caller\":\"v3@v3.5.14/retry_interceptor.go:63\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc000026000/10.32.105.69:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \\\"transport: Error while dialing: dial tcp 10.32.105.71:2379: connect: no route to host\\\"\"}\nFailed to get the status of endpoint https://10.32.105.71:2379 (context deadline exceeded)\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n| https://10.32.105.69:2379 | 79a42a230e96be95 |  3.5.14 |  135 MB |      true |      false |         2 |       4179 |               4178 |        |\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\nsh-5.1# etcdctl endpoint status --cluster -w table\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n| https://10.32.105.69:2379 | 79a42a230e96be95 |  3.5.14 |  135 MB |      true |      false |         2 |       4143 |               4140 |        |\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n</code></pre> etcdctl endpoint health  -w table <pre><code>sh-5.1# env | grep ETCDCTL\nETCDCTL_ENDPOINTS=https://10.32.105.69:2379,https://10.32.105.70:2379,https://10.32.105.71:2379\nETCDCTL_KEY=/etc/kubernetes/static-pod-certs/secrets/etcd-all-certs/etcd-peer-ocp1-cp-1.key\nETCDCTL_API=3\nETCDCTL_CACERT=/etc/kubernetes/static-pod-certs/configmaps/etcd-all-bundles/server-ca-bundle.crt\nETCDCTL_CERT=/etc/kubernetes/static-pod-certs/secrets/etcd-all-certs/etcd-peer-ocp1-cp-1.crt\nsh-5.1# etcdctl endpoint health  -w table\n{\"level\":\"warn\",\"ts\":\"2025-01-08T14:51:18.829117Z\",\"logger\":\"client\",\"caller\":\"v3@v3.5.14/retry_interceptor.go:63\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc000034000/10.32.105.71:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \\\"transport: Error while dialing: dial tcp 10.32.105.71:2379: connect: no route to host\\\"\"}\n{\"level\":\"warn\",\"ts\":\"2025-01-08T14:51:18.829126Z\",\"logger\":\"client\",\"caller\":\"v3@v3.5.14/retry_interceptor.go:63\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc0000be000/10.32.105.70:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \\\"transport: Error while dialing: dial tcp 10.32.105.70:2379: connect: no route to host\\\"\"}\n+---------------------------+--------+--------------+---------------------------+\n|         ENDPOINT          | HEALTH |     TOOK     |           ERROR           |\n+---------------------------+--------+--------------+---------------------------+\n| https://10.32.105.69:2379 |   true |  16.183827ms |                           |\n| https://10.32.105.71:2379 |  false | 5.003399674s | context deadline exceeded |\n| https://10.32.105.70:2379 |  false | 5.003434863s | context deadline exceeded |\n+---------------------------+--------+--------------+---------------------------+\nError: unhealthy cluster\nsh-5.1# etcdctl endpoint health --cluster -w table\n+---------------------------+--------+-------------+-------+\n|         ENDPOINT          | HEALTH |    TOOK     | ERROR |\n+---------------------------+--------+-------------+-------+\n| https://10.32.105.69:2379 |   true | 14.208768ms |       |\n+---------------------------+--------+-------------+-------+\nsh-5.1#\n</code></pre> Node IP Mac Leader API VIP cp-1 (0) 10.32.105.69 0E:C0:EF:20:69:45 \u2705 \u2705 cp-2 (1) 10.32.105.70 0E:C0:EF:20:69:46 \ud83d\udd34 \ud83d\udd34 cp-3 (2) 10.32.105.71 0E:C0:EF:20:69:47 \ud83d\udd34 \ud83d\udd34 <p>Test Workload</p> API(ping/https) WebUI(ping/https) App (ping/https) VM(ping/https) \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 <p>Contiue with steps in docs (kubelet restart, ovn-kubernetes, csr,...)</p>","tags":["etcd","control-plane","v4.17"]},{"location":"control-plane/lost-quorum/#add-two-new-control-plane-nodes","title":"Add two new control plane nodes","text":"Node IP Mac Leader API VIP cp-1 (0) 10.32.105.69 0E:C0:EF:20:69:45 \u2705 \u2705 cp-2 (1) 10.32.105.70 0E:C0:EF:20:69:46 \ud83d\udd34 \ud83d\udd34 cp-3 (2) 10.32.105.71 0E:C0:EF:20:69:47 \ud83d\udd34 \ud83d\udd34 cp-4 (4) 10.32.105.72 0E:C0:EF:20:69:48 \ud83d\udee0\ufe0f \ud83d\udee0\ufe0f cp-5 (5) 10.32.105.73 0E:C0:EF:20:69:49 \ud83d\udee0\ufe0f \ud83d\udee0\ufe0f API(ping/https) WebUI(ping/https) App (ping/https) VM(ping/https) \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 <ul> <li>We basically follow the steps: Cluster lifecycle -&gt; Add node</li> <li>\u2705 DNS (A / PTR) done</li> <li>\u2705 DHCP done</li> <li>\u2705 RHCOS Live ISO Uploaded (rhcos-417.94.202410090854-0-live.x86_64.iso)</li> <li>\u2705 control plane ignition file exported and available at <code>http://10.32.96.31/stormshift-ocp1-cp.ign</code></li> </ul>","tags":["etcd","control-plane","v4.17"]},{"location":"control-plane/lost-quorum/#add-cp-4","title":"Add cp-4","text":"","tags":["etcd","control-plane","v4.17"]},{"location":"control-plane/lost-quorum/#virtual-machine-at-isar-hosting-cluster","title":"Virtual Machine at ISAR / Hosting Cluster","text":"oc apply...ocp1-cp-4-vm.yaml <pre><code>isar # oc apply -n stormshift-ocp1-infra -f ocp1-cp-4-vm.yaml\nvirtualmachine.kubevirt.io/ocp1-cp-4 created\n</code></pre> <pre><code>---\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: ocp1-cp-4\n  namespace: stormshift-ocp1-infra\nspec:\n  dataVolumeTemplates:\n    - metadata:\n        creationTimestamp: null\n        name: ocp1-cp-4-root\n      spec:\n        source:\n          blank: {}\n        storage:\n          accessModes:\n            - ReadWriteMany\n          resources:\n            requests:\n              storage: 120Gi\n          storageClassName: coe-netapp-san\n  running: true\n  template:\n    metadata:\n      creationTimestamp: null\n    spec:\n      architecture: amd64\n      domain:\n        cpu:\n          cores: 8\n        devices:\n          disks:\n            - bootOrder: 1\n              disk:\n                bus: virtio\n              name: root\n            - bootOrder: 2\n              cdrom:\n                bus: sata\n              name: cdrom\n          interfaces:\n            - bridge: {}\n              macAddress: '0E:C0:EF:20:69:48'\n              model: virtio\n              name: coe\n        machine:\n          type: pc-q35-rhel9.4.0\n        memory:\n          guest: 16Gi\n        resources:\n          limits:\n            memory: 16706Mi\n          requests:\n            memory: 16Gi\n      networks:\n        - multus:\n            networkName: coe-bridge\n          name: coe\n      volumes:\n        - name: cdrom\n          persistentVolumeClaim:\n            claimName: rhcos-417-94-202410090854-0-live\n        - dataVolume:\n            name: ocp1-cp-4-root\n          name: root\n</code></pre> <p>Connect to console an run:</p> <pre><code>curl -L -O http://10.32.96.31/stormshift-ocp1-cp.ign\nsudo coreos-installer install -i stormshift-ocp1-cp.ign /dev/vda\nsudo reboot\n</code></pre> <p>Approve CSR at <code>stormshift-ocp1</code></p> <pre><code>oc get csr | awk '/Pending/ { print $1 }' | xargs oc adm certificate approve\n</code></pre> oc get nodes (stormshift-ocp1) <pre><code>stormshift-ocp1 # oc get nodes\nNAME            STATUS     ROLES                  AGE     VERSION\nocp1-cp-1       Ready      control-plane,master   22h     v1.30.4\nocp1-cp-2       NotReady   control-plane,master   22h     v1.30.4\nocp1-cp-3       NotReady   control-plane,master   22h     v1.30.4\nocp1-cp-4       Ready      control-plane,master   9m10s   v1.30.4\nocp1-worker-1   Ready      worker                 20h     v1.30.4\nocp1-worker-2   Ready      worker                 20h     v1.30.4\nocp1-worker-3   Ready      worker                 20h     v1.30.4\nstormshift-ocp1 #\n</code></pre>","tags":["etcd","control-plane","v4.17"]},{"location":"control-plane/lost-quorum/#baremetalhost-bmh","title":"BareMetalHost (BMH)","text":"oc apply...ocp1-cp-4-bmh.yaml <pre><code>stormshift-ocp1 # oc apply -f ocp1-cp-4-bmh.yaml\nbaremetalhost.metal3.io/ocp1-cp-4 created\n</code></pre> <pre><code>---\napiVersion: metal3.io/v1alpha1\nkind: BareMetalHost\nmetadata:\n  name: ocp1-cp-4\n  namespace: openshift-machine-api\nspec:\n  automatedCleaningMode: metadata\n  bootMACAddress: 0E:C0:EF:20:69:48\n  bootMode: legacy\n  customDeploy:\n    method: install_coreos\n  externallyProvisioned: true\n  online: true\n  userData:\n    name: master-user-data-managed\n    namespace: openshift-machine-api\n</code></pre> oc get bmh -n openshift-machine-api <pre><code>stormshift-ocp1 # oc get bmh -n openshift-machine-api\nNAME            STATE       CONSUMER                    ONLINE   ERROR   AGE\nocp1-cp-1       unmanaged   ocp1-g6vbv-master-0         true             22h\nocp1-cp-2       unmanaged   ocp1-g6vbv-master-1         true             22h\nocp1-cp-3       unmanaged   ocp1-g6vbv-master-2         true             22h\nocp1-cp-4       unmanaged   ocp1-cp-4                   true             54s\nocp1-worker-1   unmanaged   ocp1-g6vbv-worker-0-jmj97   true             22h\nocp1-worker-2   unmanaged   ocp1-g6vbv-worker-0-wnbqv   true             22h\nocp1-worker-3   unmanaged   ocp1-g6vbv-worker-0-zszgr   true             22h\nstormshift-ocp1 #\n</code></pre>","tags":["etcd","control-plane","v4.17"]},{"location":"control-plane/lost-quorum/#machine","title":"Machine","text":"oc apply...ocp1-cp-4-machine.yaml <pre><code>stormshift-ocp1 # oc apply -f ocp1-cp-4-machine.yaml\nmachine.machine.openshift.io/ocp1-cp-4 created\n</code></pre> <pre><code>---\napiVersion: machine.openshift.io/v1beta1\nkind: Machine\nmetadata:\n  annotations:\n    machine.openshift.io/instance-state: externally provisioned\n    metal3.io/BareMetalHost: openshift-machine-api/ocp1-cp-4\n  labels:\n    machine.openshift.io/cluster-api-cluster: ocp1-g6vbv\n    machine.openshift.io/cluster-api-machine-role: master\n    machine.openshift.io/cluster-api-machine-type: master\n  name: ocp1-cp-4\n  namespace: openshift-machine-api\nspec:\n  metadata: {}\n  providerSpec:\n    value:\n      apiVersion: baremetal.cluster.k8s.io/v1alpha1\n      customDeploy:\n        method: install_coreos\n      hostSelector: {}\n      image:\n        checksum: \"\"\n        url: \"\"\n      kind: BareMetalMachineProviderSpec\n      metadata:\n        creationTimestamp: null\n      userData:\n        name: master-user-data-managed\n</code></pre> oc get machine -n openshift-machine-api <pre><code>stormshift-ocp1 # oc get machine -n openshift-machine-api\nNAME                        PHASE         TYPE   REGION   ZONE   AGE\nocp1-cp-4                   Provisioned                          2m29s\nocp1-g6vbv-master-0         Running                              22h\nocp1-g6vbv-master-1         Running                              22h\nocp1-g6vbv-master-2         Running                              22h\nocp1-g6vbv-worker-0-jmj97   Running                              22h\nocp1-g6vbv-worker-0-wnbqv   Running                              22h\nocp1-g6vbv-worker-0-zszgr   Running                              22h\nstormshift-ocp1 #\n</code></pre>","tags":["etcd","control-plane","v4.17"]},{"location":"control-plane/lost-quorum/#link-machine-baremetalhost","title":"Link Machine &amp; BareMetalHost","text":"<p>Open API proxy in on terminal</p> <pre><code>oc proxy\n</code></pre> <p>Patch object in another terminal</p> Patch the status field of bmh object <pre><code>export HOST_PROXY_API_PATH=\"http://127.0.0.1:8001/apis/metal3.io/v1alpha1/namespaces/openshift-machine-api/baremetalhosts\"\n\nread -r -d '' host_patch &lt;&lt; EOF\n{\n\"status\": {\n    \"hardware\": {\n    \"nics\": [\n        {\n        \"ip\": \"10.32.105.72\",\n        \"mac\": \"0E:C0:EF:20:69:48\"\n        }\n    ]\n    }\n}\n}\nEOF\n\ncurl -vv \\\n    -X PATCH \\\n    \"${HOST_PROXY_API_PATH}/ocp1-cp-4/status\" \\\n    -H \"Content-type: application/merge-patch+json\" \\\n    -d \"${host_patch}\"\n</code></pre> oc get bmh,machine -n openshift-machine-api <pre><code>stormshift-ocp1 # oc get bmh,machine -n openshift-machine-api\nNAME                                    STATE       CONSUMER                    ONLINE   ERROR   AGE\nbaremetalhost.metal3.io/ocp1-cp-1       unmanaged   ocp1-g6vbv-master-0         true             22h\nbaremetalhost.metal3.io/ocp1-cp-2       unmanaged   ocp1-g6vbv-master-1         true             22h\nbaremetalhost.metal3.io/ocp1-cp-3       unmanaged   ocp1-g6vbv-master-2         true             22h\nbaremetalhost.metal3.io/ocp1-cp-4       unmanaged   ocp1-cp-4                   true             10m\nbaremetalhost.metal3.io/ocp1-worker-1   unmanaged   ocp1-g6vbv-worker-0-jmj97   true             22h\nbaremetalhost.metal3.io/ocp1-worker-2   unmanaged   ocp1-g6vbv-worker-0-wnbqv   true             22h\nbaremetalhost.metal3.io/ocp1-worker-3   unmanaged   ocp1-g6vbv-worker-0-zszgr   true             22h\n\nNAME                                                     PHASE         TYPE   REGION   ZONE   AGE\nmachine.machine.openshift.io/ocp1-cp-4                   Running                              10m\nmachine.machine.openshift.io/ocp1-g6vbv-master-0         Running                              22h\nmachine.machine.openshift.io/ocp1-g6vbv-master-1         Running                              22h\nmachine.machine.openshift.io/ocp1-g6vbv-master-2         Running                              22h\nmachine.machine.openshift.io/ocp1-g6vbv-worker-0-jmj97   Running                              22h\nmachine.machine.openshift.io/ocp1-g6vbv-worker-0-wnbqv   Running                              22h\nmachine.machine.openshift.io/ocp1-g6vbv-worker-0-zszgr   Running                              22h\nstormshift-ocp1 #\n</code></pre>","tags":["etcd","control-plane","v4.17"]},{"location":"control-plane/lost-quorum/#validate-etcd","title":"Validate etcd","text":"etcdctl... <pre><code>oc rsh etcd-ocp1-cp-1\nsh-5.1# etcdctl endpoint health  -w table\n{\"level\":\"warn\",\"ts\":\"2025-01-08T15:28:51.780019Z\",\"logger\":\"client\",\"caller\":\"v3@v3.5.14/retry_interceptor.go:63\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc000340000/10.32.105.71:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \\\"transport: Error while dialing: dial tcp 10.32.105.71:2379: connect: no route to host\\\"\"}\n{\"level\":\"warn\",\"ts\":\"2025-01-08T15:28:51.780366Z\",\"logger\":\"client\",\"caller\":\"v3@v3.5.14/retry_interceptor.go:63\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc000194000/10.32.105.70:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \\\"transport: Error while dialing: dial tcp 10.32.105.70:2379: connect: no route to host\\\"\"}\n+---------------------------+--------+--------------+---------------------------+\n|         ENDPOINT          | HEALTH |     TOOK     |           ERROR           |\n+---------------------------+--------+--------------+---------------------------+\n| https://10.32.105.69:2379 |   true |  14.545613ms |                           |\n| https://10.32.105.71:2379 |  false | 5.001124017s | context deadline exceeded |\n| https://10.32.105.70:2379 |  false | 5.001175568s | context deadline exceeded |\n+---------------------------+--------+--------------+---------------------------+\nError: unhealthy cluster\nsh-5.1# etcdctl endpoint health --cluster -w table\n+---------------------------+--------+-------------+-------+\n|         ENDPOINT          | HEALTH |    TOOK     | ERROR |\n+---------------------------+--------+-------------+-------+\n| https://10.32.105.69:2379 |   true | 12.013929ms |       |\n| https://10.32.105.72:2379 |   true | 17.867394ms |       |\n+---------------------------+--------+-------------+-------+\nsh-5.1# etcdctl endpoint status --cluster -w table\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n| https://10.32.105.72:2379 |  85cab4c209d910e |  3.5.14 |  139 MB |     false |      false |         2 |      30900 |              30900 |        |\n| https://10.32.105.69:2379 | 79a42a230e96be95 |  3.5.14 |  140 MB |      true |      false |         2 |      30900 |              30900 |        |\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\nsh-5.1# etcdctl member list  -w table\n+------------------+---------+-----------+---------------------------+---------------------------+------------+\n|        ID        | STATUS  |   NAME    |        PEER ADDRS         |       CLIENT ADDRS        | IS LEARNER |\n+------------------+---------+-----------+---------------------------+---------------------------+------------+\n|  85cab4c209d910e | started | ocp1-cp-4 | https://10.32.105.72:2380 | https://10.32.105.72:2379 |      false |\n| 79a42a230e96be95 | started | ocp1-cp-1 | https://10.32.105.69:2380 | https://10.32.105.69:2379 |      false |\n+------------------+---------+-----------+---------------------------+---------------------------+------------+\nsh-5.1#\nsh-5.1# env |grep ETCDCTL\nETCDCTL_ENDPOINTS=https://10.32.105.69:2379,https://10.32.105.70:2379,https://10.32.105.71:2379\nETCDCTL_KEY=/etc/kubernetes/static-pod-certs/secrets/etcd-all-certs/etcd-peer-ocp1-cp-1.key\nETCDCTL_API=3\nETCDCTL_CACERT=/etc/kubernetes/static-pod-certs/configmaps/etcd-all-bundles/server-ca-bundle.crt\nETCDCTL_CERT=/etc/kubernetes/static-pod-certs/secrets/etcd-all-certs/etcd-peer-ocp1-cp-1.crt\n</code></pre> oc get pods -n openshift-etcd -o wide <pre><code>stormshift-ocp1 # oc get pods -n openshift-etcd -o wide\nNAME                           READY   STATUS      RESTARTS   AGE     IP             NODE        NOMINATED NODE   READINESS GATES\netcd-guard-ocp1-cp-1           0/1     Running     0          22h     10.130.0.15    ocp1-cp-1   &lt;none&gt;           &lt;none&gt;\netcd-guard-ocp1-cp-2           1/1     Running     0          22h     10.128.0.17    ocp1-cp-2   &lt;none&gt;           &lt;none&gt;\netcd-guard-ocp1-cp-3           1/1     Running     0          22h     10.129.0.54    ocp1-cp-3   &lt;none&gt;           &lt;none&gt;\netcd-guard-ocp1-cp-4           1/1     Running     0          9m4s    10.130.2.24    ocp1-cp-4   &lt;none&gt;           &lt;none&gt;\netcd-ocp1-cp-1                 1/1     Running     0          48m     10.32.105.69   ocp1-cp-1   &lt;none&gt;           &lt;none&gt;\netcd-ocp1-cp-2                 4/4     Running     0          22h     10.32.105.70   ocp1-cp-2   &lt;none&gt;           &lt;none&gt;\netcd-ocp1-cp-3                 4/4     Running     0          21h     10.32.105.71   ocp1-cp-3   &lt;none&gt;           &lt;none&gt;\netcd-ocp1-cp-4                 4/4     Running     0          9m9s    10.32.105.72   ocp1-cp-4   &lt;none&gt;           &lt;none&gt;\ninstaller-10-ocp1-cp-1         0/1     Completed   0          21h     &lt;none&gt;         ocp1-cp-1   &lt;none&gt;           &lt;none&gt;\ninstaller-10-ocp1-cp-2         0/1     Completed   0          22h     &lt;none&gt;         ocp1-cp-2   &lt;none&gt;           &lt;none&gt;\ninstaller-10-ocp1-cp-3         0/1     Completed   0          21h     &lt;none&gt;         ocp1-cp-3   &lt;none&gt;           &lt;none&gt;\ninstaller-12-ocp1-cp-4         0/1     Completed   0          13m     10.130.2.3     ocp1-cp-4   &lt;none&gt;           &lt;none&gt;\ninstaller-14-ocp1-cp-2         0/1     Pending     0          8m8s    &lt;none&gt;         ocp1-cp-2   &lt;none&gt;           &lt;none&gt;\ninstaller-7-ocp1-cp-1          0/1     Completed   0          22h     &lt;none&gt;         ocp1-cp-1   &lt;none&gt;           &lt;none&gt;\ninstaller-9-ocp1-cp-1          0/1     Completed   0          22h     &lt;none&gt;         ocp1-cp-1   &lt;none&gt;           &lt;none&gt;\ninstaller-9-ocp1-cp-2          0/1     Completed   0          22h     &lt;none&gt;         ocp1-cp-2   &lt;none&gt;           &lt;none&gt;\ninstaller-9-ocp1-cp-3          0/1     Completed   0          22h     &lt;none&gt;         ocp1-cp-3   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-10-ocp1-cp-1   0/1     Completed   0          22h     &lt;none&gt;         ocp1-cp-1   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-10-ocp1-cp-2   0/1     Completed   0          22h     &lt;none&gt;         ocp1-cp-2   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-10-ocp1-cp-3   0/1     Completed   0          22h     &lt;none&gt;         ocp1-cp-3   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-10-ocp1-cp-4   0/1     Completed   0          14m     10.130.2.6     ocp1-cp-4   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-11-ocp1-cp-1   0/1     Completed   0          14m     &lt;none&gt;         ocp1-cp-1   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-11-ocp1-cp-2   0/1     Pending     0          14m     &lt;none&gt;         ocp1-cp-2   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-11-ocp1-cp-3   0/1     Pending     0          14m     &lt;none&gt;         ocp1-cp-3   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-11-ocp1-cp-4   0/1     Completed   0          14m     10.130.2.11    ocp1-cp-4   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-12-ocp1-cp-1   0/1     Completed   0          13m     &lt;none&gt;         ocp1-cp-1   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-12-ocp1-cp-2   0/1     Pending     0          14m     &lt;none&gt;         ocp1-cp-2   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-12-ocp1-cp-3   0/1     Pending     0          13m     &lt;none&gt;         ocp1-cp-3   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-12-ocp1-cp-4   0/1     Completed   0          13m     10.130.2.13    ocp1-cp-4   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-13-ocp1-cp-1   0/1     Completed   0          8m15s   &lt;none&gt;         ocp1-cp-1   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-13-ocp1-cp-2   0/1     Pending     0          8m23s   &lt;none&gt;         ocp1-cp-2   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-13-ocp1-cp-3   0/1     Pending     0          8m19s   &lt;none&gt;         ocp1-cp-3   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-13-ocp1-cp-4   0/1     Completed   0          8m12s   10.130.2.27    ocp1-cp-4   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-14-ocp1-cp-1   0/1     Completed   0          8m5s    &lt;none&gt;         ocp1-cp-1   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-14-ocp1-cp-2   0/1     Pending     0          8m9s    &lt;none&gt;         ocp1-cp-2   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-14-ocp1-cp-3   0/1     Pending     0          8m7s    &lt;none&gt;         ocp1-cp-3   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-14-ocp1-cp-4   0/1     Completed   0          8m3s    10.130.2.28    ocp1-cp-4   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-9-ocp1-cp-1    0/1     Completed   0          22h     &lt;none&gt;         ocp1-cp-1   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-9-ocp1-cp-2    0/1     Completed   0          22h     &lt;none&gt;         ocp1-cp-2   &lt;none&gt;           &lt;none&gt;\nrevision-pruner-9-ocp1-cp-3    0/1     Completed   0          22h     &lt;none&gt;         ocp1-cp-3   &lt;none&gt;           &lt;none&gt;\nstormshift-ocp1 #\n</code></pre> <p>Delete the old control-plane artifacts.</p>","tags":["etcd","control-plane","v4.17"]},{"location":"control-plane/lost-quorum/#delete-two-old-control-plane-artifacts","title":"Delete two old control plane artifacts","text":"<ul> <li>Date: <code>2025-01-08 16:37:12 +0100</code></li> </ul> Tip <p>During this process, the API is not availale from time to time. Because of reconfiguration of etcd. I would recommend to do the cleanup at the end. After the recovery of all control plane nodes.</p> <pre><code>stormshift-ocp1 # oc delete node/ocp1-cp-2 node/ocp1-cp-3\nnode \"ocp1-cp-2\" deleted\nnode \"ocp1-cp-3\" deleted\nstormshift-ocp1 #\n</code></pre> oc get -n openshift-machine-api bmh,machine <pre><code>stormshift-ocp1 # oc get -n openshift-machine-api bmh,machine\nNAME                                    STATE       CONSUMER                    ONLINE   ERROR   AGE\nbaremetalhost.metal3.io/ocp1-cp-1       unmanaged   ocp1-g6vbv-master-0         true             22h\nbaremetalhost.metal3.io/ocp1-cp-4       unmanaged   ocp1-cp-4                   true             30m\nbaremetalhost.metal3.io/ocp1-worker-1   unmanaged   ocp1-g6vbv-worker-0-jmj97   true             22h\nbaremetalhost.metal3.io/ocp1-worker-2   unmanaged   ocp1-g6vbv-worker-0-wnbqv   true             22h\nbaremetalhost.metal3.io/ocp1-worker-3   unmanaged   ocp1-g6vbv-worker-0-zszgr   true             22h\n\nNAME                                                     PHASE     TYPE   REGION   ZONE   AGE\nmachine.machine.openshift.io/ocp1-cp-4                   Running                          30m\nmachine.machine.openshift.io/ocp1-g6vbv-master-0         Running                          22h\nmachine.machine.openshift.io/ocp1-g6vbv-worker-0-jmj97   Running                          22h\nmachine.machine.openshift.io/ocp1-g6vbv-worker-0-wnbqv   Running                          22h\nmachine.machine.openshift.io/ocp1-g6vbv-worker-0-zszgr   Running                          22h\nstormshift-ocp1 #\n</code></pre> oc get pods -n openshift-etcd <pre><code>stormshift-ocp1 # oc get pods -n openshift-etcd\nNAME                           READY   STATUS      RESTARTS   AGE\netcd-guard-ocp1-cp-1           1/1     Running     0          22h\netcd-guard-ocp1-cp-4           1/1     Running     0          13m\netcd-ocp1-cp-1                 4/4     Running     0          23s\netcd-ocp1-cp-4                 4/4     Running     0          13m\ninstaller-10-ocp1-cp-1         0/1     Completed   0          22h\ninstaller-12-ocp1-cp-4         0/1     Completed   0          18m\ninstaller-15-ocp1-cp-1         0/1     Completed   0          2m43s\ninstaller-15-ocp1-cp-4         1/1     Running     0          9s\ninstaller-9-ocp1-cp-1          0/1     Completed   0          22h\nrevision-pruner-10-ocp1-cp-1   0/1     Completed   0          22h\nrevision-pruner-10-ocp1-cp-4   0/1     Completed   0          19m\nrevision-pruner-11-ocp1-cp-1   0/1     Completed   0          18m\nrevision-pruner-11-ocp1-cp-4   0/1     Completed   0          18m\nrevision-pruner-12-ocp1-cp-1   0/1     Completed   0          18m\nrevision-pruner-12-ocp1-cp-4   0/1     Completed   0          18m\nrevision-pruner-13-ocp1-cp-1   0/1     Completed   0          12m\nrevision-pruner-13-ocp1-cp-4   0/1     Completed   0          12m\nrevision-pruner-14-ocp1-cp-1   0/1     Completed   0          12m\nrevision-pruner-14-ocp1-cp-4   0/1     Completed   0          12m\nrevision-pruner-15-ocp1-cp-1   0/1     Completed   0          2m44s\nrevision-pruner-15-ocp1-cp-4   0/1     Completed   0          2m41s\nrevision-pruner-9-ocp1-cp-1    0/1     Completed   0          22h\n</code></pre> etcdctl... <pre><code>oc rsh etcd-ocp1-cp-1\nsh-5.1# etcdctl endpoint health  -w table\n+---------------------------+--------+-------------+-------+\n|         ENDPOINT          | HEALTH |    TOOK     | ERROR |\n+---------------------------+--------+-------------+-------+\n| https://10.32.105.72:2379 |   true | 17.002588ms |       |\n| https://10.32.105.69:2379 |   true |  23.96274ms |       |\n+---------------------------+--------+-------------+-------+\nsh-5.1# etcdctl endpoint health --cluster -w table\n+---------------------------+--------+-------------+-------+\n|         ENDPOINT          | HEALTH |    TOOK     | ERROR |\n+---------------------------+--------+-------------+-------+\n| https://10.32.105.69:2379 |   true | 11.619398ms |       |\n| https://10.32.105.72:2379 |   true | 16.879731ms |       |\n+---------------------------+--------+-------------+-------+\nsh-5.1# etcdctl endpoint status --cluster -w table\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n| https://10.32.105.72:2379 |  85cab4c209d910e |  3.5.14 |  145 MB |     false |      false |         7 |      40236 |              40236 |        |\n| https://10.32.105.69:2379 | 79a42a230e96be95 |  3.5.14 |  145 MB |      true |      false |         7 |      40236 |              40236 |        |\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\nsh-5.1# etcdctl endpoint status  -w table\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n| https://10.32.105.69:2379 | 79a42a230e96be95 |  3.5.14 |  145 MB |      true |      false |         7 |      40264 |              40264 |        |\n| https://10.32.105.72:2379 |  85cab4c209d910e |  3.5.14 |  145 MB |     false |      false |         7 |      40264 |              40264 |        |\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\nsh-5.1# etcdctl member list  -w table\n+------------------+---------+-----------+---------------------------+---------------------------+------------+\n|        ID        | STATUS  |   NAME    |        PEER ADDRS         |       CLIENT ADDRS        | IS LEARNER |\n+------------------+---------+-----------+---------------------------+---------------------------+------------+\n|  85cab4c209d910e | started | ocp1-cp-4 | https://10.32.105.72:2380 | https://10.32.105.72:2379 |      false |\n| 79a42a230e96be95 | started | ocp1-cp-1 | https://10.32.105.69:2380 | https://10.32.105.69:2379 |      false |\n+------------------+---------+-----------+---------------------------+---------------------------+------------+\nsh-5.1# env | grep ETCDCTL\nETCDCTL_ENDPOINTS=https://10.32.105.69:2379,https://10.32.105.72:2379\nETCDCTL_KEY=/etc/kubernetes/static-pod-certs/secrets/etcd-all-certs/etcd-peer-ocp1-cp-1.key\nETCDCTL_API=3\nETCDCTL_CACERT=/etc/kubernetes/static-pod-certs/configmaps/etcd-all-bundles/server-ca-bundle.crt\nETCDCTL_CERT=/etc/kubernetes/static-pod-certs/secrets/etcd-all-certs/etcd-peer-ocp1-cp-1.crt\nsh-5.1#\n</code></pre> Cluster alert: <p>NoOvnClusterManagerLeader</p> <p>Networking control plane is degraded. Networking configuration updates applied to the cluster will not be implemented while there is no OVN Kubernetes cluster manager leader. Existing workloads should continue to have connectivity. OVN-Kubernetes control plane is not functional.</p> <p>Let's handle that later, first recovery the whole control plane!</p>","tags":["etcd","control-plane","v4.17"]},{"location":"control-plane/lost-quorum/#add-cp-5","title":"Add cp-5","text":"","tags":["etcd","control-plane","v4.17"]},{"location":"control-plane/lost-quorum/#virtual-machine-at-isar-hosting-cluster-cp-5","title":"Virtual Machine at ISAR / Hosting Cluster (cp-5)","text":"oc apply...ocp1-cp-5-vm.yaml <pre><code>isar # oc apply -n stormshift-ocp1-infra -f ocp1-cp-5-vm.yaml\nvirtualmachine.kubevirt.io/ocp1-cp-5 created\n</code></pre> <pre><code>---\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: ocp1-cp-5\n  namespace: stormshift-ocp1-infra\nspec:\n  dataVolumeTemplates:\n    - metadata:\n        creationTimestamp: null\n        name: ocp1-cp-5-root\n      spec:\n        source:\n          blank: {}\n        storage:\n          accessModes:\n            - ReadWriteMany\n          resources:\n            requests:\n              storage: 120Gi\n          storageClassName: coe-netapp-san\n  running: true\n  template:\n    metadata:\n      creationTimestamp: null\n    spec:\n      architecture: amd64\n      domain:\n        cpu:\n          cores: 8\n        devices:\n          disks:\n            - bootOrder: 1\n              disk:\n                bus: virtio\n              name: root\n            - bootOrder: 2\n              cdrom:\n                bus: sata\n              name: cdrom\n          interfaces:\n            - bridge: {}\n              macAddress: '0E:C0:EF:20:69:49'\n              model: virtio\n              name: coe\n        machine:\n          type: pc-q35-rhel9.4.0\n        memory:\n          guest: 16Gi\n        resources:\n          limits:\n            memory: 16706Mi\n          requests:\n            memory: 16Gi\n      networks:\n        - multus:\n            networkName: coe-bridge\n          name: coe\n      volumes:\n        - name: cdrom\n          persistentVolumeClaim:\n            claimName: rhcos-417-94-202410090854-0-live\n        - dataVolume:\n            name: ocp1-cp-5-root\n          name: root\n</code></pre> <p>Connect to console an run:</p> <pre><code>curl -L -O http://10.32.96.31/stormshift-ocp1-cp.ign\nsudo coreos-installer install -i stormshift-ocp1-cp.ign /dev/vda\nsudo reboot\n</code></pre> <p>Approve CSR at <code>stormshift-ocp1</code></p> <pre><code>oc get csr | awk '/Pending/ { print $1 }' | xargs oc adm certificate approve\n</code></pre> oc get nodes (stormshift-ocp1) <pre><code>stormshift-ocp1 # oc get nodes\nNAME            STATUS     ROLES                  AGE     VERSION\nocp1-cp-1       Ready      control-plane,master   22h     v1.30.4\nocp1-cp-2       NotReady   control-plane,master   22h     v1.30.4\nocp1-cp-3       NotReady   control-plane,master   22h     v1.30.4\nocp1-cp-5       Ready      control-plane,master   9m10s   v1.30.4\nocp1-worker-1   Ready      worker                 20h     v1.30.4\nocp1-worker-2   Ready      worker                 20h     v1.30.4\nocp1-worker-3   Ready      worker                 20h     v1.30.4\nstormshift-ocp1 #\n</code></pre>","tags":["etcd","control-plane","v4.17"]},{"location":"control-plane/lost-quorum/#baremetalhost-bmh-cp-5","title":"BareMetalHost (BMH) (cp-5)","text":"oc apply...ocp1-cp-5-bmh.yaml <pre><code>stormshift-ocp1 # oc apply -f ocp1-cp-5-bmh.yaml\nbaremetalhost.metal3.io/ocp1-cp-5 created\n</code></pre> <pre><code>---\napiVersion: metal3.io/v1alpha1\nkind: BareMetalHost\nmetadata:\n  name: ocp1-cp-5\n  namespace: openshift-machine-api\nspec:\n  automatedCleaningMode: metadata\n  bootMACAddress: 0E:C0:EF:20:69:49\n  bootMode: legacy\n  customDeploy:\n    method: install_coreos\n  externallyProvisioned: true\n  online: true\n  userData:\n    name: master-user-data-managed\n    namespace: openshift-machine-api\n</code></pre> oc get bmh -n openshift-machine-api <pre><code>stormshift-ocp1 # oc get bmh -n openshift-machine-api\nNAME            STATE       CONSUMER                    ONLINE   ERROR   AGE\nocp1-cp-1       unmanaged   ocp1-g6vbv-master-0         true             22h\nocp1-cp-4       unmanaged   ocp1-cp-4                   true             45m\nocp1-cp-5       unmanaged   ocp1-cp-5                   true             2m34s\nocp1-worker-1   unmanaged   ocp1-g6vbv-worker-0-jmj97   true             22h\nocp1-worker-2   unmanaged   ocp1-g6vbv-worker-0-wnbqv   true             22h\nocp1-worker-3   unmanaged   ocp1-g6vbv-worker-0-zszgr   true             22h\nstormshift-ocp1 #\n</code></pre>","tags":["etcd","control-plane","v4.17"]},{"location":"control-plane/lost-quorum/#machine-cp-5","title":"Machine (cp-5)","text":"oc apply...ocp1-cp-5-machine.yaml <pre><code>stormshift-ocp1 # oc apply -f ocp1-cp-5-machine.yaml\nmachine.machine.openshift.io/ocp1-cp-5 created\n</code></pre> <pre><code>---\napiVersion: machine.openshift.io/v1beta1\nkind: Machine\nmetadata:\n  annotations:\n    machine.openshift.io/instance-state: externally provisioned\n    metal3.io/BareMetalHost: openshift-machine-api/ocp1-cp-5\n  labels:\n    machine.openshift.io/cluster-api-cluster: ocp1-g6vbv\n    machine.openshift.io/cluster-api-machine-role: master\n    machine.openshift.io/cluster-api-machine-type: master\n  name: ocp1-cp-5\n  namespace: openshift-machine-api\nspec:\n  metadata: {}\n  providerSpec:\n    value:\n      apiVersion: baremetal.cluster.k8s.io/v1alpha1\n      customDeploy:\n        method: install_coreos\n      hostSelector: {}\n      image:\n        checksum: \"\"\n        url: \"\"\n      kind: BareMetalMachineProviderSpec\n      metadata:\n        creationTimestamp: null\n      userData:\n        name: master-user-data-managed\n</code></pre> oc get machine -n openshift-machine-api <pre><code>stormshift-ocp1 # oc get machine -n openshift-machine-api\nNAME                        PHASE         TYPE   REGION   ZONE   AGE\nocp1-cp-4                   Running                              45m\nocp1-cp-5                   Provisioned                          2m49s\nocp1-g6vbv-master-0         Running                              22h\nocp1-g6vbv-worker-0-jmj97   Running                              22h\nocp1-g6vbv-worker-0-wnbqv   Running                              22h\nocp1-g6vbv-worker-0-zszgr   Running                              22h\nstormshift-ocp1 #\n</code></pre>","tags":["etcd","control-plane","v4.17"]},{"location":"control-plane/lost-quorum/#link-machine-baremetalhost-cp-5","title":"Link Machine &amp; BareMetalHost (cp-5)","text":"<p>Open API proxy in on terminal</p> <pre><code>oc proxy\n</code></pre> <p>Patch object in another terminal</p> Patch the status field of bmh object <pre><code>export HOST_PROXY_API_PATH=\"http://127.0.0.1:8001/apis/metal3.io/v1alpha1/namespaces/openshift-machine-api/baremetalhosts\"\n\nread -r -d '' host_patch &lt;&lt; EOF\n{\n\"status\": {\n    \"hardware\": {\n    \"nics\": [\n        {\n        \"ip\": \"10.32.105.73\",\n        \"mac\": \"0E:C0:EF:20:69:49\"\n        }\n    ]\n    }\n}\n}\nEOF\n\ncurl -vv \\\n    -X PATCH \\\n    \"${HOST_PROXY_API_PATH}/ocp1-cp-5/status\" \\\n    -H \"Content-type: application/merge-patch+json\" \\\n    -d \"${host_patch}\"\n</code></pre> oc get bmh,machine -n openshift-machine-api <pre><code>stormshift-ocp1 # oc get bmh,machine -n openshift-machine-api\nNAME                                    STATE       CONSUMER                    ONLINE   ERROR   AGE\nbaremetalhost.metal3.io/ocp1-cp-1       unmanaged   ocp1-g6vbv-master-0         true             23h\nbaremetalhost.metal3.io/ocp1-cp-4       unmanaged   ocp1-cp-4                   true             50m\nbaremetalhost.metal3.io/ocp1-cp-5       unmanaged   ocp1-cp-5                   true             7m56s\nbaremetalhost.metal3.io/ocp1-worker-1   unmanaged   ocp1-g6vbv-worker-0-jmj97   true             23h\nbaremetalhost.metal3.io/ocp1-worker-2   unmanaged   ocp1-g6vbv-worker-0-wnbqv   true             23h\nbaremetalhost.metal3.io/ocp1-worker-3   unmanaged   ocp1-g6vbv-worker-0-zszgr   true             23h\n\nNAME                                                     PHASE     TYPE   REGION   ZONE   AGE\nmachine.machine.openshift.io/ocp1-cp-4                   Running                          50m\nmachine.machine.openshift.io/ocp1-cp-5                   Running                          7m53s\nmachine.machine.openshift.io/ocp1-g6vbv-master-0         Running                          23h\nmachine.machine.openshift.io/ocp1-g6vbv-worker-0-jmj97   Running                          22h\nmachine.machine.openshift.io/ocp1-g6vbv-worker-0-wnbqv   Running                          22h\nmachine.machine.openshift.io/ocp1-g6vbv-worker-0-zszgr   Running                          22h\nstormshift-ocp1 #\n</code></pre>","tags":["etcd","control-plane","v4.17"]},{"location":"control-plane/lost-quorum/#validate-etcd-again","title":"Validate etcd again","text":"Check etcd rollout... <pre><code>stormshift-ocp1 # oc get pods -n openshift-etcd\nNAME                           READY   STATUS              RESTARTS   AGE\netcd-guard-ocp1-cp-1           1/1     Running             0          22h\netcd-guard-ocp1-cp-4           1/1     Running             0          35m\netcd-ocp1-cp-1                 4/4     Running             0          21m\netcd-ocp1-cp-4                 4/4     Running             0          17m\ninstaller-12-ocp1-cp-4         0/1     Completed           0          40m\ninstaller-15-ocp1-cp-1         0/1     Completed           0          24m\ninstaller-15-ocp1-cp-4         0/1     Completed           0          21m\ninstaller-17-ocp1-cp-5         0/1     ContainerCreating   0          55s\nrevision-pruner-11-ocp1-cp-1   0/1     Completed           0          40m\nrevision-pruner-11-ocp1-cp-4   0/1     Completed           0          40m\nrevision-pruner-12-ocp1-cp-1   0/1     Completed           0          39m\nrevision-pruner-12-ocp1-cp-4   0/1     Completed           0          39m\nrevision-pruner-13-ocp1-cp-1   0/1     Completed           0          34m\nrevision-pruner-13-ocp1-cp-4   0/1     Completed           0          34m\nrevision-pruner-14-ocp1-cp-1   0/1     Completed           0          34m\nrevision-pruner-14-ocp1-cp-4   0/1     Completed           0          34m\nrevision-pruner-15-ocp1-cp-1   0/1     Completed           0          24m\nrevision-pruner-15-ocp1-cp-4   0/1     Completed           0          24m\nrevision-pruner-15-ocp1-cp-5   0/1     ContainerCreating   0          87s\nrevision-pruner-16-ocp1-cp-1   0/1     Completed           0          73s\nrevision-pruner-16-ocp1-cp-4   0/1     Completed           0          70s\nrevision-pruner-16-ocp1-cp-5   0/1     ContainerCreating   0          66s\nrevision-pruner-17-ocp1-cp-1   0/1     Completed           0          62s\nrevision-pruner-17-ocp1-cp-4   0/1     Completed           0          59s\nrevision-pruner-17-ocp1-cp-5   0/1     ContainerCreating   0          56s\noc get pods\nNAME                           READY   STATUS      RESTARTS   AGE\netcd-guard-ocp1-cp-1           1/1     Running     0          22h\netcd-guard-ocp1-cp-4           1/1     Running     0          37m\netcd-guard-ocp1-cp-5           1/1     Running     0          72s\netcd-ocp1-cp-1                 4/4     Running     0          23m\netcd-ocp1-cp-4                 4/4     Running     0          20m\netcd-ocp1-cp-5                 4/4     Running     0          76s\ninstaller-12-ocp1-cp-4         0/1     Completed   0          42m\ninstaller-15-ocp1-cp-1         0/1     Completed   0          26m\ninstaller-15-ocp1-cp-4         0/1     Completed   0          23m\ninstaller-17-ocp1-cp-5         0/1     Completed   0          3m5s\ninstaller-19-ocp1-cp-1         1/1     Running     0          14s\nrevision-pruner-11-ocp1-cp-1   0/1     Completed   0          42m\nrevision-pruner-11-ocp1-cp-4   0/1     Completed   0          42m\nrevision-pruner-12-ocp1-cp-1   0/1     Completed   0          42m\nrevision-pruner-12-ocp1-cp-4   0/1     Completed   0          42m\nrevision-pruner-13-ocp1-cp-1   0/1     Completed   0          36m\nrevision-pruner-13-ocp1-cp-4   0/1     Completed   0          36m\nrevision-pruner-14-ocp1-cp-1   0/1     Completed   0          36m\nrevision-pruner-14-ocp1-cp-4   0/1     Completed   0          36m\nrevision-pruner-15-ocp1-cp-1   0/1     Completed   0          26m\nrevision-pruner-15-ocp1-cp-4   0/1     Completed   0          26m\nrevision-pruner-15-ocp1-cp-5   0/1     Completed   0          3m37s\nrevision-pruner-16-ocp1-cp-1   0/1     Completed   0          3m23s\nrevision-pruner-16-ocp1-cp-4   0/1     Completed   0          3m20s\nrevision-pruner-16-ocp1-cp-5   0/1     Completed   0          3m16s\nrevision-pruner-17-ocp1-cp-1   0/1     Completed   0          3m12s\nrevision-pruner-17-ocp1-cp-4   0/1     Completed   0          3m9s\nrevision-pruner-17-ocp1-cp-5   0/1     Completed   0          3m6s\nrevision-pruner-18-ocp1-cp-1   0/1     Completed   0          29s\nrevision-pruner-18-ocp1-cp-4   0/1     Completed   0          26s\nrevision-pruner-18-ocp1-cp-5   0/1     Completed   0          23s\nrevision-pruner-19-ocp1-cp-1   0/1     Completed   0          20s\nrevision-pruner-19-ocp1-cp-4   0/1     Completed   0          16s\nrevision-pruner-19-ocp1-cp-5   0/1     Completed   0          14s\nstormshift-ocp1 # oc get co/etcd\nNAME   VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE\netcd   4.17.0    True        True          True       22h     EtcdCertSignerControllerDegraded: EtcdCertSignerController can't evaluate whether quorum is safe: etcd cluster has quorum of 2 which is not fault tolerant: [{Member:ID:602544793613865230 name:\"ocp1-cp-4\" peerURLs:\"https://10.32.105.72:2380\" clientURLs:\"https://10.32.105.72:2379\"  Healthy:true Took:1.580703ms Error:&lt;nil&gt;} {Member:ID:8765177104826810005 name:\"ocp1-cp-1\" peerURLs:\"https://10.32.105.69:2380\" clientURLs:\"https://10.32.105.69:2379\"  Healthy:true Took:1.605857ms Error:&lt;nil&gt;}]\n</code></pre> <p>=&gt; Wait until all etcd member are at revision 19</p> etcdctl ... <pre><code>sh-5.1# etcdctl endpoint health  -w table\n+---------------------------+--------+-------------+-------+\n|         ENDPOINT          | HEALTH |    TOOK     | ERROR |\n+---------------------------+--------+-------------+-------+\n| https://10.32.105.72:2379 |   true | 13.517796ms |       |\n| https://10.32.105.69:2379 |   true | 17.334331ms |       |\n| https://10.32.105.73:2379 |   true | 17.188206ms |       |\n+---------------------------+--------+-------------+-------+\nsh-5.1# etcdctl endpoint health --cluster -w table\n\\+---------------------------+--------+-------------+-------+\n|         ENDPOINT          | HEALTH |    TOOK     | ERROR |\n+---------------------------+--------+-------------+-------+\n| https://10.32.105.73:2379 |   true | 13.722722ms |       |\n| https://10.32.105.69:2379 |   true |  15.68852ms |       |\n| https://10.32.105.72:2379 |   true | 18.331362ms |       |\n+---------------------------+--------+-------------+-------+\nsh-5.1#  etcdctl endpoint status --cluster -w table\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n| https://10.32.105.72:2379 |  85cab4c209d910e |  3.5.14 |  153 MB |     false |      false |        10 |      59190 |              59190 |        |\n| https://10.32.105.69:2379 | 79a42a230e96be95 |  3.5.14 |  151 MB |      true |      false |        10 |      59190 |              59190 |        |\n| https://10.32.105.73:2379 | 86fab56c0f939612 |  3.5.14 |  150 MB |     false |      false |        10 |      59190 |              59190 |        |\n+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\nsh-5.1# etcdctl member list  -w table\n+------------------+---------+-----------+---------------------------+---------------------------+------------+\n|        ID        | STATUS  |   NAME    |        PEER ADDRS         |       CLIENT ADDRS        | IS LEARNER |\n+------------------+---------+-----------+---------------------------+---------------------------+------------+\n|  85cab4c209d910e | started | ocp1-cp-4 | https://10.32.105.72:2380 | https://10.32.105.72:2379 |      false |\n| 79a42a230e96be95 | started | ocp1-cp-1 | https://10.32.105.69:2380 | https://10.32.105.69:2379 |      false |\n| 86fab56c0f939612 | started | ocp1-cp-5 | https://10.32.105.73:2380 | https://10.32.105.73:2379 |      false |\n+------------------+---------+-----------+---------------------------+---------------------------+------------+\nsh-5.1#\n</code></pre>","tags":["etcd","control-plane","v4.17"]},{"location":"control-plane/lost-quorum/#norunningovncontrolplane","title":"NoRunningOvnControlPlane","text":"<p>Still persist.</p> Details about crashed Pods <pre><code>stormshift-ocp1 # oc -n openshift-ovn-kubernetes delete pod -l app=ovnkube-control-plane --wait=false\npod \"ovnkube-control-plane-78c675bd69-vlx6k\" deleted\npod \"ovnkube-control-plane-78c675bd69-wt8dc\" deleted\nstormshift-ocp1 #\nstormshift-ocp1 # oc -n openshift-ovn-kubernetes get pod -l app=ovnkube-control-plane\n\nNAME                                     READY   STATUS             RESTARTS      AGE\novnkube-control-plane-78c675bd69-cvcfm   2/2     Running            1 (37s ago)   39s\novnkube-control-plane-78c675bd69-thp9d   1/2     CrashLoopBackOff   2 (20s ago)   39s\n\nstormshift-ocp1 # oc -n openshift-ovn-kubernetes get pod -l app=ovnkube-control-plane -o wide\nNAME                                     READY   STATUS             RESTARTS        AGE   IP             NODE        NOMINATED NODE   READINESS GATES\novnkube-control-plane-78c675bd69-cvcfm   1/2     NotReady           10 (115s ago)   19m   10.32.105.73   ocp1-cp-5   &lt;none&gt;           &lt;none&gt;\novnkube-control-plane-78c675bd69-thp9d   1/2     CrashLoopBackOff   12 (48s ago)    19m   10.32.105.72   ocp1-cp-4   &lt;none&gt;           &lt;none&gt;\nstormshift-ocp1 #\n</code></pre> <p>Try to restart kubelet on ocp1-cp-5 and ocp1-cp-4, doesn't help. Let's reboot the nodes.</p> <p>Restart of ocp1-cp-5 done, still CrashLoopBackOff.</p> <p>Solution: ovnkube-control-plane crashes on restart after adding an OVN-Kubernetes NAD for localnet topology</p> Fix ovnkube-control-plane crashes <pre><code>stormshift-ocp1 # oc get net-attach-def -A\nNAMESPACE       NAME   AGE\nlocalnet-demo   coe    23h\nstormshift-ocp1 # oc get net-attach-def -n localnet-demo coe -o yaml | yq 'del(.metadata.creationTimestamp)| del(.metadata.generation)|del(.metadata.resourceVersion)|del(.metadata.uid)' &gt; localnet-demo.yaml\nstormshift-ocp1 # oc delete  net-attach-def -n localnet-demo coe\nnetworkattachmentdefinition.k8s.cni.cncf.io \"coe\" deleted\nstormshift-ocp1 # oc -n openshift-ovn-kubernetes get pod -l app=ovnkube-control-plane -o wide\nNAME                                     READY   STATUS             RESTARTS          AGE   IP             NODE        NOMINATED NODE   READINESS GATES\novnkube-control-plane-78c675bd69-7v7qm   1/2     CrashLoopBackOff   210 (3m5s ago)    17h   10.32.105.69   ocp1-cp-1   &lt;none&gt;           &lt;none&gt;\novnkube-control-plane-78c675bd69-bzs6c   1/2     CrashLoopBackOff   210 (2m10s ago)   17h   10.32.105.73   ocp1-cp-5   &lt;none&gt;           &lt;none&gt;\nstormshift-ocp1 # oc -n openshift-ovn-kubernetes delete pod -l app=ovnkube-control-plane --wait=false\npod \"ovnkube-control-plane-78c675bd69-7v7qm\" deleted\npod \"ovnkube-control-plane-78c675bd69-bzs6c\" deleted\nstormshift-ocp1 # oc -n openshift-ovn-kubernetes get pod -l app=ovnkube-control-plane -o wide\nNAME                                     READY   STATUS    RESTARTS   AGE   IP             NODE        NOMINATED NODE   READINESS GATES\novnkube-control-plane-78c675bd69-c5j7l   2/2     Running   0          14s   10.32.105.73   ocp1-cp-5   &lt;none&gt;           &lt;none&gt;\novnkube-control-plane-78c675bd69-qlwzn   2/2     Running   0          14s   10.32.105.72   ocp1-cp-4   &lt;none&gt;           &lt;none&gt;\nstormshift-ocp1 # oc create -f localnet-demo.yaml\nnetworkattachmentdefinition.k8s.cni.cncf.io/coe created\nstormshift-ocp1 #\n</code></pre> <p>Workload / VM was not available during the timeframe of delete net-attach-def!</p>","tags":["etcd","control-plane","v4.17"]},{"location":"deploy/","title":"Deployments","text":""},{"location":"deploy/#deployment-from-private-registry","title":"Deployment from private registry","text":"<p>Documentation: Using image pull secrets</p>"},{"location":"deploy/#create-pull-secret","title":"Create pull-secret","text":"<pre><code>oc create secret generic &lt;pull_secret_name&gt; \\\n    --from-file=.dockerconfigjson=&lt;path/to/.docker/config.json&gt; \\\n    --type=kubernetes.io/dockerconfigjson\n</code></pre> <pre><code>oc create secret docker-registry &lt;pull_secret_name&gt; \\\n    --docker-server=&lt;registry_server&gt; \\\n    --docker-username=&lt;user_name&gt; \\\n    --docker-password=&lt;password&gt; \\\n    --docker-email=&lt;email&gt;\n</code></pre>"},{"location":"deploy/#option-1-link-service-account-to-pull-secret","title":"Option 1) Link service account to pull secret","text":"<pre><code>oc secrets link default &lt;pull_secret_name&gt; --for=pull\n</code></pre>"},{"location":"deploy/#option-2-pod-spec","title":"Option 2) Pod Spec","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: private-reg\nspec:\n  containers:\n  - name: private-reg-container\n    image: &lt;your-private-image&gt;\n  imagePullSecrets:\n  - name: generic\n</code></pre>"},{"location":"deploy/#run-red-hat-enterprise-linux-support-tools","title":"Run Red Hat Enterprise Linux Support Tools","text":"Pod run-rhel-support-tools-pod.yamloc apply -f .... <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  generateName: rhel-support-tools-\nspec:\n  containers:\n    - name: tools\n      image: registry.redhat.io/rhel9/support-tools:latest\n      command:\n        - \"/bin/sh\"\n        - \"-c\"\n        - \"sleep infinity\"\n  restartPolicy: Never\n</code></pre> <pre><code>oc apply -f https://examples.openshift.pub/pr-133/deploy/run-rhel-support-tools-pod.yaml\n</code></pre> <p>source: run-rhel-support-tools-pod.yaml</p> Deployment run-rhel-support-tools-deployment.yamloc apply -f .... <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rhel-support-tools\n  labels:\n    app: rhel-support-tools\n    app.openshift.io/runtime: redhat\n    app.kubernetes.io/part-of: rhel-support-tools\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: rhel-support-tools\n  template:\n    metadata:\n      labels:\n        app: rhel-support-tools\n    spec:\n      automountServiceAccountToken: false\n      containers:\n        - name: tools\n          image: registry.redhat.io/rhel9/support-tools:latest\n          command:\n            - \"/bin/sh\"\n            - \"-c\"\n            - \"sleep infinity\"\n</code></pre> <pre><code>oc apply -f https://examples.openshift.pub/pr-133/deploy/run-rhel-support-tools-deployment.yaml\n</code></pre> <p>source: run-rhel-support-tools-deployment.yaml</p>"},{"location":"deploy/#run-a-ubi-micro","title":"Run a ubi-micro","text":"Pod run-ubi-micro-pod.yamloc apply -f .... <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  generateName: ubi-micro-\nspec:\n  containers:\n    - name: ubi-micro\n      image: registry.access.redhat.com/ubi9/ubi-micro:latest\n      command:\n        - \"/bin/sh\"\n        - \"-c\"\n        - \"sleep infinity\"\n  restartPolicy: Never\n</code></pre> <pre><code>oc apply -f https://examples.openshift.pub/pr-133/deploy/run-ubi-micro-pod.yaml\n</code></pre> <p>source: run-ubi-micro-pod.yaml</p> Deployment run-ubi-micro-deployment.yamloc apply -f .... <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ubi-micro\n  labels:\n    app: ubi-micro\n    app.openshift.io/runtime: redhat\n    app.kubernetes.io/part-of: ubi-micro\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: ubi-micro\n  template:\n    metadata:\n      labels:\n        app: ubi-micro\n    spec:\n      automountServiceAccountToken: false\n      containers:\n        - name: tools\n          image: registry.access.redhat.com/ubi9/ubi-micro:latest\n          command:\n            - \"/bin/sh\"\n            - \"-c\"\n            - \"sleep infinity\"\n</code></pre> <pre><code>oc apply -f https://examples.openshift.pub/pr-133/deploy/run-ubi-micro-deployment.yaml\n</code></pre> <p>source: run-ubi-micro-deployment.yaml</p>"},{"location":"deploy/#ubi9-deployment-with-pvc","title":"UBI9 deployment with pvc","text":"<pre><code>kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ubi9\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ubi9\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: ubi9\n    spec:\n      volumes:\n        - name: pvc\n          persistentVolumeClaim:\n            claimName: pvc\n      containers:\n        - name: ubi\n          image: 'registry.access.redhat.com/ubi9/ubi-micro:latest'\n          volumeMounts:\n            - name: pvc\n              mountPath: /pvc\n          command:\n            - /bin/sh\n            - '-c'\n            - |\n              sleep infinity\n</code></pre> <p>source: ubi-deployment-w-pvc.yaml</p>"},{"location":"deploy/#s2i-playground","title":"S2I playground","text":"<pre><code>---\napiVersion: image.openshift.io/v1\nkind: ImageStream\nmetadata:\n  name: builder-test\n---\napiVersion: build.openshift.io/v1\nkind: BuildConfig\nmetadata:\n  labels:\n    build: builder-test\n  name: builder-test\nspec:\n  failedBuildsHistoryLimit: 5\n  nodeSelector: null\n  output:\n    to:\n      kind: ImageStreamTag\n      name: builder-test:latest\n  postCommit: {}\n  resources: {}\n  runPolicy: Serial\n  source:\n    dockerfile: \"FROM rhscl/s2i-base-rhel7:latest \\nENTRYPOINT bash\\n\"\n    type: Dockerfile\n  strategy:\n    dockerStrategy:\n      from:\n        kind: DockerImage\n        name: registry.redhat.io/rhscl/s2i-base-rhel7:latest\n    type: Docker\n  successfulBuildsHistoryLimit: 5\n---\napiVersion: apps.openshift.io/v1\nkind: DeploymentConfig\nmetadata:\n  labels:\n    app: builder-test\n  name: builder-test\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    app: builder-test\n    deploymentconfig: builder-test\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: builder-test\n        deploymentconfig: builder-test\n    spec:\n      containers:\n      - image: builder-test:latest\n        imagePullPolicy: Always\n        name: builder-test\n        command:\n        - /bin/sh\n        - -c\n        - while true ; do date; sleep 1; done;\n        resources: {}\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n      dnsPolicy: ClusterFirst\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n  test: false\n  triggers:\n  - type: ConfigChange\n  - imageChangeParams:\n      automatic: true\n      containerNames:\n      - builder-test\n      from:\n        kind: ImageStreamTag\n        name: builder-test:latest\n        namespace: anyuid\n    type: ImageChange\n</code></pre>"},{"location":"deploy/#example","title":"Example","text":"<p>List of allocatable memory:</p> <pre><code>$ oc get nodes -o custom-columns=NAME:.metadata.name,MEM-allocatable:.status.allocatable.memory  -l node-role.kubernetes.io/worker\nNAME                                 MEM-allocatable\nworker-1.rbohne.e2e.bos.redhat.com   15270340Ki\nworker-2.rbohne.e2e.bos.redhat.com   15270356Ki\nworker-3.rbohne.e2e.bos.redhat.com   15270356Ki\n</code></pre> <p>Note</p> <p>This is allocatable memory on the whole host for Pods. The amount of allocatable memory do NOT include allocated memory of running Pods!</p> <p>Request &amp; limit:</p> <pre><code>resources:\n  limits:\n    memory: 32Gi\n  requests:\n    memory: 32Gi\n</code></pre> <p>Result: <code>0/6 nodes are available: 6 Insufficient memory.</code></p> <p>Request &amp; limit:</p> <pre><code>resources:\n  limits:\n    memory: 10Gi\n  requests:\n    memory: 10Gi\n</code></pre> <p>Result:</p> <p>Scale up to 3 Pods: <code>oc scale --replicas=3 dc/ubi8</code></p> <pre><code>$ oc get pods -o wide -l deploymentconfig=ubi8\nNAME           READY   STATUS    RESTARTS   AGE   IP            NODE                                 NOMINATED NODE   READINESS GATES\nubi8-7-56bqv   1/1     Running   0          19m   10.131.0.18   worker-3.rbohne.e2e.bos.redhat.com   &lt;none&gt;           &lt;none&gt;\nubi8-7-5wlhm   1/1     Running   0          19m   10.128.2.65   worker-2.rbohne.e2e.bos.redhat.com   &lt;none&gt;           &lt;none&gt;\nubi8-7-gdtf2   1/1     Running   0          19m   10.129.2.28   worker-1.rbohne.e2e.bos.redhat.com   &lt;none&gt;           &lt;none&gt;\n</code></pre>"},{"location":"deploy/eap-cluster/","title":"EAP Cluster demo","text":""},{"location":"deploy/eap-cluster/#grant-view-permission-to-default-service-account","title":"Grant view permission to default service account","text":"<p>This ensures that the EAP can query which PODs are still available:</p> <pre><code>oc policy add-role-to-user view -z default\n</code></pre>"},{"location":"deploy/eap-cluster/#deploy-jboss-eap-with-demo-application","title":"Deploy JBoss EAP with demo application","text":"<pre><code>oc process -n openshift eap70-basic-s2i \\\n   -v SOURCE_REPOSITORY_URL=https://github.com/openshift-examples/SimpleWebApp.git \\\n      SOURCE_REPOSITORY_REF=master \\\n  | oc create -f -\n</code></pre>"},{"location":"deploy/eap-cluster/#scale-up-deployment-to-more-than-1-pod","title":"Scale up deployment to more than 1 pod","text":"<pre><code>oc scale --replicas=2 dc/eap-app\n</code></pre>"},{"location":"deploy/eap-cluster/#enforce-round-robin","title":"Enforce round robin","text":"<p>This should be default, but we will enforce just in case roundrobin to check the session replication. For more information: routes.html#route-specific-annotations</p> <pre><code>oc patch \\\n  -p '{\"metadata\":{\"annotations\":{\"haproxy.router.openshift.io/balance\": \"roundrobin\"}}}' \\\n  route/eap-app\n</code></pre>"},{"location":"deploy/eap-cluster/#check-session-replication","title":"Check session replication","text":"<p>2017-03-14: since there is a haproxy stickyness bug in OCP 3.4 (disable_cookies is ignored), we have to delete the HttpOnly line in our temporary cookie cache.</p> <pre><code>$  while true; do \\\n     curl -s -b /tmp/mycookies.jar -c /tmp/mycookies.jar \\\n     http://&lt;your-application-route&gt;/SimpleWebApp/SessionInfoServlet | \\\n     grep -E '(Hostname|session)'; \\\n     sleep 1; \\\n     sed -i '/HttpOnly/d' /tmp/mycookies.jar; \\\n     echo \"\"; \\\n     done\n</code></pre>"},{"location":"deploy/eap-cluster/#example","title":"Example","text":"<p>Expected behaviour: session ID and creation time remains the same, hostname changes.</p> <pre><code>$  while true; do \\\n&gt;    curl -s -b /tmp/mycookies.jar -c /tmp/mycookies.jar \\\n&gt;    http://eap-app-zisis.paas.osp.consol.de/SimpleWebApp/SessionInfoServlet | \\\n&gt;    grep -E '(Hostname|session)'; \\\n&gt;    sleep 1; \\\n&gt;    sed -i '/HttpOnly/d' /tmp/mycookies.jar; \\\n&gt;    echo \"\"; \\\n&gt;    done\nHostname: eap-app-1-n52jz\nsession id:             NtXAm9gUvpHvYPmAmprjH0JBAUA3wGgfEHk4Tzvw\nsession createTime:     Tue Mar 14 13:52:17 UTC 2017\nsession lastAccessTime: Tue Mar 14 13:54:18 UTC 2017\n\nHostname: eap-app-1-m1qz6\nsession id:             NtXAm9gUvpHvYPmAmprjH0JBAUA3wGgfEHk4Tzvw\nsession createTime:     Tue Mar 14 13:52:17 UTC 2017\nsession lastAccessTime: Tue Mar 14 13:55:29 UTC 2017\n\nHostname: eap-app-1-n52jz\nsession id:             NtXAm9gUvpHvYPmAmprjH0JBAUA3wGgfEHk4Tzvw\nsession createTime:     Tue Mar 14 13:52:17 UTC 2017\nsession lastAccessTime: Tue Mar 14 13:55:30 UTC 2017\n\nHostname: eap-app-1-m1qz6\nsession id:             NtXAm9gUvpHvYPmAmprjH0JBAUA3wGgfEHk4Tzvw\nsession createTime:     Tue Mar 14 13:52:17 UTC 2017\nsession lastAccessTime: Tue Mar 14 13:55:31 UTC 2017\n\n...\n</code></pre> <p></p>"},{"location":"deploy/faketime/","title":"Faketime for your application","text":"<p>How to inject https://github.com/wolfcw/libfaketime into your application deployment or you use the Time Machine Operator</p> libfaketime.yamlOC <pre><code>kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: libfaketime\n  labels:\n    app: libfaketime\n    app.kubernetes.io/component: libfaketime\n    app.kubernetes.io/instance: libfaketime\n    app.kubernetes.io/name: libfaketime\n    app.kubernetes.io/part-of: libfaketime-app\n    app.openshift.io/runtime: other-linux\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: libfaketime\n  template:\n    metadata:\n      labels:\n        app: libfaketime\n    spec:\n      volumes:\n      - name: libfaketime\n        emptyDir: {}\n      initContainers:\n        - name: build-libfaketime\n          image: registry.redhat.io/rhel8/gcc-toolset-11-toolchain:11-6\n          env:\n            - name: VERSION\n              value: 0.9.9\n          volumeMounts:\n            - mountPath: /libfaketime/\n              name: libfaketime\n          command:\n            - /bin/sh\n            - -c\n            - |\n              set -x\n              cd /tmp/\n              curl -# -L -O https://github.com/wolfcw/libfaketime/archive/refs/tags/v${VERSION}.tar.gz\n              tar xzvf v${VERSION}.tar.gz\n              cd libfaketime-${VERSION}/\n              PREFIX=/libfaketime/ make install\n      containers:\n        - name: date\n          image: registry.access.redhat.com/ubi8/ubi-micro\n          env:\n            - name: LD_PRELOAD\n              value: /libfaketime/lib/faketime/libfaketime.so.1\n            - name: FAKETIME\n              value: \"-365d\"\n          command:\n            - /bin/sh\n            - '-c'\n            - |\n              while true; do\n                date\n                sleep 1\n              done;\n          volumeMounts:\n            - mountPath: /libfaketime/\n              name: libfaketime\n          imagePullPolicy: Always\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      dnsPolicy: ClusterFirst\n      securityContext: {}\n      schedulerName: default-scheduler\n  strategy:\n    type: Recreate\n  revisionHistoryLimit: 10\n  progressDeadlineSeconds: 600\n</code></pre> <pre><code>oc apply -f https://examples.openshift.pub/pr-133/deploy/faketime/content/deploy/libfaketime.yaml\n</code></pre>","tags":["time","tagB"]},{"location":"deploy/imagestreams/","title":"ImageStreams","text":""},{"location":"deploy/imagestreams/#resources","title":"Resources","text":"<ul> <li>Variations on imagestreams in OpenShift 4</li> <li>Using Red Hat OpenShift image streams with Kubernetes deployments</li> </ul>"},{"location":"deploy/initcontainers/","title":"Init containers","text":"<p>Example for</p> <ul> <li>DNS Check</li> <li>TCP Check</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app: myapp\nspec:\n  containers:\n  - name: myapp-container\n    image: busybox\n    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']\n  initContainers:\n  - name: check-tcp\n    image: rhel7/rhel-tools\n    command: ['sh', '-c', 'while true ; do echo \"Try to connect \" ; nc -z myserver 8080 &amp;&amp; break; sleep 5; done; echo \"Eventserver is running...\"']\n  - name: check-nslookup\n    image: busybox\n    command: ['sh', '-c', 'while true; do nslookup mydb || break ; sleep 2 ; echo \"waiting for mydb\"; done;']\n</code></pre>"},{"location":"deploy/jar/","title":"Java/JAR Deployment","text":"","tags":["java","jar","deploy"]},{"location":"deploy/jar/#multi-stage-build","title":"Multi-stage build","text":"","tags":["java","jar","deploy"]},{"location":"deploy/jar/#build-of-container-image","title":"Build of container image","text":"<p>Source &amp; Containerfile is available at openshift-quickstarts/undertow-servlet</p> Containerfile <pre><code>FROM registry.access.redhat.com/ubi8/openjdk-11:latest as builder\nUSER root\nCOPY ./ /tmp/src\nRUN chown -R 185:0 /tmp/src\nUSER 185\nRUN /usr/local/s2i/assemble\n\nFROM registry.access.redhat.com/ubi8/openjdk-11-runtime\nCOPY --from=builder /deployments /deployments\n\nCMD [\"java\",\"-jar\",\"/deployments/undertow-servlet.jar\"]\n</code></pre>","tags":["java","jar","deploy"]},{"location":"deploy/jar/#deployment","title":"Deployment","text":"deployment-multi-stage.yamlOC <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jar-multi-stage-example\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: jar-multi-stage\n  template:\n    metadata:\n      labels:\n        app: jar-multi-stage\n    spec:\n      containers:\n      - name: app\n        image: quay.io/openshift-examples/jar-deploy-example:multi-stage\n  triggers:\n  - type: ConfigChange\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: jar-multi-stage\n  name: jar-multi-stage\nspec:\n  ports:\n  - name: 8080-8080\n    port: 8080\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: jar-multi-stage\n  sessionAffinity: None\n  type: ClusterIP\n---\napiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  labels:\n    app: jar-multi-stage\n  name: jar-multi-stage\nspec:\n  port:\n    targetPort: 8080\n  to:\n    kind: \"Service\"\n    name: jar-multi-stage\n    weight: null\n</code></pre> <pre><code>oc apply -f https://examples.openshift.pub/pr-133/deploy/jar/deployment-multi-stage.yaml\n</code></pre>","tags":["java","jar","deploy"]},{"location":"deploy/jar/#unconventional-copy-via-initcontainer-into-runtime","title":"Unconventional: copy via initContainer into runtime","text":"","tags":["java","jar","deploy"]},{"location":"deploy/jar/#build-of-initcontainer-image","title":"Build of initContainer image","text":"<p>Source &amp; Containerfile is available at openshift-quickstarts/undertow-servlet</p> Containerfile <pre><code>FROM registry.access.redhat.com/ubi8/openjdk-11:latest as builder\nUSER root\nCOPY ./ /tmp/src\nRUN chown -R 185:0 /tmp/src\nUSER 185\nRUN /usr/local/s2i/assemble\n\nFROM registry.access.redhat.com/ubi8/ubi-micro\nCOPY --from=builder /deployments /deployments\nCMD echo \"Please don't start...\"\n</code></pre>","tags":["java","jar","deploy"]},{"location":"deploy/jar/#deployment_1","title":"Deployment","text":"deployment-init-container.yamlOC <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jar-init-container-example\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: jar-init-container\n  template:\n    metadata:\n      labels:\n        app: jar-init-container\n    spec:\n      volumes:\n       - name: app\n         emptyDir: {}\n      initContainers:\n      - name: copy-jar\n        image: quay.io/openshift-examples/jar-deploy-example:initContainer\n        volumeMounts:\n          - name: app\n            mountPath: /app\n        command:\n          - sh\n          - -c\n          - |\n            cp -v /deployments/undertow-servlet.jar /app/undertow-servlet.jar\n\n      containers:\n      - name: app\n        image: registry.access.redhat.com/ubi8/openjdk-11-runtime\n        volumeMounts:\n          - name: app\n            mountPath: /app\n        command:\n          - java\n          - -jar\n          - /app/undertow-servlet.jar\n  triggers:\n  - type: ConfigChange\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: jar-init-container\n  name: jar-init-container\nspec:\n  ports:\n  - name: 8080-8080\n    port: 8080\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: jar-init-container\n  sessionAffinity: None\n  type: ClusterIP\n---\napiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  labels:\n    app: jar-init-container\n  name: jar-init-container\nspec:\n  port:\n    targetPort: 8080\n  to:\n    kind: \"Service\"\n    name: jar-init-container\n    weight: null\n</code></pre> <pre><code>oc apply -f https://examples.openshift.pub/pr-133/deploy/jar/deployment-init-container.yaml\n</code></pre>","tags":["java","jar","deploy"]},{"location":"deploy/liveness-probe/","title":"Liveness probe - Work-in-progress","text":"<pre><code>apiVersion: v1\nkind: DeploymentConfig\nmetadata:\n  name: liveness-probe\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      name: liveness-probe\n      labels:\n        app: liveness-probe\n    spec:\n      # only supports always\n      # restartPolicy: Always\n      initContainers:\n      - name: initcontainer\n        image:  quay.io/openshift-examples/gitlab-runner:latest\n        command: [ \"/bin/sh\", \"-c\", \"sleep 1; echo init done\" ]\n      containers:\n      - name: container-runner\n        image:  quay.io/openshift-examples/gitlab-runner:latest\n        command:\n        - /bin/sh\n        - -c\n        - |\n          counter=0; while true ; do echo \"$counter : $(date)\";sleep 1 ; counter=$((counter+1)) ; done;\n      - name: container-crashes\n        image:  quay.io/openshift-examples/gitlab-runner:latest\n        command:\n        - /bin/sh\n        - -c\n        - |\n          counter=0; while true ; do echo \"$counter : $(date)\";sleep 1 ; counter=$((counter+1)) ; done;\n        startupProbe:\n          exec:\n            command:\n            - /usr/bin/bash\n            - -x\n            - -c\n            - |\n              false\n        livenessProbe:\n          exec:\n            command:\n            - /usr/bin/bash\n            - -x\n            - -c\n            - |\n              false\n          initialDelaySeconds: 10\n          periodSeconds: 5\n  triggers:\n  - type: ConfigChange\n</code></pre>"},{"location":"deploy/nginx-reverse-proxy/","title":"Nginx reverse proxy","text":"<p>Based on</p> <p>Run Nginx as reverse proxy on Openshift</p>"},{"location":"deploy/nginx-reverse-proxy/#build","title":"Build","text":"<pre><code>oc new-build openshift/nginx~https://github.com/openshift-examples/nginx-reverse-proxy.git \\\n  --name=nginxbase \\\n  --context-dir=nginx-reverse-proxy \\\n  --strategy=source\n</code></pre>"},{"location":"deploy/nginx-reverse-proxy/#build-deploy","title":"Build &amp; Deploy","text":"<pre><code>oc new-app https://github.com/openshift-examples/nginx-reverse-proxy.git \\\n  --context-dir=nginx-reverse-proxy \\\n  --strategy=docker \\\n  --name=reverse-proxy\noc expose svc/reverse-proxy\n</code></pre>"},{"location":"deploy/ops-container/","title":"Ops Container","text":""},{"location":"deploy/ops-container/#daemonset","title":"DaemonSet","text":"<pre><code>#\n# oc adm new-project --node-selector=\"\" ops-container\n# oc project ops-container\n# oc adm policy add-scc-to-user privileged -z default\n#\n# How to do on K8s: https://itnext.io/get-a-shell-to-a-kubernetes-node-9b720a15a4fe\n# Or use \"oc debug node/...\"\noc apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ops-container\nspec:\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      name: ops-container\n  template:\n    metadata:\n      labels:\n        name: ops-container\n    spec:\n      tolerations:\n        - operator: Exists\n      hostPID: true\n      hostIPC: true\n      hostNetwork: true\n      hostname: ops\n      # nodeSelector:\n      #   kubernetes.io/hostname: node3.novalocal\n      # nodeName: node3.novalocal\n      volumes:\n        - name: host\n          hostPath:\n            path: /\n        - name: run\n          hostPath:\n            path: /run\n        - name: log\n          hostPath:\n            path: /var/log\n        - name: localtime\n          hostPath:\n            path: /etc/localtime\n      containers:\n        - name: rhel\n          image: registry.access.redhat.com/rhel7/support-tools\n          command: [ \"/bin/sh\", \"-c\", \"sleep infinity\" ]\n          securityContext:\n            privileged: true\n            runAsUser: 0\n          env:\n            - name: HOST\n              value: \"/host\"\n            - name: NAME\n              value: \"ops\"\n            - name: IMAGE\n              value: \"registry.access.redhat.com/rhel7/support-tools\"\n          volumeMounts:\n            - name: host\n              mountPath: /host\n            - name: run\n              mountPath: /run\n            - name: log\n              mountPath: /var/log\n            - name: localtime\n              mountPath: /etc/localtime\nEOF\n</code></pre>"},{"location":"deploy/ops-container/#rhel-pod","title":"RHEL Pod","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: rhel\nspec:\n  hostPID: true\n  hostIPC: true\n  hostNetwork: true\n  hostname: toor\n  # nodeSelector:\n  #   kubernetes.io/hostname: node3.novalocal\n  # nodeName: node3.novalocal\n  volumes:\n    - name: host\n      hostPath:\n        path: /\n    - name: run\n      hostPath:\n        path: /run\n    - name: log\n      hostPath:\n        path: /var/log\n    - name: localtime\n      hostPath:\n        path: /etc/localtime\n  containers:\n    - name: rhel\n      image: rhel7/rhel-tools\n      command: [ \"/bin/sh\", \"-c\", \"while true ; do date; sleep 1; done;\" ]\n      securityContext:\n        privileged: true\n      env:\n        - name: HOST\n          value: \"/host\"\n        - name: NAME\n          value: \"toor\"\n        - name: IMAGE\n          value: \"rhel/rhel-tools\"\n      volumeMounts:\n        - name: host\n          mountPath: /host\n        - name: run\n          mountPath: /run\n        - name: log\n          mountPath: /var/log\n        - name: localtime\n          mountPath: /etc/localtime\n  restartPolicy: Never\n</code></pre>"},{"location":"deploy/quota/","title":"Quota - WiP","text":"<pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: storage\nspec:\n  hard:\n    hostpath-provisioner.storageclass.storage.k8s.io/persistentvolumeclaims: 0\n    managed-nfs-storage.storageclass.storage.k8s.io/persistentvolumeclaims: 1\nEOF\n\n$ oc describe quota/storage\nName:                                                                    storage\nNamespace:                                                               cnv-demo\nResource                                                                 Used  Hard\n--------                                                                 ----  ----\nhostpath-provisioner.storageclass.storage.k8s.io/persistentvolumeclaims  0     0\nmanaged-nfs-storage.storageclass.storage.k8s.io/persistentvolumeclaims   0     1\n$\n\noc apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-hostpath-provisioner\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: hostpath-provisioner\nEOF\n\nError from server (Forbidden): error when creating \"STDIN\": persistentvolumeclaims \"pvc-hostpath-provisioner\" is forbidden: exceeded quota: storage, requested: hostpath-provisioner.storageclass.storage.k8s.io/persistentvolumeclaims=1, used: hostpath-provisioner.storageclass.storage.k8s.io/persistentvolumeclaims=0, limited: hostpath-provisioner.storageclass.storage.k8s.io/persistentvolumeclaims=0\n\noc apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-managed-nfs-storage-1\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: managed-nfs-storage\nEOF\npersistentvolumeclaim/pvc-managed-nfs-storage-1 created\n\noc apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-managed-nfs-storage-2\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: managed-nfs-storage\nEOF\nError from server (Forbidden): error when creating \"STDIN\": persistentvolumeclaims \"pvc-managed-nfs-storage-2\" is forbidden: exceeded quota: storage, requested: managed-nfs-storage.storageclass.storage.k8s.io/persistentvolumeclaims=1, used: managed-nfs-storage.storageclass.storage.k8s.io/persistentvolumeclaims=1, limited: managed-nfs-storage.storageclass.storage.k8s.io/persistentvolumeclaims=1\n\n$ oc describe quota/storage\nName:                                                                    storage\nNamespace:                                                               cnv-demo\nResource                                                                 Used  Hard\n--------                                                                 ----  ----\nhostpath-provisioner.storageclass.storage.k8s.io/persistentvolumeclaims  0     0\nmanaged-nfs-storage.storageclass.storage.k8s.io/persistentvolumeclaims   1     1\n</code></pre>","tags":["work-in-progress"]},{"location":"deploy/scale-down-deploymentconfigs/","title":"Auto scale down of deployment configs","text":"<p>Quite old, not tested yet and old fassion way. Next time write an operator! Maybe you can use https://github.com/zalando-incubator/kopf/blob/master/README.md</p>"},{"location":"deploy/scale-down-deploymentconfigs/#build-container","title":"Build container","text":"<p>scale_down.py: <pre><code>#!/usr/bin/env python\n\nimport openshift\n# https://github.com/openshift/openshift-restclient-python/blob/master/openshift/docs/OapiApi.md\nfrom openshift import client, config\nfrom pprint import pprint\nfrom kubernetes.client.rest import ApiException\nfrom datetime import datetime, timedelta\nfrom dateutil.parser import parse\nimport dateutil.tz\n\n\n#config.load_kube_config()\nconfig.load_incluster_config()\n\noapi = client.OapiApi()\n\n# for i in dir(oapi):\n    # print i\n\nproject_list = oapi.list_project()\nfor project in project_list.items:\n    requester=\"unknown\"\n    if 'openshift.io/requester' in project.metadata.annotations.keys():\n        requester=project.metadata.annotations['openshift.io/requester']\n    if 'bohne.io/auto-scale-down' in project.metadata.annotations.keys():\n        d = datetime.now(dateutil.tz.tzutc()) - timedelta(days=int(project.metadata.annotations['bohne.io/auto-scale-down']))\n        #print('Name: {}, Create: {}'.format(project.metadata.name, project.metadata.creation_timestamp))\n\n        if project.metadata.creation_timestamp &lt; d :\n            try:\n                api_response = oapi.list_namespaced_deployment_config(project.metadata.name)\n                for dc in api_response.items:\n                    if int(dc.status.replicas) &gt; 0 :\n                        print(\"Scale down: dc/{} from {} to 0 (namespace: {}, requester: {} )\".format(dc.metadata.name,dc.status.replicas,project.metadata.name,requester))\n                        try:\n                            oapi.patch_namespaced_deployment_config(dc.metadata.name, project.metadata.name,openshift.client.V1DeploymentConfig(spec=openshift.client.V1DeploymentConfigSpec(replicas=0)))\n                        except ApiException as e:\n                            print(\"Exception when calling OapiApi-&gt;patch_namespaced_deployment_config_scale: %s\\n\" % e)\n            except ApiException as e:\n                print(\"Exception when calling OapiApi-&gt;list_deployment_config_for_all_namespaces: %s\\n\" % e)\n</code></pre></p> <p>Containerfile: <pre><code>FROM python:2\n\nRUN pip install openshift\nADD scale_down.py /scale_down.py\n\nENV K8S_AUTH_KEY_FILE=/var/run/secrets/kubernetes.io/serviceaccount\nENTRYPOINT [\"/scale_down.py\"]\nCMD []\n</code></pre></p>"},{"location":"deploy/scale-down-deploymentconfigs/#setup-scheduledjob","title":"Setup ScheduledJob","text":"<pre><code>oc create sa cluster-admin -n openshift-jobs\n\noc adm policy add-cluster-role-to-user cluster-admin system:serviceaccount:openshift-jobs:cluster-admin\n</code></pre> <pre><code>apiVersion: batch/v2alpha1\nkind: ScheduledJob\nmetadata:\n  name: auto-scale-down\nspec:\n  schedule: 10 22 * * 5\n  successfulJobsHistoryLimit: 10\n  failedJobsHistoryLimit: 10\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          serviceAccount: cluster-admin\n          serviceAccountName: cluster-admin\n          containers:\n          - name: auto-scale-down\n            image: auto-scale-down\n          restartPolicy: Never\n</code></pre>"},{"location":"deploy/scc-anyuid/","title":"SCC anyuid example","text":"","tags":["SCC"]},{"location":"deploy/scc-anyuid/#create-project-and-service-account","title":"Create project and service account","text":"<pre><code>oc new-project anyuid-demo\noc create sa anyuid\n</code></pre>","tags":["SCC"]},{"location":"deploy/scc-anyuid/#allow-service-account-to-use-scc-anyuid","title":"Allow service account to use scc anyuid","text":"","tags":["SCC"]},{"location":"deploy/scc-anyuid/#prior-438","title":"prior 4.3.8","text":"<pre><code>oc adm policy add-scc-to-user -n anyuid-demo -z anyuid anyuid\n</code></pre>","tags":["SCC"]},{"location":"deploy/scc-anyuid/#past-438","title":"past 4.3.8","text":"<p>Use Role-based access to Security Context Constraints.</p> <pre><code>oc create -f - &lt;&lt;EOF\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: scc-anyuid\n  namespace: anyuid-demo\nrules:\n- apiGroups:\n  - security.openshift.io\n  resourceNames:\n  - anyuid\n  resources:\n  - securitycontextconstraints\n  verbs:\n  - use\nEOF\n\noc create -f - &lt;&lt;EOF\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: sa-to-scc-anyuid\n  namespace: anyuid-demo\nsubjects:\n  - kind: ServiceAccount\n    name: anyuid\nroleRef:\n  kind: Role\n  name: scc-anyuid\n  apiGroup: rbac.authorization.k8s.io\nEOF\n</code></pre>","tags":["SCC"]},{"location":"deploy/scc-anyuid/#deploy","title":"Deploy","text":"","tags":["SCC"]},{"location":"deploy/scc-anyuid/#without-anyuid","title":"without-anyuid","text":"<pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: DeploymentConfig\nmetadata:\n  name: without-anyuid\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        deploymentconfig: without-anyuid\n    spec:\n      containers:\n      - image: ubi8/ubi-minimal\n        name: container\n        command:\n          - \"/bin/sh\"\n          - \"-c\"\n          - |\n            while true ; do\n              date;\n              echo -n \"id: \"\n              id;\n              sleep 1;\n            done;\n  triggers:\n  - type: ConfigChange\nEOF\n</code></pre>","tags":["SCC"]},{"location":"deploy/scc-anyuid/#with-anyuid","title":"with-anyuid","text":"<p>Note</p> <p>Important is the <code>serviceAccount</code> and <code>serviceAccountName</code>!</p> <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: DeploymentConfig\nmetadata:\n  name: with-anyuid\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        deploymentconfig: with-anyuid\n    spec:\n      serviceAccount: anyuid\n      serviceAccountName: anyuid\n      containers:\n      - image: ubi8/ubi-minimal\n        name: container\n        command:\n          - \"/bin/sh\"\n          - \"-c\"\n          - |\n            while true ; do\n              date;\n              echo -n \"id: \"\n              id;\n              sleep 1;\n            done;\n  triggers:\n  - type: ConfigChange\nEOF\n</code></pre>","tags":["SCC"]},{"location":"deploy/scc-anyuid/#result","title":"Result:","text":"<pre><code>$ oc get pods -l deployment -o \"custom-columns=NAME:.metadata.name,SCC:.metadata.annotations.openshift\\.io/scc,SERVICEACCOUNT:.spec.serviceAccountName\"\nNAME                     SCC          SERVICEACCOUNT\nwith-anyuid-1-gxczf      anyuid       anyuid\nwithout-anyuid-1-fdhfb   restricted   default\n\n$ oc logs  dc/without-anyuid  | tail -2\nFri Apr 17 10:11:14 UTC 2020\nid: uid=1000540000(1000540000) gid=0(root) groups=0(root),1000540000\n$ oc logs  dc/with-anyuid  | tail -2\nFri Apr 17 10:11:18 UTC 2020\nid: uid=0(root) gid=0(root) groups=0(root)\n</code></pre>","tags":["SCC"]},{"location":"deploy/statefulset/","title":"StatefulSet","text":""},{"location":"deploy/statefulset/#nginx-example","title":"nginx example","text":"<pre><code>---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  ports:\n  - port: 80\n    name: web\n  clusterIP: None\n  selector:\n    app: nginx\n---\napiVersion: apps/v1beta1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  serviceName: \"nginx\"\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: gcr.io/google_containers/nginx-slim:0.8\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n  volumeClaimTemplates:\n  - metadata:\n      name: www\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 1Gi\n</code></pre> <p>Source: https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/</p>"},{"location":"deploy/storage/","title":"Storage","text":""},{"location":"deploy/storage/#hostpath-examples-pvpvcpod-varlog","title":"HostPath examples (PV,PVC,Pod) /var/log","text":"<pre><code># oc adm policy add-cluster-role-to-user sudoer admin\n# oc create sa anyuid\n# oc adm policy add-scc-to-user anyuid -z anyuid --as=system:admin\n\n# Don't forget:  chcon -Rt svirt_sandbox_file_t /pv/hostpath-example\noc create -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  creationTimestamp: null\n  name: hostpath-var-log\nspec:\n  accessModes:\n  - ReadWriteOnce\n  capacity:\n    storage: 100Mi\n  hostPath:\n    path: /var/log\n  persistentVolumeReclaimPolicy: Retain\nEOF\n\noc create -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: hostpath\nspec:\n  accessModes: [ \"ReadWriteMany\" ]\n  resources:\n    requests:\n      storage: 100Mi\nEOF\n\noc create -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox-hostpath\nspec:\n  containers:\n    - name: busybox-hostpath\n      image: busybox\n      command: [ \"/bin/sh\", \"-c\", \"while true ; do date; sleep 1; done;\" ]\n      volumeMounts:\n        - mountPath: /hostpath\n          name: hostpath\n  volumes:\n    - name: hostpath\n      persistentVolumeClaim:\n        claimName: hostpath\n  restartPolicy: Never\nEOF\n</code></pre>"},{"location":"deploy/storage/#dynamic-provisioning","title":"Dynamic Provisioning","text":""},{"location":"deploy/storage/#pvc-with-storageclass","title":"PVC with StorageClass","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: example-pvc-rwo\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: standard\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: gce1\n  annotations:\n    volume.alpha.kubernetes.io/storage-class: anything\nspec:\n  accessModes: [ \"ReadWriteOnce\" ]\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre>"},{"location":"deploy/storage/#pvc-label-selector","title":"PVC label selector","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  creationTimestamp: null\n  labels:\n    special: \"10\"\n    type: nfs\n  name: pv-utility-app5g0010\nspec:\n  accessModes:\n  - ReadWriteMany\n  capacity:\n    storage: 5Gi\n  nfs:\n    path: /exports/app/app5g0010\n    server: utility\n  persistentVolumeReclaimPolicy: Recycle\nstatus: {}\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-utility-app5g0010\nspec:\n  accessModes: [ \"ReadWriteMany\" ]\n  selector:\n    matchLabels:\n      special: 10\n  resources:\n    requests:\n      storage: 2Gi\n</code></pre>"},{"location":"deploy/storage/#example-pod-to-pvc","title":"Example POD to PVC","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  containers:\n    - name: busybox\n      image: busybox\n      command: [ \"/bin/sh\", \"-c\", \"while true ; do date; sleep 1; done;\" ]\n      volumeMounts:\n        - mountPath: /pvc\n          name: pvc\n  volumes:\n    - name: pvc\n      persistentVolumeClaim:\n        claimName: example-pvc-rwo\n  restartPolicy: Never\n</code></pre>"},{"location":"deploy/autoscaling/","title":"POD Autoscaling","text":"","tags":["autoscaling"]},{"location":"deploy/autoscaling/#deploy-pod-autoscaling-exmaple-with-chaos-professor","title":"Deploy Pod Autoscaling exmaple with Chaos Professor","text":"OCpod-autoscaling-template.yaml <pre><code>oc process -f https://examples.openshift.pub/pr-133/deploy/autoscaling/pod-autoscaling-template.yaml | oc apply -f -\n</code></pre> <pre><code>---\nkind: Template\napiVersion: template.openshift.io/v1\nmetadata:\n  name: pod-autoscaling-example\n  creationTimestamp:\n  annotations:\n    description: Pod Autoscaling exmaple with Chaos Professor\n    iconClass: icon-tomcat\n    tags: tomcat,tomcat7,java,jboss,xpaas,autoscaling,chaos-professor\n    version: 0.0.1\nobjects:\n- kind: ImageStream\n  apiVersion: v1\n  metadata:\n    name: redhat-openjdk18-openshift\n  spec:\n    dockerImageRepository: registry.access.redhat.com/redhat-openjdk-18/openjdk18-openshift\n    tags:\n    - name: '1.0'\n      annotations:\n        description: OpenJDK S2I images.\n        iconClass: icon-jboss\n        tags: builder,java,xpaas\n        supports: java:8,xpaas:1.0\n        sampleRepo: https://github.com/jboss-openshift/openshift-quickstarts\n        sampleContextDir: undertow-servlet\n        version: '1.0'\n- kind: Service\n  apiVersion: v1\n  spec:\n    ports:\n    - port: 8080\n      targetPort: 8080\n    selector:\n      deploymentConfig: \"${APPLICATION_NAME}\"\n  metadata:\n    name: \"${APPLICATION_NAME}\"\n    labels:\n      application: \"${APPLICATION_NAME}\"\n    annotations:\n      description: The web server's http port.\n- kind: Route\n  apiVersion: v1\n  id: \"${APPLICATION_NAME}-http\"\n  metadata:\n    name: \"${APPLICATION_NAME}\"\n    labels:\n      application: \"${APPLICATION_NAME}\"\n    annotations:\n      description: Route for application's http service.\n  spec:\n    host: \"${HOSTNAME_HTTP}\"\n    to:\n      name: \"${APPLICATION_NAME}\"\n- kind: ImageStream\n  apiVersion: v1\n  metadata:\n    name: \"${APPLICATION_NAME}\"\n    labels:\n      application: \"${APPLICATION_NAME}\"\n- kind: BuildConfig\n  apiVersion: v1\n  metadata:\n    name: \"${APPLICATION_NAME}\"\n    labels:\n      application: \"${APPLICATION_NAME}\"\n  spec:\n    source:\n      type: Git\n      git:\n        uri: \"${SOURCE_REPOSITORY_URL}\"\n        ref: \"${SOURCE_REPOSITORY_REF}\"\n      contextDir: \"${CONTEXT_DIR}\"\n    strategy:\n      type: Source\n      sourceStrategy:\n        forcePull: true\n        from:\n          kind: ImageStreamTag\n          name: redhat-openjdk18-openshift:latest\n    output:\n      to:\n        kind: ImageStreamTag\n        name: \"${APPLICATION_NAME}:latest\"\n    triggers:\n    - type: GitHub\n      github:\n        secret: \"${GITHUB_WEBHOOK_SECRET}\"\n    - type: Generic\n      generic:\n        secret: \"${GENERIC_WEBHOOK_SECRET}\"\n    - type: ImageChange\n      imageChange: {}\n    - type: ConfigChange\n- kind: DeploymentConfig\n  apiVersion: v1\n  metadata:\n    name: \"${APPLICATION_NAME}\"\n    labels:\n      application: \"${APPLICATION_NAME}\"\n  spec:\n    strategy:\n      type: Recreate\n    triggers:\n    - type: ImageChange\n      imageChangeParams:\n        automatic: true\n        containerNames:\n        - \"${APPLICATION_NAME}\"\n        from:\n          kind: ImageStream\n          name: \"${APPLICATION_NAME}\"\n    - type: ConfigChange\n    replicas: 1\n    selector:\n      deploymentConfig: \"${APPLICATION_NAME}\"\n    template:\n      metadata:\n        name: \"${APPLICATION_NAME}\"\n        labels:\n          deploymentConfig: \"${APPLICATION_NAME}\"\n          application: \"${APPLICATION_NAME}\"\n      spec:\n        terminationGracePeriodSeconds: 60\n        containers:\n        - name: \"${APPLICATION_NAME}\"\n          image: \"${APPLICATION_NAME}\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 512Mi\n            requests:\n              cpu: 500m\n              memory: 512Mi\n          imagePullPolicy: Always\n          readinessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: \"/\"\n              port: 8080\n              scheme: HTTP\n            initialDelaySeconds: 25\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          ports:\n          - name: jolokia\n            containerPort: 8778\n            protocol: TCP\n          - name: http\n            containerPort: 8080\n            protocol: TCP\n          env:\n          - name: JWS_ADMIN_USERNAME\n            value: \"${JWS_ADMIN_USERNAME}\"\n          - name: JWS_ADMIN_PASSWORD\n            value: \"${JWS_ADMIN_PASSWORD}\"\n- kind: HorizontalPodAutoscaler\n  apiVersion: autoscaling/v1\n  metadata:\n    name: \"${APPLICATION_NAME}\"\n    labels:\n      application: \"${APPLICATION_NAME}\"\n  spec:\n    scaleTargetRef:\n      kind: DeploymentConfig\n      name: \"${APPLICATION_NAME}\"\n      apiVersion: v1\n      subresource: scale\n    minReplicas: ${{HorizontalPodAutoscaler_MIN_REPLICAS}}\n    maxReplicas: ${{HorizontalPodAutoscaler_MAX_REPLICAS}}\n    cpuUtilization:\n      targetCPUUtilizationPercentage: \"${HorizontalPodAutoscaler_CPU_TARGET_PERCENTAGE}\"\nparameters:\n- name: APPLICATION_NAME\n  description: The name for the application.\n  value: choas-professor\n  required: true\n- name: HorizontalPodAutoscaler_MIN_REPLICAS\n  description: 'HorizontalPodAutoscaler: min replicas'\n  value: \"1\"\n  required: true\n- name: HorizontalPodAutoscaler_MAX_REPLICAS\n  description: 'HorizontalPodAutoscaler: max replicas'\n  value: \"4\"\n  required: true\n- name: HorizontalPodAutoscaler_CPU_TARGET_PERCENTAGE\n  description: 'HorizontalPodAutoscaler: targetPercentage of cpuUtilization'\n  value: '60'\n  required: true\n- name: HOSTNAME_HTTP\n  description: 'Custom hostname for http service route.  Leave blank for default hostname,\n    e.g.: &lt;application-name&gt;-&lt;project&gt;.&lt;default-domain-suffix&gt;'\n- name: SOURCE_REPOSITORY_URL\n  description: Git source URI for application\n  value: https://github.com/ConSol/chaos-professor.git\n  required: true\n- name: SOURCE_REPOSITORY_REF\n  description: Git branch/tag reference\n  value: master\n- name: CONTEXT_DIR\n  description: Path within Git project to build; empty for root project directory.\n  value: ''\n- name: GITHUB_WEBHOOK_SECRET\n  description: GitHub trigger secret\n  generate: expression\n  from: \"[a-zA-Z0-9]{8}\"\n  required: true\n- name: GENERIC_WEBHOOK_SECRET\n  description: Generic build trigger secret\n  generate: expression\n  from: \"[a-zA-Z0-9]{8}\"\n  required: true\nlabels:\n  template: jws30-tomcat7-basic-s2i\n  xpaas: 1.2.0\n</code></pre>","tags":["autoscaling"]},{"location":"deploy/autoscaling/#test-autoscaling","title":"Test autoscaling","text":"<pre><code># Only 4fun\nab 'http://choas-professor-omd.paas.osp.consol.de/chaos/heapheap?size=500&amp;time=10000'\nab 'http://choas-professor-omd.paas.osp.consol.de/chaos/cpu?threads=100&amp;keepAlive=20000'\nab -c 10 -n 100 'http://choas-professor-omd.paas.osp.consol.de/chaos/cpu?threads=100&amp;keepAlive=200'\n</code></pre>","tags":["autoscaling"]},{"location":"deploy/autoscaling/#delete-all","title":"Delete all","text":"<pre><code>oc get all -o name | xargs -n1  oc delete\noc delete hpa/choas-professor\n</code></pre>","tags":["autoscaling"]},{"location":"deploy/keycloak/","title":"How to deploy and configure keycloak","text":"","tags":["keycloak","redhatsso"]},{"location":"deploy/keycloak/#goals","title":"Goals","text":"<ul> <li> One central Keycloak/SSO instance for varios OpenShift Cluster</li> <li> Keycloak use Google as identifyprovider (via oauth)</li> <li> Configurat OpenShift with two different Identify Providers:<ul> <li> \"COE SSO Admin\"</li> <li> User created with <code>-admin</code> postfix</li> <li> User is automatic in keycloak group <code>idp-coe-sso-admin</code></li> <li> \"COE SSO\"</li> <li> User is automatic in keycloak group <code>idp-coe-sso</code></li> </ul> </li> <li> Keycloak provide a group <code>coe-sso-admin</code> where we add admin user</li> <li> OpenShift Cluster give group <code>coe-sso-admin</code> cluster-admin privileges: <code>oc adm policy add-cluster-role-to-group cluster-admin coe-sso-admin</code></li> </ul>","tags":["keycloak","redhatsso"]},{"location":"deploy/keycloak/#keycloak-installation","title":"Keycloak installation","text":"<ul> <li>Install following operators via OperatorHub<ul> <li>CloudNativePG (Cerified Operator)</li> <li>Keycloak Operator (Red Hat Operator)</li> </ul> </li> </ul>","tags":["keycloak","redhatsso"]},{"location":"deploy/keycloak/#spinup-postgresql-database-via-cloudnativepg","title":"Spinup PostgreSQL database via CloudNativePG","text":"<p>In my setup we use SAN Storage (iscsi) provided from a Netapp via Trident and the database should run on my control plan/master nodes.</p> postgresql cluster cr <pre><code>kind: Cluster\napiVersion: postgresql.cnpg.io/v1\nmetadata:\n  name: pq-for-rhbk\n  namespace: rhbk-operator\nspec:\n  affinity:\n    nodeSelector:\n      node-role.kubernetes.io/master: \"\"\n    tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n  instances: 3\n  logLevel: info\n  primaryUpdateStrategy: unsupervised\n  storage:\n    size: 3Gi\n    storageClass: coe-netapp-san\n  walStorage:\n    size: 3Gi\n    storageClass: coe-netapp-san\n</code></pre>","tags":["keycloak","redhatsso"]},{"location":"deploy/keycloak/#spinup-keycloak","title":"Spinup Keycloak","text":"<ul> <li> <p>I'm using a customer DNS name, <code>sso.coe.muc.redhat.com</code>   DNS configuration:</p> <pre><code>sso.coe.muc.redhat.com.   86400   IN  CNAME  *.apps.isar.coe.muc.redhat.com.\n</code></pre> </li> <li> <p>SSL Certificate for <code>sso.coe.muc.redhat.com</code> is stared in Vault and copied into a secret via ExternalSecret Operator</p> ExternalSecret <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: cert-wildcard-coe\n  namespace: rhbk-operator\nspec:\n  data:\n  - remoteRef:\n      key: coe-lab/wildcard-cert-coe\n      property: wildcard-coe.chain.cert\n    secretKey: tls.crt\n  - remoteRef:\n      key: coe-lab/wildcard-cert-coe\n      property: wildcard-coe.key\n    secretKey: tls.key\n  refreshInterval: 12h\n  secretStoreRef:\n    kind: ClusterSecretStore\n    name: redhat-vault\n  target:\n    creationPolicy: Owner\n    deletionPolicy: Retain\n    name: cert-wildcard-coe\n    template:\n      type: kubernetes.io/tls\n</code></pre> </li> <li> <p>Deploy Red Hat Build of Keycloak</p> Keycloak <pre><code>apiVersion: k8s.keycloak.org/v2alpha1\nkind: Keycloak\nmetadata:\n  name: coe-sso\n  namespace: rhbk-operator\nspec:\n  instances: 2\n  db:\n    vendor: postgres\n    host: pq-for-rhbk-rw\n    database: app\n    usernameSecret:\n      name: pq-for-rhbk-app\n      key: username\n    passwordSecret:\n      name: pq-for-rhbk-app\n      key: password\n  http:\n    tlsSecret: cert-wildcard-coe\n  hostname:\n    hostname: sso.coe.muc.redhat.com\n</code></pre> </li> </ul> <p>GitOpsified deployment is here: https://github.com/stormshift/clusters/tree/main/isar-apps/keycloak</p>","tags":["keycloak","redhatsso"]},{"location":"deploy/keycloak/#keycloak-configuration","title":"Keycloak Configuration","text":"<p>Login into keycloak, in my case https://sso.coe.muc.redhat.com</p> <p>Get the initial admin user and password:</p> <pre><code># Username\n$ oc get secrets -n rhbk-operator  coe-sso-initial-admin  -o jsonpath=\"{.data.username}\" | base64 -d;echo\nadmin\n\n# Password\n$ oc get secrets -n rhbk-operator  coe-sso-initial-admin  -o jsonpath=\"{.data.password}\" | base64 -d;echo\ncbd4f....\n</code></pre>","tags":["keycloak","redhatsso"]},{"location":"deploy/keycloak/#create-a-new-realm-coe-sso","title":"Create a new realm <code>coe-sso</code>","text":"<ul> <li> <p>Create realm</p> Screenshot <p></p> </li> <li> <p>Realm name: coe-sso</p> </li> </ul>","tags":["keycloak","redhatsso"]},{"location":"deploy/keycloak/#add-group-idp-coe-sso","title":"Add group <code>idp-coe-sso</code>","text":"<ul> <li> <p>Select Groups on the left</p> </li> <li> <p>Click Greate group</p> </li> <li> <p>Fill out the form, Name: <code>idp-coe-sso</code></p> </li> <li> <p>Click Create</p> </li> </ul>","tags":["keycloak","redhatsso"]},{"location":"deploy/keycloak/#add-identity-provider","title":"Add identity provider","text":"<ul> <li> <p>Go to \"Identity providers\" and select one provider of choice - in my case Google.</p> Screenshot <p></p> </li> <li> <p>Fill out the form in your Keycloak instance:</p> <ul> <li> <p>Copy the Redirect URI and paste it into Google's  Authorised redirect URIs part, details: Red Hat SSO - via Google</p> </li> <li> <p>Copy &amp; Paste Client ID and Client Secret from Google to Keycloak</p> </li> <li> <p>Click Add</p> Screenshot <p></p> </li> </ul> </li> </ul>","tags":["keycloak","redhatsso"]},{"location":"deploy/keycloak/#configure-identity-provider","title":"Configure Identity provider","text":"<ul> <li> <p>Advanced settings -&gt; Switch on Trust Email</p> Screenshot <p></p> </li> <li> <p>Add Hardcoded Group mapper,</p> <ul> <li> <p>Go to your identity provider, click Mappers tab next to Settings tab</p> </li> <li> <p>Fill out the form:</p> Screenshot <p></p> </li> </ul> </li> </ul>","tags":["keycloak","redhatsso"]},{"location":"deploy/keycloak/#create-another-realm-coe-sso-admin","title":"Create another realm <code>coe-sso-admin</code>","text":"<p>Idea here is having a second realm analog to the first one, but it creates user with <code>-admin</code> post-fix to the username.</p> <ul> <li> <p>Create a new realm and add identity provider, as described above.</p> </li> <li> <p>Add a Username Template Importer mapper</p> <ul> <li> <p>Go to your identity provider, click Mappers tab next to Settings tab</p> </li> <li> <p>Fill out the form:</p> </li> <li> <p>Template: <code>${CLAIM.email}-admin</code></p> <p>??? note \"Screenshot\"</p> <pre><code>  ![](images/add-identity-provider-mapper.png)\n</code></pre> </li> </ul> </li> </ul>","tags":["keycloak","redhatsso"]},{"location":"deploy/keycloak/#skip-the-keycloak-login-page","title":"Skip the Keycloak login page","text":"<ul> <li> <p>Click Authentication at the left menu</p> </li> <li> <p>Select Tab Flows</p> </li> <li> <p>Click Flow name browser</p> Screenshot <p></p> </li> <li> <p>Configure Identity Provider Redirector</p> Screenshot <p></p> Screenshot <p></p> </li> </ul>","tags":["keycloak","redhatsso"]},{"location":"deploy/keycloak/#create-client-for-openshift-cluster","title":"Create Client for OpenShift Cluster","text":"<ul> <li> <p>Click Clients</p> </li> <li> <p>Select Tab Client list</p> </li> <li> <p>Click Create client</p> </li> <li> <p>General Settings</p> <ul> <li>Important is here Client ID, this is needed for OpenShift OAuth config later</li> </ul> Screenshot <p></p> </li> <li> <p>Capability config</p> Screenshot <p></p> </li> <li> <p>Login settings</p> <ul> <li> <p>Valid redirect URIs: <code>https://oauth-openshift.apps.$cluster-name$.$basedomain$:443/oauth2callback/*</code></p> </li> <li> <p>Web origins: <code>https://oauth-openshift.apps.$cluster-name$.$basedomain$:443</code></p> </li> </ul> Screenshot <p></p> </li> <li> <p>Click Create</p> </li> <li> <p>Select tab Client scopes</p> </li> <li> <p>Click <code>$NAME</code>-dedicted client scope</p> </li> <li> <p>Click Configure a new mapper</p> </li> <li> <p>Click Group Membership</p> Screenshot <p></p> </li> <li> <p>Fill out form</p> Screenshot <p></p> </li> <li> <p>Go back to Client details</p> </li> <li> <p>Select tab Credentials</p> </li> <li> <p>Store/Copy Client secret this is needed for OpenShift OAuth config later</p> </li> </ul>","tags":["keycloak","redhatsso"]},{"location":"deploy/keycloak/#configure-keycloak-at-your-openshift-cluster","title":"Configure Keycloak at your OpenShift Cluster","text":"<ul> <li> <p>Go to Administration -&gt; Cluster Settings -&gt; Configuration -&gt; OAuth</p> </li> <li> <p>Added YAML and add OpenID Connect provider:</p> <pre><code>- mappingMethod: add\n    name: COE-SSO-Admin\n    openID:\n      ca:\n        # Configmap with root ca, key: ca.crt\n        name: openid-ca-gnmjm\n      claims:\n        email:\n        - email\n        groups:\n        - groups\n        name:\n        - name\n        preferredUsername:\n        - preferred_username\n      clientID: isar.coe.muc.redhat.com\n      clientSecret:\n        # Secret with client secret, key: clientSecret\n        name: openid-client-secrot-coe-sso-admin\n      issuer: https://sso.coe.muc.redhat.com/realms/coe-sso-admin\n    type: OpenID\n</code></pre> </li> </ul>","tags":["keycloak","redhatsso"]},{"location":"deploy/keycloak/#resources","title":"Resources","text":"<ul> <li>https://keycloakthemes.com/blog/how-to-setup-sign-in-with-google-using-keycloak</li> <li>https://medium.com/keycloak/using-keycloak-identity-provider-to-secure-openshift-f929a7a0f7f1</li> </ul>","tags":["keycloak","redhatsso"]},{"location":"deploy/monitoring/workload/","title":"Application Monitoring example","text":""},{"location":"deploy/monitoring/workload/#deploy-cluster-wide-workload-monitoring-cluster-admin-needed","title":"Deploy cluster wide workload monitoring (cluster-admin needed)","text":"<p>Enabling monitoring for user-defined projects</p>"},{"location":"deploy/monitoring/workload/#enable-user-workload-monitoring","title":"Enable user workload monitoring","text":"<pre><code>oc create -f - &lt;&lt;EOF\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cluster-monitoring-config\n  namespace: openshift-monitoring\ndata:\n  config.yaml: |\n    enableUserWorkload: true\nEOF\n</code></pre>"},{"location":"deploy/monitoring/workload/#check-user-workload-monitoring-stack","title":"Check user workload monitoring stack","text":"<pre><code>$ oc get pods -n openshift-user-workload-monitoring\nNAME                                   READY   STATUS    RESTARTS   AGE\nprometheus-operator-84d9857947-wlmws   2/2     Running   0          49s\nprometheus-user-workload-0             5/5     Running   1          37s\nprometheus-user-workload-1             5/5     Running   1          37s\nthanos-ruler-user-workload-0           3/3     Running   0          38s\nthanos-ruler-user-workload-1           3/3     Running   0          37s\n</code></pre>"},{"location":"deploy/monitoring/workload/#granting-permission","title":"Granting permission","text":"<ul> <li>Granting users permission to monitor user-defined projects</li> <li>Granting user permissions by using the web console or CLI</li> </ul>"},{"location":"deploy/monitoring/workload/#deploy-application","title":"Deploy application","text":"<p>Official example</p> OCdeployment.yaml <pre><code>oc apply -f https://examples.openshift.pub/pr-133/deploy/monitoring/workload/deployment.yaml\n</code></pre> <pre><code>---\napiVersion: v1\nkind: Service\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: opensift-examples\n  name: opensift-examples\nspec:\n  ports:\n  - name: http\n    port: 8080\n    protocol: TCP\n    targetPort: 8080\n  - name: metrics\n    port: 9113\n    protocol: TCP\n    targetPort: 9113\n  selector:\n    app: opensift-examples\n  type: ClusterIP\nstatus:\n  loadBalancer: {}\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations: null\n  labels:\n    app: opensift-examples\n    app.kubernetes.io/component: opensift-examples\n    app.kubernetes.io/instance: opensift-examples\n    app.kubernetes.io/part-of: opensift-examples\n    app.openshift.io/runtime: openshift\n  name: opensift-examples\nspec:\n  progressDeadlineSeconds: 30\n  replicas: 1\n  selector:\n    matchLabels:\n      app: opensift-examples\n  template:\n    metadata:\n      labels:\n        app: opensift-examples\n    spec:\n      containers:\n      - image: quay.io/openshift-examples/web:master\n        name: web\n        ports:\n        - containerPort: 8080\n          name: http\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n      - args:\n        - -nginx.scrape-uri=http://localhost:8081/\n        image: docker.io/nginx/nginx-prometheus-exporter:latest\n        name: nginx-prometheus-exporter\n        ports:\n        - containerPort: 9113\n          name: metrics\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 9113\n</code></pre>"},{"location":"deploy/monitoring/workload/#create-service-monitoring","title":"Create Service Monitoring","text":"OCdeployment.yaml <pre><code>oc apply -f https://examples.openshift.pub/pr-133/deploy/monitoring/workload/servicemonitor.yaml\n</code></pre> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  labels:\n    app: opensift-examples\n  name: opensift-examples\nspec:\n  endpoints:\n  - interval: 30s\n    port: metrics\n    scheme: http\n  selector:\n    matchLabels:\n      app: opensift-examples\n</code></pre>"},{"location":"deploy/monitoring/workload/#result","title":"Result","text":""},{"location":"deploy/workload/gitlab-runner/","title":"Gitlab Runner","text":"<p>Gitlab runner image based on ubi: https://quay.io/repository/openshift-examples/gitlab-runner</p> <p></p>"},{"location":"deploy/workload/gitlab-runner/#deploy-the-runner","title":"Deploy the runner","text":"<pre><code>oc new-project gitlab-runner\n\noc create sa gitlab-runner\noc adm policy add-role-to-user edit -z gitlab-runner\n\n# Create empty secret gitlab-runner-config\noc create secret generic gitlab-runner-config \\\n    --from-literal=config.toml=\"\"\n\n# Create secret for registration gitlab-runner-register\noc create secret generic gitlab-runner-register \\\n    --from-literal=REGISTRATION_TOKEN=xL6C6HEg2CFdXS3rM_BZ \\\n    --from-literal=CI_SERVER_URL=https://gitlab.com/ \\\n    --from-literal=RUNNER_EXECUTOR=kubernetes \\\n    --from-literal=RUNNER_TAG_LIST=ocp\n</code></pre> <p>Apply deployment config</p> <pre><code>oc create -f - &lt;&lt;EOF\napiVersion: v1\nkind: DeploymentConfig\nmetadata:\n  name: gitlab-runner\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      name: gitlab-runner\n      labels:\n        app: gitlab-runner\n    spec:\n      serviceAccount: gitlab-runner\n      containers:\n      - name: run\n        image:  quay.io/openshift-examples/gitlab-runner:latest\n        command:\n          - /usr/bin/bash\n          - -x\n          - -c\n          - |\n            # Register\n            cp -v /gitlab-runner-config/config.toml $HOME/.gitlab-runner/config.toml\n\n            gitlab-runner unregister --all-runners\n            # unregister do not update config.toml if unregister fail.\n            rm $HOME/.gitlab-runner/config.*\n\n            gitlab-runner register --non-interactive\n\n            # Update secret\n            oc create secret generic gitlab-runner-config  --from-file=config.toml=$HOME/.gitlab-runner/config.toml --dry-run -o yaml | oc apply -f -\n\n            gitlab-runner run\n        volumeMounts:\n        - name: gitlab-runner-config\n          mountPath: \"/gitlab-runner-config\"\n          readOnly: true\n        env:\n        - name: KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        envFrom:\n        - secretRef:\n            name: gitlab-runner-register\n        livenessProbe:\n          exec:\n            command:\n            - /usr/bin/bash\n            - -x\n            - -c\n            - |\n              # gitlab-runner verify wont work with $CI_SERVER_URL\n              unset CI_SERVER_URL\n              gitlab-runner verify\n          initialDelaySeconds: 10\n          periodSeconds: 5\n      volumes:\n      - name: gitlab-runner-register\n        secret:\n          secretName: gitlab-runner-register\n      - name: gitlab-runner-config\n        secret:\n          secretName: gitlab-runner-config\n  triggers:\n  - type: ConfigChange\nEOF\n</code></pre>"},{"location":"deploy/workload/gitlab-runner/#build-gitlab-runner-image","title":"Build gitlab-runner image","text":"<pre><code>git clone https://github.com/rbo/openshift-examples.git\ncd openshift-examples/workload/gitlab-runner/\npodman build -t gitlab-runer .\n</code></pre> <p>Checkout the Dockerfile for more details.</p>"},{"location":"deploy/workload/gitlab-runner/#notes","title":"Notes","text":"<ul> <li><code>gitlab-runner verify</code> don't work with env variable <code>CI_SERVER_URL</code>, know issue: https://gitlab.com/gitlab-org/gitlab-runner/issues/3904</li> <li><code>gitlab-runner run</code> won't die in case of errors</li> <li>Overall it looks like gitlab-runner is not build to run inside a container, but it work.</li> </ul>"},{"location":"deploy/workload/grafana-oauth/","title":"Grafana with OAuth Proxy","text":""},{"location":"deploy/workload/grafana-oauth/#build","title":"Build","text":"<pre><code>---\napiVersion: v1\nkind: ImageStream\nmetadata:\n  labels:\n    app: grafana\n  name: grafana\n---\napiVersion: v1\nkind: BuildConfig\nmetadata:\n  labels:\n    app: grafana\n  name: grafana\nspec:\n  failedBuildsHistoryLimit: 5\n  nodeSelector: null\n  output:\n    to:\n      kind: ImageStreamTag\n      name: grafana:latest\n  postCommit: {}\n  resources: {}\n  runPolicy: Serial\n  source:\n    git:\n      ref: master\n      uri: https://github.com/rbo/grafana-docker.git\n    type: Git\n  strategy:\n    dockerStrategy: {}\n    type: Docker\n  successfulBuildsHistoryLimit: 5\n  triggers:\n  - type: ConfigChange\n</code></pre>"},{"location":"deploy/workload/grafana-oauth/#deployment","title":"Deployment","text":"<pre><code>kind: List\napiVersion: v1\nitems:\n# Create a proxy service account and ensure it will use the route \"proxy\"\n- apiVersion: v1\n  kind: ServiceAccount\n  metadata:\n    name: grafana\n    annotations:\n      serviceaccounts.openshift.io/oauth-redirectreference.primary: '{\"kind\":\"OAuthRedirectReference\",\"apiVersion\":\"v1\",\"reference\":{\"kind\":\"Route\",\"name\":\"grafana\"}}'\n# Create a secure connection to the proxy via a route\n- apiVersion: route.openshift.io/v1\n  kind: Route\n  metadata:\n    name: grafana\n  spec:\n    to:\n      name: grafana\n    tls:\n      termination: Reencrypt\n- apiVersion: v1\n  kind: Service\n  metadata:\n    name: grafana\n    annotations:\n      service.alpha.openshift.io/serving-cert-secret-name: grafana-tls\n  spec:\n    ports:\n    - name: grafana\n      port: 443\n      targetPort: 8443\n    selector:\n      app: grafana\n- apiVersion: v1\n  kind: DeploymentConfig\n  metadata:\n    labels:\n      app: grafana\n    name: grafana\n  spec:\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      deploymentConfig: grafana\n    strategy:\n      activeDeadlineSeconds: 21600\n      recreateParams:\n        timeoutSeconds: 600\n      resources: {}\n      type: Recreate\n    template:\n      metadata:\n        labels:\n          app: grafana\n          deploymentConfig: grafana\n        name: grafana\n      spec:\n        serviceAccountName: grafana\n        containers:\n        - name: oauth-proxy\n          image: openshift/oauth-proxy:v1.0.0\n          imagePullPolicy: IfNotPresent\n          ports:\n          - containerPort: 8443\n            name: public\n          args:\n          - --https-address=:8443\n          - --provider=openshift\n          - --openshift-service-account=grafana\n          - --upstream=http://localhost:3000\n          - --tls-cert=/etc/tls/private/tls.crt\n          - --tls-key=/etc/tls/private/tls.key\n          - --cookie-secret=SECRET\n          - --pass-basic-auth=false\n          volumeMounts:\n          - mountPath: /etc/tls/private\n            name: grafana-tls\n        - image: grafana:latest\n          imagePullPolicy: Always\n          name: grafana\n          env:\n          - name: GF_AUTH_BASIC_ENABLED\n            value: 'true'\n          - name: GF_AUTH_PROXY_ENABLED\n            value: 'true'\n          - name: GF_AUTH_PROXY_HEADER_NAME\n            value: 'X-Forwarded-User'\n          - name: GF_AUTH_PROXY_HEADER_PROPERTY\n            value: 'username'\n          - name: GF_AUTH_PROXY_AUTO_SIGN_UP\n            value: 'true'\n          - name: GF_AUTH_DISABLE_LOGIN_FORM\n            value: 'true'\n          - name: GF_USERS_ALLOW_SIGN_UP\n            value: 'false'\n\n          ports:\n          - containerPort: 3000\n            name: http\n            protocol: TCP\n          resources: {}\n          securityContext: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n        volumes:\n          - name: grafana-tls\n            secret:\n              secretName: grafana-tls\n        dnsPolicy: ClusterFirst\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 60\n    test: false\n    triggers:\n    - imageChangeParams:\n        automatic: true\n        containerNames:\n        - grafana\n        from:\n          kind: ImageStreamTag\n          name: grafana:latest\n      type: ImageChange\n    - type: ConfigChange\n</code></pre>"},{"location":"deploy/workload/jfrog-artifactory-enterprise-operator-air-gapped/","title":"JFrog Artifactory Enterprise Operator","text":"<p>How to setup JFrog Artifactory Enterprise Operator on a disconnected cluster</p> <p>Tested on OpenShift 4.5.14</p> <p>Operator mirror follows the official documentation: Using Operator Lifecycle Manager on restricted networks</p>","tags":["air-gapped","restricted-network","disconnected"]},{"location":"deploy/workload/jfrog-artifactory-enterprise-operator-air-gapped/#export-some-environment-variables","title":"Export some environment variables","text":"<p>Some variables for better copy&amp;paste experience of commands.</p> <pre><code>export OCP_RELEASE=$(oc version -o json  --client | jq -r '.releaseClientVersion')\nexport LOCAL_REGISTRY='host.compute.local:5000'\nexport LOCAL_REPOSITORY='ocp4/openshift4'\nexport PRODUCT_REPO='openshift-release-dev'\nexport LOCAL_SECRET_JSON='/root/hetzner-ocp4/pullsecret.json'\nexport RELEASE_NAME=\"ocp-release\"\nexport ARCHITECTURE=x86_64\nexport KUBECONFIG=/root/hetzner-ocp4/air-gapped/auth/kubeconfig\n# export SERIAL=$(date +%s)\nexport SERIAL=1604840032\n</code></pre>","tags":["air-gapped","restricted-network","disconnected"]},{"location":"deploy/workload/jfrog-artifactory-enterprise-operator-air-gapped/#setup-a-postgresql-database","title":"Setup a PostgreSQL database","text":"<p>For demo purpose we only setup a singe instance PostgreSQL, for production usage you should setup a PostgreSQL cluster.</p>","tags":["air-gapped","restricted-network","disconnected"]},{"location":"deploy/workload/jfrog-artifactory-enterprise-operator-air-gapped/#mirror-images","title":"Mirror images","text":"<pre><code>oc image mirror \\\n  -a ${LOCAL_SECRET_JSON} \\\n  --filter-by-os='.*' \\\n  registry.redhat.io/rhel8/postgresql-12:latest  \\\n  ${LOCAL_REGISTRY}/rhel8/postgresql-12:latest\n</code></pre>","tags":["air-gapped","restricted-network","disconnected"]},{"location":"deploy/workload/jfrog-artifactory-enterprise-operator-air-gapped/#deploy-postgresql","title":"Deploy PostgreSQL","text":"<pre><code>oc import-image -n openshift postgresql:12-el8 --from=${LOCAL_REGISTRY}/rhel8/postgresql-12:latest --confirm\n\noc process -f https://raw.githubusercontent.com/openshift/library/master/official/postgresql/templates/postgresql-persistent.json \\\n  -p POSTGRESQL_VERSION=12-el8 \\\n  -p POSTGRESQL_USER=jfrog \\\n  -p POSTGRESQL_PASSWORD=jfrog \\\n  -p POSTGRESQL_DATABASE=jfrog  | oc apply -f -\n</code></pre>","tags":["air-gapped","restricted-network","disconnected"]},{"location":"deploy/workload/jfrog-artifactory-enterprise-operator-air-gapped/#build-catalog-of-certified-operators","title":"Build catalog of certified-operators","text":"<pre><code>oc adm catalog build \\\n  --appregistry-org certified-operators \\\n  --from=registry.redhat.io/openshift4/ose-operator-registry:v4.5 \\\n  --to=${LOCAL_REGISTRY}/olm/certified-operators:${SERIAL} \\\n  -a ${LOCAL_SECRET_JSON} 2&gt;&amp;1 | tee certified-operators.build.${SERIAL}.log\n</code></pre>","tags":["air-gapped","restricted-network","disconnected"]},{"location":"deploy/workload/jfrog-artifactory-enterprise-operator-air-gapped/#apply-catalog-source","title":"Apply catalog source","text":"<pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: operators.coreos.com/v1alpha1\nkind: CatalogSource\nmetadata:\n  name: my-certified-operators-${SERIAL}\n  namespace: openshift-marketplace\nspec:\n  sourceType: grpc\n  image: ${LOCAL_REGISTRY}/olm/certified-operators:${SERIAL}\n  displayName: My Red Hat Catalog ${SERIAL}\n  publisher: grpc\nEOF\n</code></pre>","tags":["air-gapped","restricted-network","disconnected"]},{"location":"deploy/workload/jfrog-artifactory-enterprise-operator-air-gapped/#check-after-a-while-if-the-jfrog-artifactory-operator-is-available","title":"Check after a while if the jFrog Artifactory Operator is available","text":"<pre><code>$ oc get  packagemanifests -n openshift-marketplace openshiftartifactoryha-operator\nNAME                              CATALOG                         AGE\nopenshiftartifactoryha-operator   My Red Hat Catalog 1604840032   2m54s\n</code></pre>","tags":["air-gapped","restricted-network","disconnected"]},{"location":"deploy/workload/jfrog-artifactory-enterprise-operator-air-gapped/#mirroring-images","title":"Mirroring images","text":"<p>To save time, bandwidth and storage I mirrored only the necessary images.</p>","tags":["air-gapped","restricted-network","disconnected"]},{"location":"deploy/workload/jfrog-artifactory-enterprise-operator-air-gapped/#find-out-necessary-images","title":"Find out necessary images","text":"<pre><code>$ podman run --authfile ${LOCAL_SECRET_JSON} -ti --rm --entrypoint bash  ${LOCAL_REGISTRY}/olm/certified-operators:${SERIAL}\nsh-4 $ echo \"select image from related_image \\\n      where operatorbundle_name like 'artifactory-ha-operator%';\"       | sqlite3 -line /bundles.db | grep -v '^$' | sort -u\n\nimage = registry.connect.redhat.com/jfrog/artifactory-operator@sha256:1ee230a6e86d85fe775649c245a015bc661b486979d84f76906a272ecd5be86b\nimage = registry.connect.redhat.com/jfrog/artifactory-operator@sha256:376f2922911113a0e217614e1f0d26672d62258170b7ae1dcad0fdd596e035a6\nimage = registry.connect.redhat.com/jfrog/artifactory-operator@sha256:65388065cdb5614b57090d70850246c1d9072e7af08f710b6126766b1bdf36c5\nimage = registry.connect.redhat.com/jfrog/artifactory-operator@sha256:9c1765bd4934716b6e8221a3eb626e8b427f5d1025be3dbfb4b780d00e198187\nimage = registry.connect.redhat.com/jfrog/artifactory-operator@sha256:a5e9e99889cbe2c4ee96c80d908e80987371b66f9d99c6c6c6155583a9ef505a\nimage = registry.connect.redhat.com/jfrog/artifactory-operator@sha256:afa99e3eed71856cbfed218f50a6d098a57976237253157b2e2b6bfabc874838\nimage = registry.connect.redhat.com/jfrog/artifactory-operator@sha256:fcbfec1b21e41b9203f05ed82df072f6741dfbae76f998a2c311c25219c01f01\nimage = registry.connect.redhat.com/jfrog/artifactory-pro@sha256:26129b25cc851b7fcd1b90f26e47470900a4de6db857638deeb06cde5877befa\nimage = registry.connect.redhat.com/jfrog/artifactory-pro@sha256:97eb6bd2639523ec5f8f7d7e87953ceda515244ef2e7ee4bef08f7eb19faa7ca\nimage = registry.connect.redhat.com/jfrog/artifactory-pro@sha256:a9bb2467a5fe09a347664130930543a1d902ffec708bc25b0638c3a4fa75dc0c\nimage = registry.connect.redhat.com/jfrog/artifactory-pro@sha256:ad71e9f1082427f3e9930ba0fd21676feeda22df4b8f210609b8b471aa7ca5f6\nimage = registry.connect.redhat.com/jfrog/artifactory-pro@sha256:cf846eb2ffd6aaffa033a0d3fa4cd1c029707f97080627c0cd747a7a7a915084\nimage = registry.connect.redhat.com/jfrog/artifactory-pro@sha256:f0b061c4126f58b70ddaf39a3c5cc009be21b75ac4530ec5088a40d1e6f50e3e\nimage = registry.connect.redhat.com/jfrog/init@sha256:197f5a1e7a3dd934e72a03a106f0de83e992e6926a774e26483c06fa46faeee5\nimage = registry.connect.redhat.com/jfrog/init@sha256:a93d4e5afe363edacf3928c510b1a4d82344df71203ae3fc950144d8b0098f5e\nimage = registry.redhat.io/rhel8/nginx-116@sha256:0ba76a7b26e5ffb95b4354243337ac2b3ff84ae8637c0782631084d1b2f99a33\n</code></pre>","tags":["air-gapped","restricted-network","disconnected"]},{"location":"deploy/workload/jfrog-artifactory-enterprise-operator-air-gapped/#build-mapping-file","title":"Build mapping file","text":"<pre><code>oc adm catalog mirror \\\n  ${LOCAL_REGISTRY}/olm/certified-operators:${SERIAL} \\\n  ${LOCAL_REGISTRY} \\\n  --to-manifests=certified-operators-${SERIAL} \\\n  --manifests-only \\\n  -a ${LOCAL_SECRET_JSON} 2&gt;&amp;1 | tee certified-operators.mirror-manifests-only.${SERIAL}.log\n</code></pre>","tags":["air-gapped","restricted-network","disconnected"]},{"location":"deploy/workload/jfrog-artifactory-enterprise-operator-air-gapped/#select-only-jfrog-necessary-images-basically-the-list-above","title":"Select only jfrog necessary images (basically the list above.)","text":"<pre><code>grep 'jfrog/artifactory' certified-operators-${SERIAL}/mapping.txt  | tee -a certified-operators-${SERIAL}/mapping.jfrog.txt\ngrep 'jfrog/init' certified-operators-${SERIAL}/mapping.txt | tee -a certified-operators-${SERIAL}/mapping.jfrog.txt\ngrep 'rhel8/nginx-116' certified-operators-${SERIAL}/mapping.txt | tee -a certified-operators-${SERIAL}/mapping.jfrog.txt\n</code></pre>","tags":["air-gapped","restricted-network","disconnected"]},{"location":"deploy/workload/jfrog-artifactory-enterprise-operator-air-gapped/#mirror-the-images","title":"Mirror the images","text":"<pre><code>oc image mirror \\\n  -a ${LOCAL_SECRET_JSON} \\\n  --filter-by-os='.*' \\\n  -f certified-operators-${SERIAL}/mapping.jfrog.txt\n\noc image mirror \\\n  -a ${LOCAL_SECRET_JSON} \\\n  --filter-by-os='.*' \\\n  registry.connect.redhat.com/jfrog/artifactory-pro:7.10.2-1 \\\n  ${LOCAL_REGISTRY}/jfrog/artifactory-pro:7.10.2-1\n\noc image mirror \\\n  -a ${LOCAL_SECRET_JSON} \\\n  --filter-by-os='.*' \\\n  registry.redhat.io/rhel8/nginx-116:latest \\\n  ${LOCAL_REGISTRY}/rhel8/nginx-116:latest\n</code></pre>","tags":["air-gapped","restricted-network","disconnected"]},{"location":"deploy/workload/jfrog-artifactory-enterprise-operator-air-gapped/#apply-imagecontentsourcepolicy","title":"Apply imageContentSourcePolicy","text":"<p>optional you can cleanup/filter imageContentSourcePolicy too</p> <pre><code>oc  apply -f certified-operators-${SERIAL}/imageContentSourcePolicy.yaml\n</code></pre>","tags":["air-gapped","restricted-network","disconnected"]},{"location":"deploy/workload/jfrog-artifactory-enterprise-operator-air-gapped/#install-jfrog-artifactory-enterprise-operator-via-operatorhub-gui","title":"Install JFrog Artifactory Enterprise Operator via OperatorHub GUI","text":"<p>Follow the WebUI</p>","tags":["air-gapped","restricted-network","disconnected"]},{"location":"deploy/workload/jfrog-artifactory-enterprise-operator-air-gapped/#install-jfrog-artifactory-enterprise-operator-via-operatorhub-cli","title":"Install JFrog Artifactory Enterprise Operator via OperatorHub CLI","text":"<p>Official documentation Installing from OperatorHub using the CLI</p> <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: openshiftartifactoryha-operator\n  namespace: openshift-operators\nspec:\n  channel: alpha\n  name: openshiftartifactoryha-operator\n  source:  my-certified-operators-${SERIAL}\n  sourceNamespace: openshift-marketplace\nEOF\n</code></pre>","tags":["air-gapped","restricted-network","disconnected"]},{"location":"deploy/workload/jfrog-artifactory-enterprise-operator-air-gapped/#deploy-jfrog-instance","title":"Deploy JFrog instance","text":"","tags":["air-gapped","restricted-network","disconnected"]},{"location":"deploy/workload/jfrog-artifactory-enterprise-operator-air-gapped/#create-dummy-service-to-get-an-internal-certificate","title":"Create dummy service to get an internal certificate","text":"<pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Service\nmetadata:\n  name: jfrog\n  annotations:\n    service.alpha.openshift.io/serving-cert-secret-name: jfrog-cert\nspec:\n  ports:\n  - name: service-serving-cert\n    port: 443\n    targetPort: 8443\n  selector:\n    app: service-serving-cert\nEOF\n</code></pre>","tags":["air-gapped","restricted-network","disconnected"]},{"location":"deploy/workload/jfrog-artifactory-enterprise-operator-air-gapped/#configure-some-anyuid-privileges","title":"Configure some anyuid privileges","text":"<pre><code>oc adm policy add-scc-to-user anyuid -z openshiftartifactoryha-artifactory-ha\n</code></pre>","tags":["air-gapped","restricted-network","disconnected"]},{"location":"deploy/workload/jfrog-artifactory-enterprise-operator-air-gapped/#create-cr-openshiftartifactoryha","title":"Create CR OpenshiftArtifactoryHa","text":"<pre><code>KEY=$(openssl rand -hex 32)\nTARGET_NAMESPACE=demo\n\noc new-project ${TARGET_NAMESPACE}\n\nNAMESPACE_UID=$(oc get namespace/${TARGET_NAMESPACE} -o jsonpath=\"{.metadata.annotations.openshift\\.io/sa\\.scc\\.uid-range}\" | cut -f1 -d'/')\nNAMESPACE_GID=$(oc get namespace/${TARGET_NAMESPACE} -o jsonpath=\"{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}\" | cut -f1 -d'/')\n\noc apply -f - &lt;&lt;EOF\napiVersion: charts.helm.k8s.io/v1alpha1\nkind: OpenshiftArtifactoryHa\nmetadata:\n  name: openshiftartifactoryha\nspec:\n  artifactory-ha:\n    artifactory:\n      image:\n        registry: ${LOCAL_REGISTRY}\n        repository: jfrog/artifactory-pro\n        tag: 7.10.2-1\n      joinKey: ${KEY}\n      masterKey: ${KEY}\n      uid: ${NAMESPACE_UID}\n      node:\n        replicaCount: 2\n        waitForPrimaryStartup:\n          enabled: false\n    databaseUpgradeReady: true\n    database:\n      driver: org.postgresql.Driver\n      password: jfrog\n      type: postgresql\n      url: jdbc:postgresql://postgresql:5432/jfrog\n      user: jfrog\n    initContainerImage: &gt;-\n      registry.connect.redhat.com/jfrog/init@sha256:a93d4e5afe363edacf3928c510b1a4d82344df71203ae3fc950144d8b0098f5e\n    nginx:\n      uid: ${NAMESPACE_UID}\n      gid: ${NAMESPACE_GID}\n      http:\n        externalPort: 80\n        internalPort: 8080\n      https:\n        externalPort: 443\n        internalPort: 8443\n      image:\n        registry: ${LOCAL_REGISTRY}\n        repository: rhel8/nginx-116\n        tag: latest\n      service:\n        ssloffload: false\n      tlsSecretName: jfrog-cert\n    postgresql:\n      enabled: false\n    waitForDatabase: true\nEOF\n</code></pre>","tags":["air-gapped","restricted-network","disconnected"]},{"location":"deploy/workload/jfrog-artifactory-enterprise-operator-air-gapped/#expose-jfrog-via-an-route","title":"Expose JFrog via an route","text":"<pre><code>oc expose svc/openshiftartifactoryha-nginx\n</code></pre> <p>Get the URL: ```bash $ oc get routes NAME                           HOST/PORT                                                               PATH   SERVICES                       PORT   TERMINATION   WILDCARD openshiftartifactoryha-nginx   openshiftartifactoryha-nginx-demo.apps.air-gapped.azure.openshift.pub          openshiftartifactoryha-nginx   http                 None bash</p>","tags":["air-gapped","restricted-network","disconnected"]},{"location":"deploy/workload/own-apache-container/","title":"Own apache","text":""},{"location":"deploy/workload/own-apache-container/#build-images","title":"Build Images","text":"<pre><code>oc new-build https://github.com/openshift-examples/own-apache-container.git \\\n--name httpd\n\noc new-build https://github.com/openshift-examples/own-apache-container.git \\\n--context-dir=anyuid --name httpd-anyuid\n</code></pre>"},{"location":"deploy/workload/own-apache-container/#deploy-images","title":"Deploy Images","text":"<pre><code>oc new-app httpd\noc expose svc/httpd\n\noc new-app httpd-anyuid\noc expose svc/httpd-anyuid\n# FAIL, because by default it is not allowed to run POD's with uid 0\n\n# Create service account\noc create sa anyuid\n\n# Add privileges to service account to run POD's with uid 0\noc adm policy add-scc-to-user -z anyuid anyuid\n\noc patch dc/httpd-anyuid --patch '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\": \"anyuid\", \"serviceAccountName\": \"anyuid\"}}}}'\n</code></pre>"},{"location":"deploy/workload/own-apache-container/#add-persistent-volume","title":"Add persistent volume","text":"<pre><code>cat &lt;&lt;EOF | oc apply -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n    annotations:\n        volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/glusterfs\n    name: httpd\nspec:\n    accessModes:\n    - ReadWriteMany\n    resources:\n        requests:\n            storage: 1Gi\n    storageClassName: glusterfs-ocs\nEOF\n</code></pre> <pre><code>oc get pvc --watch\n\noc set volume dc/httpd --add --name=httpd-volume-1 -t pvc --claim-name=httpd --overwrite  --mount-path=/var/www/html/\n\noc set volume dc/httpd-anyuid --add --name=httpd-anyuid-volume-1 -t pvc --claim-name=httpd --overwrite  --mount-path=/var/www/html/\n</code></pre>"},{"location":"deploy/workload/own-apache-container/#rollback","title":"Rollback","text":"<pre><code>oc rollback dc/httpd-anyuid\n</code></pre>"},{"location":"deploy/workload/quake3/","title":"Run Quake3 on OpenShift","text":""},{"location":"deploy/workload/quake3/#deployment","title":"Deployment","text":"<p>Based on https://github.com/criticalstack/quake-kube</p> <pre><code>oc new-project quake\noc apply -f https://raw.githubusercontent.com/criticalstack/quake-kube/master/example.yaml\noc expose --name web svc/quake\noc create route edge web \\\n  --insecure-policy='Redirect' \\\n  --service='quake'\n</code></pre> <p></p>"},{"location":"deploy/workload/quake3/#my-configuration","title":"My configuration","text":"<ul> <li>The Q3A Guide - Basic Console Commands and Cheats</li> </ul> <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: quake3-server-config\ndata:\n  config.yaml: |\n    fragLimit: 25\n    timeLimit: 15m\n    bot:\n      minPlayers: 3\n    game:\n      motd: \"Welcome to Quake 3 on OpenShift\"\n      type: FreeForAll\n      forceRespawn: false\n      inactivity: 10m\n      quadFactor: 3\n      weaponRespawn: 3\n    server:\n      hostname: \"quakekube\"\n      maxClients: 12\n      password: \"$(pwgen -n 16 1)\"\n    commands:\n      - addbot sarge 2\n    maps:\n    - name: q3dm17\n      type: FreeForAll\n      timeLimit: 10m\nEOF\n\noc delete pods -l run=quake\n</code></pre>"},{"location":"deploy/workload/quake3/#get-the-rcon-password","title":"Get the rcon password:","text":"<pre><code>oc get cm/quake3-server-config -oyaml | grep 'password: \"'\n</code></pre>"},{"location":"deploy/workload/quake3/#gametype","title":"Gametype","text":"<pre><code>/g_gametype\nWill display or set the current game type:\n\n\"0\" - free-for-all DM\n\"1\" - Tournament 1-on-1\n\"2\" - Single-Player\n\"3\" - Team Deathmatch\n\"4\" - Capture the Flag.\n\nExample: /g_gametype \"3\"\n</code></pre>"},{"location":"gitops/","title":"GitOps","text":"","tags":["gitops","argocd"]},{"location":"gitops/#argocd-comand-line","title":"ArgoCD comand line","text":"<pre><code>argocd login --username admin \\\n  --password $(oc get secrets -n openshift-gitops openshift-gitops-cluster -o jsonpath=\"{.data.admin\\.password}\" | base64 -d) \\\n  --insecure \\\n  $(oc get route -n openshift-gitops openshift-gitops-server -o jsonpath=\"{.spec.host}\")\n</code></pre>","tags":["gitops","argocd"]},{"location":"gitops/#sealed-secrets","title":"Sealed secrets","text":"<p>Installed via OLM: https://github.com/openshift-examples/apps/tree/main/cluster-scope/components/sealed-secrets-operator</p> <p>Important, a rolebinding to get all admins access to sealedsecrets object:</p> <pre><code>---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: sealedsecrets-admin\n  labels:\n    rbac.authorization.k8s.io/aggregate-to-admin: \"true\"\nrules:\n  - apiGroups:\n      - \"bitnami.com\"\n    resources:\n      - SealedSecret\n    verbs:\n      - \"*\"\n</code></pre> <pre><code>kubeseal  \\\n  --controller-name sealed-secret-controller-sealed-secrets \\\n  --controller-namespace sealed-secrets \\\n  --fetch-cert\n\nkubeseal \\\n  --controller-name sealed-secret-controller-sealed-secrets \\\n  --controller-namespace sealed-secrets \\\n  --format yaml \\\n  &lt; &lt;(oc create secret generic test --from-literal=key1=supersecret --dry-run=client -o yaml)\n</code></pre>","tags":["gitops","argocd"]},{"location":"gitops/#ksop","title":"KSOP","text":"<p>https://blog.oddbit.com/post/2021-03-09-getting-started-with-ksops/</p> <pre><code># Kustomize\ncurl -L https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize%2Fv4.2.0/kustomize_v4.2.0_linux_amd64.tar.gz | tar xz\ninstall -m 755\nsudo install -m 755 kustomize /usr/local/bin/kustomize\nrm kustomize\n\n# SOPS\ncurl -L -o sops https://github.com/mozilla/sops/releases/download/v3.7.1/sops-v3.7.1.linux\nsudo install -m 755 sops /usr/local/bin/sops\nrm sops\n\n# KSOPS Plugin\ncurl -L https://github.com/viaduct-ai/kustomize-sops/releases/download/v2.5.7/ksops_2.5.7_Linux_x86_64.tar.gz | tar xz ksops\nmkdir  -p ~/.config/kustomize/plugin/viaduct.ai/v1/ksops/\ninstall -m 755 ksops ~/.config/kustomize/plugin/viaduct.ai/v1/ksops/ksops\nrm ksops\n</code></pre>","tags":["gitops","argocd"]},{"location":"gitops/#resources-examples","title":"Resources &amp; Examples","text":"<ul> <li>https://github.com/openshift-examples/apps</li> <li>https://github.com/gnunn-gitops/standards =&gt; Text-Beschreibung</li> <li>https://github.com/gnunn-gitops/cluster-config =&gt; Example f\u00fcr Cluster Config (von Gerald)</li> <li>https://github.com/PixelJonas/cluster-gitops =&gt; Example Cluster Config (von nem anderen dude)</li> <li>https://github.com/redhat-canada-gitops/catalog =&gt; Repo mit kustomize base-folder f\u00fcr verschiedene Operator und/oder KOnfigurationenexi</li> <li>https://argoproj.github.io/argo-cd/faq/#why-is-my-app-out-of-sync-even-after-syncing</li> </ul>","tags":["gitops","argocd"]},{"location":"kubevirt/","title":"OpenShift Virtualization (CNV/KubeVirt)","text":"<p>OpenShift Virtualization is a fully developed virtualization solution utilizing the type-1 KVM hypervisor from Red Hat Enterprise Linux with the powerful features and capabilities of OpenShift for managing hypervisor nodes, virtual machines, and their consumers.</p>","tags":["cnv","kubevirt","ocp-v"]},{"location":"kubevirt/#usefull-resources","title":"Usefull resources:","text":"<ul> <li>OpenShift Virtualization cookbook</li> </ul>","tags":["cnv","kubevirt","ocp-v"]},{"location":"kubevirt/#example-deployments","title":"Example deployments","text":"Tiny RHEL 9 VM with pod bridge network tiny-rhel-pod-bridge.yamloc apply -f .... <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  annotations:\n  labels:\n    app: rhel9-pod-bridge\n    kubevirt.io/dynamic-credentials-support: \"true\"\n  name: rhel9-pod-bridge\nspec:\n  dataVolumeTemplates:\n    - apiVersion: cdi.kubevirt.io/v1beta1\n      kind: DataVolume\n      metadata:\n        name: rhel9-pod-bridge\n      spec:\n        sourceRef:\n          kind: DataSource\n          name: rhel9\n          namespace: openshift-virtualization-os-images\n        storage:\n          accessModes:\n            - ReadWriteMany\n          storageClassName: ocs-storagecluster-ceph-rbd-virtualization\n          resources:\n            requests:\n              storage: 30Gi\n  running: false\n  template:\n    metadata:\n      annotations:\n        vm.kubevirt.io/flavor: tiny\n        vm.kubevirt.io/os: rhel9\n        vm.kubevirt.io/workload: server\n        kubevirt.io/allow-pod-bridge-network-live-migration: \"\"\n      labels:\n        kubevirt.io/domain: rhel9-pod-bridge\n        kubevirt.io/size: tiny\n    spec:\n      domain:\n        cpu:\n          cores: 1\n          sockets: 1\n          threads: 1\n        devices:\n          disks:\n            - disk:\n                bus: virtio\n              name: rootdisk\n            - disk:\n                bus: virtio\n              name: cloudinitdisk\n          interfaces:\n            - bridge: {}\n              name: default\n        machine:\n          type: pc-q35-rhel9.2.0\n        memory:\n          guest: 1.5Gi\n      networks:\n        - name: default\n          pod: {}\n      terminationGracePeriodSeconds: 180\n      volumes:\n        - dataVolume:\n            name: rhel9-pod-bridge\n          name: rootdisk\n        - cloudInitNoCloud:\n            userData: |-\n              #cloud-config\n              user: cloud-user\n              password: redhat\n              chpasswd: { expire: False }\n          name: cloudinitdisk\n</code></pre> <pre><code>oc apply -f https://examples.openshift.pub/pr-133/kubevirt/example/tiny-rhel-pod-bridge.yaml\n</code></pre> Red Hat CoreOS with ignition &amp; pod bridge network rhcos-pod-bridge.yamloc apply -f .... <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  annotations:\n  labels:\n    app: rhcos-pod-bridge\n    kubevirt.io/dynamic-credentials-support: \"true\"\n  name: rhcos-pod-bridge\nspec:\n  dataVolumeTemplates:\n    - apiVersion: cdi.kubevirt.io/v1beta1\n      kind: DataVolume\n      metadata:\n        name: rhcos-pod-bridge\n      spec:\n        source:\n          registry:\n            pullMethod: node\n            # openshift-install coreos print-stream-json | jq '.architectures.x86_64.images.kubevirt'\n            url: docker://quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ab118238b01765f103fe0739c0cd48ba10e745f25d5d1da202faf8c08b57fb58\n        storage:\n          accessModes:\n            - ReadWriteMany\n          storageClassName: ocs-storagecluster-ceph-rbd-virtualization\n          resources:\n            requests:\n              storage: 30Gi\n  running: false\n  template:\n    metadata:\n      annotations:\n        vm.kubevirt.io/flavor: tiny\n        vm.kubevirt.io/os: rhcos\n        vm.kubevirt.io/workload: server\n        kubevirt.io/allow-pod-bridge-network-live-migration: \"\"\n      labels:\n        kubevirt.io/domain: rhcos-pod-bridge\n        kubevirt.io/size: large\n    spec:\n      domain:\n        cpu:\n          cores: 1\n          sockets: 2\n          threads: 1\n        devices:\n          disks:\n            - disk:\n                bus: virtio\n              name: rootdisk\n            - disk:\n                bus: virtio\n              name: cloudinitdisk\n          interfaces:\n            - bridge: {}\n              name: default\n        machine:\n          type: pc-q35-rhel9.2.0\n        memory:\n          guest: 8Gi\n      networks:\n        - name: default\n          pod: {}\n      terminationGracePeriodSeconds: 180\n      volumes:\n        - dataVolume:\n            name: rhcos-pod-bridge\n          name: rootdisk\n        - cloudInitConfigDrive:\n            # Password hash\n            #   podman run -ti --rm quay.io/coreos/mkpasswd --method=yescrypt\n            # Password: redhat\n            userData: |-\n              {\n                \"ignition\": {\n                  \"version\": \"3.2.0\"\n                },\n                \"passwd\": {\n                  \"users\": [\n                    {\n                      \"name\": \"core\",\n                      \"passwordHash\": \"$y$j9T$15cuONdoH5AKB62c9qTtD.$oOf4GqrwEnNzT7WuEFvkDuSOyv2xIx/z4EXzbQivdO0\",\n                      \"sshAuthorizedKeys\": [\n                        \"ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIAOfl+764UFbDkkxpsQYjET7ZAWoVApSf4I64L1KImoc rbohne@redhat.com\"\n                      ]\n                    }\n                  ]\n                }\n              }\n          name: cloudinitdisk\n</code></pre> <pre><code>oc apply -f https://examples.openshift.pub/pr-133/kubevirt/example/rhcos-pod-bridge.yaml\n</code></pre> Boot from ISO boot-from-iso.yamloc apply -f .... <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  annotations:\n  labels:\n    app: beryllium\n  name: beryllium\n  namespace: demo-cluster-disco\nspec:\n  dataVolumeTemplates:\n    - metadata:\n        name: beryllium-root\n      spec:\n        storage:\n          accessModes:\n            - ReadWriteMany\n          storageClassName: coe-netapp-nas\n          resources:\n            requests:\n              storage: 80Gi\n        source:\n          blank: {}\n  running: true\n  template:\n    metadata:\n      labels:\n        kubevirt.io/domain: beryllium\n    spec:\n      volumes:\n        - name: cdrom\n          persistentVolumeClaim:\n            claimName: beryllium-1-i386-hybrid\n        - name: root\n          dataVolume:\n            name: beryllium-root\n      networks:\n        - name: coe\n          multus:\n            networkName: coe-bridge\n        - name: disco\n          multus:\n            networkName: coe-br-vlan-69\n      domain:\n        cpu:\n          cores: 4\n        memory:\n          guest: 8Gi\n        resources:\n          requests:\n            memory: 8Gi\n        devices:\n          disks:\n            - name: root\n              bootOrder: 1\n              disk:\n                bus: virtio\n            - name: cdrom\n              bootOrder: 2\n              cdrom:\n                bus: sata\n          interfaces:\n            - bridge: {}\n              # macAddress: 02:d8:6d:00:00:12\n              model: virtio\n              name: coe\n            - bridge: {}\n              # macAddress: 02:d8:6d:00:00:13\n              model: virtio\n              name: disco\n</code></pre> <pre><code>oc apply -f https://examples.openshift.pub/pr-133/kubevirt/example/boot-from-iso.yaml\n</code></pre> Example Fedora with httpd cloud-init and network localnet-fedora-vm.yamloc apply -f .... <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: fedora\nspec:\n  runStrategy: Always\n  template:\n    spec:\n      domain:\n        devices:\n          disks:\n            - disk:\n                bus: virtio\n              name: containerdisk\n            - disk:\n                bus: virtio\n              name: cloudinit\n          rng: {}\n          interfaces:\n            - bridge: {}\n              model: virtio\n              name: coe\n        features:\n          acpi: {}\n          smm:\n            enabled: true\n        firmware:\n          bootloader:\n            efi:\n              secureBoot: true\n        resources:\n          requests:\n            memory: 1Gi\n      terminationGracePeriodSeconds: 180\n      networks:\n        - multus:\n            networkName: coe\n          name: coe\n      volumes:\n        - name: containerdisk\n          containerDisk:\n            image: quay.io/containerdisks/fedora:41\n        - name: cloudinit\n          cloudInitNoCloud:\n            networkData: |\n              version: 2\n              ethernets:\n                eth0:\n                  dhcp4: true\n            userData: |-\n              #cloud-config\n\n              users:\n                - name: coe\n                  lock_passwd: false\n                  # redhat // mkpasswd --method=SHA-512 --rounds=4096\n                  hashed_passwd: \"$6$rounds=4096$kmUERoUZHwzYfQMJ$G70T2Qg24d0XUhu.GTCH7Ia1F0B/B48JqIFdzVfigeMgfG5nsxp3dEWFKokfXGmhuetFXl4l41L8t1AZgEDW0.\"\n                  sudo: ['ALL=(ALL) NOPASSWD:ALL']\n                  chpasswd: { expire: False }\n                  groups: wheel\n                  shell: /bin/bash\n                  ssh_authorized_keys:\n                    - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIEQM82o2imwpHyGVO7DxCNbdE0ZWnkp6oxdawb7/MOCT coe-muc\n\n              packages:\n                - httpd\n\n              # install puppet (and dependencies); make sure apache and postgres\n              # both start at boot-time\n              runcmd:\n                - [ systemctl, enable, httpd.service ]\n                - [ systemctl, start, httpd.service ]\n</code></pre> <pre><code>oc apply -f https://examples.openshift.pub/pr-133/kubevirt/networking/localnet-fedora-vm.yaml\n</code></pre>","tags":["cnv","kubevirt","ocp-v"]},{"location":"kubevirt/#useful-commands","title":"Useful Commands","text":"","tags":["cnv","kubevirt","ocp-v"]},{"location":"kubevirt/#configure-a-new-number-of-cpus-for-a-vm","title":"Configure a new number of CPUs for a VM","text":"<pre><code># Set the number of CPUs which you'd like to configure\nexport NEW_CPU=8\n\n# This loop will configure the new number of CPUs\nfor VM in $(kubectl get vm -o jsonpath='{.items[*].metadata.name}'); do\n    echo \"Updating compute resources for VM: $VM\"\n    kubectl patch vm \"$VM\" --type='json' -p=\"[{'op': 'replace', 'path': '/spec/template/spec/domain/cpu/sockets', 'value': $NEW_CPU}]\"\ndone\n</code></pre>","tags":["cnv","kubevirt","ocp-v"]},{"location":"kubevirt/#add-the-ocp-descheduler-annotation-to-true-or-false","title":"Add the OCP Descheduler Annotation to True or False","text":"<pre><code># Set the Descheduler Annotation to True or False\nexport DESCHEDULER=True\n\n# Add the OCP Descheduler Annotation to True or False\nfor VM in $(kubectl get vm -o jsonpath='{.items[*].metadata.name}'); do\n    echo \"Updating descheduler annotation: $VM\"\n    kubectl patch vm \"$VM\" --type='json' -p=\"[{'op': 'add', 'path': '/spec/template/metadata/annotations/descheduler.alpha.kubernetes.io~1evict', 'value': '$DESCHEDULER'}]\"\ndone\n</code></pre>","tags":["cnv","kubevirt","ocp-v"]},{"location":"kubevirt/#create-multiple-vms-loop","title":"Create multiple VMs loop","text":"<pre><code>for i in $(seq 1 15);  do oc process -n openshift rhel9-server-medium  -p NAME=vm${i} | oc apply -f - ; done;\n</code></pre>","tags":["cnv","kubevirt","ocp-v"]},{"location":"kubevirt/#containerized-data-importer-cdi-datavolume","title":"Containerized Data Importer (CDI) / DataVolume","text":"<pre><code>apiVersion: cdi.kubevirt.io/v1alpha1\nkind: DataVolume\nmetadata:\n  name: registry-image-datavolume\nspec:\n  pvc:\n    accessModes:\n    - ReadWriteMany\n    resources:\n      requests:\n        storage: 5Gi\n  source:\n    registry:\n      url: docker://image-registry.openshift-image-registry.svc:5000/cnv-demo/build-vm-image-container:latest\n      certConfigMap: \"tls-certs\"\n</code></pre> <p>Source: cdi-examples</p>","tags":["cnv","kubevirt","ocp-v"]},{"location":"kubevirt/#openshift-virtualization-container-storage","title":"OpenShift Virtualization &amp; Container Storage","text":"<p>Recommended storage settings:</p> <pre><code>$ oc edit cm kubevirt-storage-class-defaults -n openshift-cnv\n\naccessMode: ReadWriteMany\nocs-storagecluster-ceph-rbd.accessMode: ReadWriteMany\nocs-storagecluster-ceph-rbd.volumeMode: Block\nocs-storagecluster-cephfs.accessMode: ReadWriteMany\nocs-storagecluster-cephfs.volumeMode: Filesystem\nvolumeMode: Block\n</code></pre>","tags":["cnv","kubevirt","ocp-v"]},{"location":"kubevirt/#build-container-image-with-os-disk","title":"Build container image with OS disk","text":"<pre><code>oc new-build --name cirros \\\n    --build-arg image_url=http://download.cirros-cloud.net/0.5.1/cirros-0.5.1-x86_64-disk.img \\\n    https://github.com/openshift-examples/cnv-container-disk-build.git\n</code></pre>","tags":["cnv","kubevirt","ocp-v"]},{"location":"kubevirt/#local-iis-build-in-my-lab","title":"Local IIS build in my lab","text":"<pre><code>qemu-img convert -f raw -O qcow2 disk.img iis.qcow2\n\ncat - &gt; Dockerfile &lt;&lt;EOF\nFROM scratch\nLABEL maintainer=\"Robert Bohne &lt;robert.bohne@redhat.com&gt;\"\nADD iis.qcow2 /disk/rhel.qcow2\nEOF\n\noc create is iis -n cnv\n\nexport REGISTRY=$(oc get route default-route -n openshift-image-registry -o jsonpath='{.spec.host}')\nexport REGISTRY_TOKEN=$(oc whoami -t)\npodman login -u $(oc whoami) -p $REGISTRY_TOKEN --tls-verify=false $HOST\n\npodman build -t ${REGISTRY}/cnv/iis:latest .\npodman push ${REGISTRY}/cnv/iis:latest\n\n# Deploy template\noc apply -f https://raw.githubusercontent.com/openshift-examples/web/master/content/kubevirt/iis-template.yaml\n</code></pre>","tags":["cnv","kubevirt","ocp-v"]},{"location":"kubevirt/#resource-capacity-calculation","title":"Resource Capacity Calculation","text":"<p>At a high level, the process is to determine the amount of virtualization resources needed (VM sizes, overhead, burst capacity, failover capacity), add that to the amount of resources needed for cluster services (logging, metrics, ODF/ACM/ACS if hosted in the same cluster, etc.) and customer workload (hosted control planes, other Pods deployed to the hardware, etc.), then find a balance of node size vs node count.</p> <ul> <li>Source: Red Hat Architecting OpenShift Virtualization</li> </ul>","tags":["cnv","kubevirt","ocp-v"]},{"location":"kubevirt/#cpu-capacity-calculation","title":"CPU capacity calculation","text":"<pre><code>Formula\n\n(((physical_cpu_cores - odf_requirements - control_plane_requirements) * node_count * overcommitment_ratio) * (1 -ha_reserve_percent)) * (1 - spare_capacity_percent)\n</code></pre> <ul> <li><code>physical_cpu_cores</code> = the number of physical cores available on the node.</li> <li><code>odf_requirements</code> = the amount of resources reserved for ODF. A value of 32 cores was used for the example architectures.</li> <li><code>control_plane_requirements</code> = the amount of CPU reserved for the control plane workload. A value of 4 cores was used for the example architectures.</li> <li><code>node_count</code> = the number of nodes with this geometry. For small, all nodes were equal. For medium, the nodes are mixed-purpose, so the previous steps would need to be repeated for each node type, taking into account the appropriate node type.</li> <li><code>overcommitment_ratio</code> = the amount of CPU overcommitment. A ratio of 4:1 was used for this document.</li> <li><code>spare_capacity</code> = the amount of capacity reserved for spare/burst. A value of 10% was used for this document.</li> <li><code>ha_reserve_percent</code> = the amount of capacity reserved for recovering workload lost in the event of node failure. For the small example, a value of 25% was used, allowing for one node to fail. For the medium example, a value of 20% was used, allowing for two nodes to fail.</li> </ul>","tags":["cnv","kubevirt","ocp-v"]},{"location":"kubevirt/#memory-capacity-calculation","title":"Memory capacity calculation","text":"<pre><code>Formula\n\n((total_node_memory - odf_requirements - control_plane_requirements) * soft_eviction_threshold_percent * node_count) * (1 - ha_reserve_percent)\n</code></pre> <ul> <li><code>total_node_memory</code> = the total physical memory installed on the node.</li> <li><code>odf_requirements</code> = the amount of memory assigned to ODF. A value of 72GiB was used for the example architectures in this document.</li> <li><code>control_plane_requirements</code> = the amount of memory reserved for the control plane functions. A value of 24GiB was used for the example architectures.</li> <li><code>soft_eviction_threshold_percent</code> = the value at which soft eviction is triggered to rebalance resource utilization on the node. Unless all nodes in the cluster exceed this value, it\u2019s expected that the node will be below this utilization. A value of 90% was used for this document.</li> <li><code>node_count</code> = the number of nodes with this geometry. For small, all nodes were equal. For medium, the nodes are mixed-purpose, so the previous steps would need to be repeated for each node type, taking into account the appropriate node type.</li> <li><code>ha_reserve_percent</code> = the amount of capacity reserved for recovering workload lost in the event of node failure. For the small example, a value of 25% was used, allowing for one node to fail. For the medium example, a value of 20% was used, allowing for two nodes to fail.</li> </ul>","tags":["cnv","kubevirt","ocp-v"]},{"location":"kubevirt/#odf-capacity-calculation","title":"ODF capacity calculation","text":"<pre><code>Formula\n\n(((disk_size * disk_count) * node_count) / replica_count) * (1 - utilization_percent)\n</code></pre> <ul> <li><code>disk_size</code> = the size of the disk(s) used. 4TB and 8TB disks were used in the example architectures.</li> <li><code>disk_count</code> = the number of disks of disk_size in the node.</li> <li><code>node_count</code> = the number of nodes with this geometry. For small, all nodes were equal. For medium, the nodes are mixed-purpose, so the previous steps would need to be repeated for each node type taking into account the appropriate node type.</li> <li><code>replica_count</code> = the number of copies ODF stores of the data for protection/resiliency. A value of 3 was used for this document.</li> <li><code>utilization_percent</code> = the desired threshold of capacity used in the ODF instance. A value of 65% was used for this document.</li> </ul>","tags":["cnv","kubevirt","ocp-v"]},{"location":"kubevirt/#resources-and-useful-articles","title":"Resources and useful articles","text":"<ul> <li>Deploying Vms On Kubernetes Glusterfs Kubevirt</li> <li>Kubevirt Network Deep Dive</li> <li>Upstream containerized-data-importer</li> <li>Deploy openSUSE Leap15 VM in Kubernetes using KubeVirt</li> <li>Kubernetes and Virtualization: kubevirt will let you spawn virtual machine on your cluster!</li> <li>Very old, please double check: Know Issue: No IP address in VM after pod deletion #1646</li> <li>https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking/</li> <li>https://www.praqma.com/stories/debugging-kubernetes-networking/</li> </ul>","tags":["cnv","kubevirt","ocp-v"]},{"location":"kubevirt/nfs-csi-driver/","title":"CSI Driver NFS","text":"<p>Based on</p> <ul> <li>https://github.com/kubernetes-csi/csi-driver-nfs</li> <li>https://hackmd.io/@johnsimcall/BJeW2Y5mT</li> <li>https://hackmd.io/@kincl/csi-driver-nfs-with-console</li> </ul> Snapshots are very slow! <p>Snapshots are coping data via tar .. $source $targert and that is incredible slow. OpenShift Virtualization runs in timeout, for example, during VM cloning via WebUI. Possible solution, create an VM snapshot with an extralong timeout:</p> <pre><code>apiVersion: snapshot.kubevirt.io/v1beta1\nkind: VirtualMachineSnapshot\nmetadata:\n  name: snapshot-with-60-minute-timeout\nspec:\n  failureDeadline: 1h0m0s\n  source:\n    apiGroup: kubevirt.io\n    kind: VirtualMachine\n    name: rhel9-violet-halibut-12\n</code></pre>"},{"location":"kubevirt/nfs-csi-driver/#deployment-via-helm","title":"Deployment via helm","text":""},{"location":"kubevirt/nfs-csi-driver/#prepare-namespace","title":"Prepare namespace","text":"<pre><code>export NAMESPACE=openshift-csi-driver-nfs\noc create namespace ${NAMESPACE}\noc adm policy add-scc-to-user -n  ${NAMESPACE}  privileged -z csi-nfs-controller-sa\noc adm policy add-scc-to-user -n  ${NAMESPACE}  privileged -z csi-nfs-node-sa\n</code></pre>"},{"location":"kubevirt/nfs-csi-driver/#deploy","title":"Deploy","text":"<p>Download <code>values.yaml</code> and adjust the NFS-Server and Path in the last lines.</p> helmvalues.yaml <pre><code>curl -L -O  {{ page.canonical_url }}values.yaml\n</code></pre> <pre><code>---\ncontroller:\n  resources:\n    csiProvisioner:\n      limits:\n        memory: 1024Mi\n    csiSnapshotter:\n      limits:\n        memory: 1024Mi\n    livenessProbe:\n      limits:\n        memory: 1024Mi\n    nfs:\n      limits:\n        memory: 1024Mi\n\nexternalSnapshotter:\n  enabled: true\n  customResourceDefinitions:\n    enabled: false\n\n## StorageClass resource example:\nstorageClass:\n  create: true\n  name: nfs-csi\n  annotations:\n    storageclass.kubevirt.io/is-default-virt-class: \"true\"\n    storageclass.kubernetes.io/is-default-class: \"true\"\n  parameters:\n    server: 10.32.97.1\n    share: /coe_stormshift_ocp1\n    subDir: ${pvc.metadata.namespace}-${pvc.metadata.name}-${pv.metadata.name}\n</code></pre> <pre><code># Do use openshift/csi-driver-nfs because csi resizer is missing\nhelm repo add csi-driver-nfs https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/charts\n\n#  v0.0.0, because we want latest with csi-resizer\n\nhelm install csi-driver-nfs \\\n  csi-driver-nfs/csi-driver-nfs \\\n  --namespace openshift-csi-driver-nfs \\\n  --version v0.0.0 \\\n  --values values.yaml\n</code></pre>"},{"location":"kubevirt/nfs-csi-driver/#create-volumesnapshotclass","title":"Create VolumeSnapshotClass","text":"<p>It's missing in the HelmChart: https://github.com/kubernetes-csi/csi-driver-nfs/issues/825</p> <pre><code>oc apply -f &lt;&lt;EOF\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotClass\nmetadata:\n  name: nfs-csi-snapclass\ndriver: nfs.csi.k8s.io\ndeletionPolicy: Delete\nEOF\n</code></pre>"},{"location":"kubevirt/node-health-check/","title":"Node Health Check","text":"","tags":["kubevirt","ocp-v","cnv"]},{"location":"kubevirt/node-health-check/#resources","title":"Resources","text":"<ul> <li>OpenShift Virtualization - Fencing and VM High Availability Guide</li> </ul>","tags":["kubevirt","ocp-v","cnv"]},{"location":"kubevirt/node-health-check/#installation-configuration","title":"Installation &amp; configuration","text":"<ul> <li>Install Operator \"Node Health Check Operator\"</li> <li>Install Operator \"Self Node Remediation Operator\"</li> </ul>","tags":["kubevirt","ocp-v","cnv"]},{"location":"kubevirt/node-health-check/#start-operator-for-worker-nodes","title":"Start operator for worker nodes","text":"<pre><code>apiVersion: remediation.medik8s.io/v1alpha1\nkind: NodeHealthCheck\nmetadata:\n  name: worker-availability\nspec:\n  minHealthy: 51%\n  remediationTemplate:\n    apiVersion: self-node-remediation.medik8s.io/v1alpha1\n    kind: SelfNodeRemediationTemplate\n    name: self-node-remediation-automatic-strategy-template\n    namespace: openshift-workload-availability\n  selector:\n    matchExpressions:\n      - key: node-role.kubernetes.io/worker\n        operator: Exists\n        values: []\n  unhealthyConditions:\n    - duration: 1s # (1)!\n      status: 'False'\n      type: Ready\n    - duration: 1s # (2)!\n      status: Unknown\n      type: Ready\n</code></pre> <ol> <li>Change the seconds to achieve the fasted VM recovery, according to OpenShift Virtualization - Fencing and VM High Availability Guide</li> <li>Change the seconds to achieve the fasted VM recovery, according to OpenShift Virtualization - Fencing and VM High Availability Guide</li> </ol>","tags":["kubevirt","ocp-v","cnv"]},{"location":"kubevirt/node-health-check/#update-self-node-remediation-automatic-strategy-template","title":"Update <code>self-node-remediation-automatic-strategy-template</code>","text":"<pre><code>apiVersion: self-node-remediation.medik8s.io/v1alpha1\nkind: SelfNodeRemediationTemplate\nmetadata:\n  annotations:\n    remediation.medik8s.io/multiple-templates-support: \"true\"\n  labels:\n    remediation.medik8s.io/default-template: \"true\"\n  name: self-node-remediation-automatic-strategy-template\n  namespace: openshift-workload-availability\nspec:\n  template:\n    spec:\n      remediationStrategy: OutOfServiceTaint # (1)!\n</code></pre> <ol> <li> <p>Default is \"Automatic\", but I want a predictable behavor. Offical documentation</p> <pre><code>$ oc explain SelfNodeRemediationTemplate.spec.template.spec.remediationStrategy\nGROUP:      self-node-remediation.medik8s.io\nKIND:       SelfNodeRemediationTemplate\nVERSION:    v1alpha1\n\nFIELD: remediationStrategy &lt;string&gt;\n\nDESCRIPTION:\n    RemediationStrategy is the remediation method for unhealthy nodes.\n    Currently, it could be either \"Automatic\", \"OutOfServiceTaint\" or\n    \"ResourceDeletion\".\n    ResourceDeletion will iterate over all pods and VolumeAttachment related to\n    the unhealthy node and delete them.\n    OutOfServiceTaint will add the out-of-service taint which is a new\n    well-known taint \"node.kubernetes.io/out-of-service\"\n    that enables automatic deletion of pv-attached pods on failed nodes,\n    \"out-of-service\" taint is only supported on clusters with k8s version 1.26+\n    or OCP/OKD version 4.13+.\n    Automatic will choose the most appropriate strategy during runtime.\n</code></pre> </li> </ol>","tags":["kubevirt","ocp-v","cnv"]},{"location":"kubevirt/node-health-check/#example","title":"Example","text":"<ul> <li>Start a RHEL VM with network access</li> <li>Provide <code>~/bin/l</code></li> </ul> <pre><code>#!/usr/bin/env bash\n\n(echo \"# $@\"; exec \"$@\") |  ts '[%Y-%m-%d %H:%M:%S]'  | tee -a /tmp/app.log\n</code></pre> <ul> <li>Run: <code>l ping $VM_IP</code></li> <li>Run: <code>oc get pods -o wide --watch | ts '[%Y-%m-%d %H:%M:%S]' | tee -a /tmp/app.log</code></li> <li>Watch the log <code>tail -f /tmp/app.log</code></li> <li>Stop the VM where the node is running: <code>l virtctl stop --force --grace-period=0  ocp1-worker-0</code></li> </ul>","tags":["kubevirt","ocp-v","cnv"]},{"location":"kubevirt/node-health-check/#applog","title":"app.log","text":"<pre><code>[2025-06-16 19:00:01] # oc get vm,vmi\n[2025-06-16 19:00:01] NAME                              AGE   STATUS    READY\n[2025-06-16 19:00:01] virtualmachine.kubevirt.io/rhel   14m   Running   True\n[2025-06-16 19:00:01]\n[2025-06-16 19:00:01] NAME                                      AGE     PHASE     IP                              NODENAME        READY\n[2025-06-16 19:00:01] virtualmachineinstance.kubevirt.io/rhel   6m49s   Running   2620:52:0:2060:63:97ff:fe00:b   ocp1-worker-0   True\n[2025-06-16 19:00:13] # oc get vm,vmi\n[2025-06-16 19:00:14] NAME                                       AGE     STATUS    READY\n[2025-06-16 19:00:14] virtualmachine.kubevirt.io/ocp1-cp-0       3d19h   Running   True\n[2025-06-16 19:00:14] virtualmachine.kubevirt.io/ocp1-cp-1       3d19h   Running   True\n[2025-06-16 19:00:14] virtualmachine.kubevirt.io/ocp1-cp-2       3d19h   Running   True\n[2025-06-16 19:00:14] virtualmachine.kubevirt.io/ocp1-worker-0   3d19h   Running   True\n[2025-06-16 19:00:14] virtualmachine.kubevirt.io/ocp1-worker-1   3d19h   Running   True\n[2025-06-16 19:00:14] virtualmachine.kubevirt.io/ocp1-worker-2   3d19h   Running   True\n[2025-06-16 19:00:14]\n[2025-06-16 19:00:14] NAME                                               AGE     PHASE     IP             NODENAME   READY\n[2025-06-16 19:00:14] virtualmachineinstance.kubevirt.io/ocp1-cp-0       3d19h   Running   10.32.105.66   storm2     True\n[2025-06-16 19:00:14] virtualmachineinstance.kubevirt.io/ocp1-cp-1       3d9h    Running   10.32.105.67   ucs57      True\n[2025-06-16 19:00:14] virtualmachineinstance.kubevirt.io/ocp1-cp-2       3d19h   Running   10.32.105.68   storm6     True\n[2025-06-16 19:00:14] virtualmachineinstance.kubevirt.io/ocp1-worker-0   3d12h   Running   10.32.105.69   ucs55      True\n[2025-06-16 19:00:14] virtualmachineinstance.kubevirt.io/ocp1-worker-1   3d12h   Running   10.32.105.70   storm3     True\n[2025-06-16 19:00:14] virtualmachineinstance.kubevirt.io/ocp1-worker-2   3d12h   Running   10.32.105.71   ucs56      True\n[2025-06-16 19:01:34] # ping 10.32.111.147\n[2025-06-16 19:01:34] PING 10.32.111.147 (10.32.111.147): 56 data bytes\n[2025-06-16 19:01:34] 64 bytes from 10.32.111.147: icmp_seq=0 ttl=57 time=52.220 ms\n[..snipped..]\n[2025-06-16 19:01:39] # virtctl stop --force ocp1-worker-0\n[2025-06-16 19:01:40] 64 bytes from 10.32.111.147: icmp_seq=6 ttl=57 time=52.835 ms\n[..snipped..]\n[2025-06-16 19:02:02] 64 bytes from 10.32.111.147: icmp_seq=28 ttl=57 time=51.931 ms\n[2025-06-16 19:02:02] # virtctl stop --force --grace-period=0 ocp1-worker-0\n[2025-06-16 19:02:03] VM ocp1-worker-0 was scheduled to stop\n[2025-06-16 19:02:04] Request timeout for icmp_seq 29\n[..snipped..]\n[2025-06-16 19:02:57] Request timeout for icmp_seq 82\n[2025-06-16 19:02:58] virt-launcher-rhel-hc2ds   1/1     Running             0          9m46s   10.131.0.36   ocp1-worker-0   &lt;none&gt;           1/1\n[2025-06-16 19:02:58] Request timeout for icmp_seq 83\n[2025-06-16 19:02:59] Request timeout for icmp_seq 84\n[2025-06-16 19:02:59] virt-launcher-rhel-hc2ds   1/1     Terminating         0          9m47s   10.131.0.36   ocp1-worker-0   &lt;none&gt;           1/1\n[2025-06-16 19:03:00] Request timeout for icmp_seq 85\n[..snipped..]\n[2025-06-16 19:04:38] Request timeout for icmp_seq 183\n[2025-06-16 19:04:39] virt-launcher-rhel-hc2ds   1/1     Terminating         0          11m     10.131.0.36   ocp1-worker-0   &lt;none&gt;           1/1\n[2025-06-16 19:04:39] Request timeout for icmp_seq 184\n[..snipped..]\n[2025-06-16 19:06:42] Request timeout for icmp_seq 306\n[2025-06-16 19:06:42] virt-launcher-rhel-hc2ds   1/1     Failed              0          13m     10.131.0.36   ocp1-worker-0   &lt;none&gt;           1/1\n[2025-06-16 19:06:43] Request timeout for icmp_seq 307\n[2025-06-16 19:06:43] virt-launcher-rhel-hc2ds   1/1     Failed              0          13m     10.131.0.36   ocp1-worker-0   &lt;none&gt;           1/1\n[2025-06-16 19:06:43] virt-launcher-rhel-hc2ds   1/1     Failed              0          13m     10.131.0.36   ocp1-worker-0   &lt;none&gt;           1/1\n[2025-06-16 19:06:44] Request timeout for icmp_seq 308\n[..snipped..]\n[2025-06-16 19:07:30] Request timeout for icmp_seq 354\n[2025-06-16 19:07:31] virt-launcher-rhel-rhf6h   0/1     Pending             0          2s      &lt;none&gt;        &lt;none&gt;          &lt;none&gt;           0/1\n[2025-06-16 19:07:31] virt-launcher-rhel-rhf6h   0/1     Pending             0          2s      &lt;none&gt;        ocp1-worker-1   &lt;none&gt;           0/1\n[2025-06-16 19:07:31] virt-launcher-rhel-rhf6h   0/1     Pending             0          2s      &lt;none&gt;        ocp1-worker-1   &lt;none&gt;           0/1\n[2025-06-16 19:07:31] Request timeout for icmp_seq 355\n[2025-06-16 19:07:32] Request timeout for icmp_seq 356\n[2025-06-16 19:07:33] virt-launcher-rhel-rhf6h   0/1     Pending             0          4s      &lt;none&gt;        ocp1-worker-1   &lt;none&gt;           0/1\n[2025-06-16 19:07:33] Request timeout for icmp_seq 357\n[2025-06-16 19:07:34] virt-launcher-rhel-rhf6h   0/1     ContainerCreating   0          5s      &lt;none&gt;        ocp1-worker-1   &lt;none&gt;           0/1\n[2025-06-16 19:07:34] Request timeout for icmp_seq 358\n[2025-06-16 19:07:35] Request timeout for icmp_seq 359\n[2025-06-16 19:07:35] virt-launcher-rhel-rhf6h   0/1     ContainerCreating   0          6s      &lt;none&gt;        ocp1-worker-1   &lt;none&gt;           1/1\n[2025-06-16 19:07:36] Request timeout for icmp_seq 360\n[..snipped..]\n[2025-06-16 19:07:54] Request timeout for icmp_seq 378\n[2025-06-16 19:07:54] virt-launcher-rhel-rhf6h   0/1     ContainerCreating   0          25s     &lt;none&gt;        ocp1-worker-1   &lt;none&gt;           1/1\n[2025-06-16 19:07:55] virt-launcher-rhel-rhf6h   1/1     Running             0          26s     10.128.2.71   ocp1-worker-1   &lt;none&gt;           1/1\n[2025-06-16 19:07:55] virt-launcher-rhel-rhf6h   1/1     Running             0          26s     10.128.2.71   ocp1-worker-1   &lt;none&gt;           1/1\n[2025-06-16 19:07:55] Request timeout for icmp_seq 379\n[..snipped..]\n[2025-06-16 19:09:41] Request timeout for icmp_seq 484\n[2025-06-16 19:09:41] 64 bytes from 10.32.111.147: icmp_seq=485 ttl=57 time=51.578 ms\n[2025-06-16 19:09:42] 64 bytes from 10.32.111.147: icmp_seq=486 ttl=57 time=51.141 ms\n[2025-06-16 19:09:43] 64 bytes from 10.32.111.147: icmp_seq=487 ttl=57 time=51.136 ms\n[2025-06-16 19:09:44] 64 bytes from 10.32.111.147: icmp_seq=488 ttl=57 time=51.472 ms\n..\n</code></pre>","tags":["kubevirt","ocp-v","cnv"]},{"location":"kubevirt/pci-passthrough/","title":"PCI passthrough","text":"<ul> <li>Official documentation</li> <li>Upstream documentation</li> </ul>","tags":["cnv","kubevirt","v4.17"]},{"location":"kubevirt/pci-passthrough/#tested-with","title":"Tested with","text":"Component Version OpenShift v4.17.14 OpenShift Virt v4.17.4","tags":["cnv","kubevirt","v4.17"]},{"location":"kubevirt/pci-passthrough/#high-level-flow","title":"High-level flow","text":"<p>1) Enable iommu 2) Configure vfio-pci 3) Disable/don't allow orginal kernel driver feels responsible for the device 4) Configure KubeVirt / OpenShift Virt.</p> <p>PCI devices I want to forward:</p> <pre><code>sh-5.1# lspci -nnk -d '1137:0043'\n47:00.0 Ethernet controller [0200]: Cisco Systems Inc VIC Ethernet NIC [1137:0043] (rev a2)\n        Subsystem: Cisco Systems Inc VIC 1225 PCIe Ethernet NIC [1137:0085]\n        Kernel driver in use: enic\n48:00.0 Ethernet controller [0200]: Cisco Systems Inc VIC Ethernet NIC [1137:0043] (rev a2)\n        Subsystem: Cisco Systems Inc VIC 1225 PCIe Ethernet NIC [1137:0085]\n        Kernel driver in use: enic\n87:00.0 Ethernet controller [0200]: Cisco Systems Inc VIC Ethernet NIC [1137:0043] (rev a2)\n        Subsystem: Cisco Systems Inc VIC 1225 PCIe Ethernet NIC [1137:0085]\n        Kernel driver in use: enic\n88:00.0 Ethernet controller [0200]: Cisco Systems Inc VIC Ethernet NIC [1137:0043] (rev a2)\n        Subsystem: Cisco Systems Inc VIC 1225 PCIe Ethernet NIC [1137:0085]\n        Kernel driver in use: enic\n8b:00.0 Ethernet controller [0200]: Cisco Systems Inc VIC Ethernet NIC [1137:0043] (rev a2)\n        Subsystem: Cisco Systems Inc VIC 1225 PCIe Ethernet NIC [1137:0085]\n        Kernel driver in use: enic\nsh-5.1#\n</code></pre>","tags":["cnv","kubevirt","v4.17"]},{"location":"kubevirt/pci-passthrough/#machineconfig-to-achieve-point-12-and-3","title":"MachineConfig to achieve Point 1,2 and 3","text":"<pre><code>apiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfig\nmetadata:\n  labels:\n    machineconfiguration.openshift.io/role: worker\n  name: 100-virt-node-pci-passthrough\nspec:\n  config:\n    ignition:\n      version: 3.4.0\n    storage:\n      files:\n        - contents:\n            compression: \"\"\n            source: data:,options%20vfio-pci%20ids%3D1137%3A0043%0A\n          mode: 420\n          overwrite: true\n          path: /etc/modprobe.d/vfio.conf\n        - contents:\n            compression: \"\"\n            source: data:,vfio-pci\n          mode: 420\n          overwrite: true\n          path: /etc/modules-load.d/vfio-pci.conf\n        - contents:\n            compression: \"\"\n            source: data:,blacklist%20enic%0A\n          mode: 420\n          overwrite: true\n          path: /etc/modprobe.d/blacklist-enic.conf\n  kernelArguments:\n    - intel_iommu=on\n    - enic.blacklist=1\n    - rd.driver.blacklist=enic\n</code></pre>","tags":["cnv","kubevirt","v4.17"]},{"location":"kubevirt/pci-passthrough/#kubevirt-openshift-virtualization-configuration-changes-to-achive-point-4","title":"KubeVirt / OpenShift Virtualization configuration changes to achive point 4","text":"<pre><code>spec:\n  permittedHostDevices:\n    pciHostDevices:\n    - pciDeviceSelector: 1137:0043\n      resourceName: cisco.com/VIC_1225\n</code></pre>","tags":["cnv","kubevirt","v4.17"]},{"location":"kubevirt/pci-passthrough/#check-the-node","title":"Check the node","text":"<pre><code>$ oc describe node/ucs57 | grep -A10 'Allocatable:'\nAllocatable:\n  bridge.network.kubevirt.io/coe-bridge:  1k\n  cisco.com/VIC_1225:                     4\n  cpu:                                    59780m\n  devices.kubevirt.io/kvm:                1k\n  devices.kubevirt.io/tun:                1k\n  devices.kubevirt.io/vhost-net:          1k\n  ephemeral-storage:                      718240181082\n  hugepages-1Gi:                          0\n  hugepages-2Mi:                          0\n  memory:                                 1028260784Ki\n</code></pre>","tags":["cnv","kubevirt","v4.17"]},{"location":"kubevirt/pci-passthrough/#create-a-virtual-machine","title":"Create a virtual machine","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nspec:\n  template:\n    spec:\n      domain:\n        devices:\n          hostDevices:\n            - deviceName: cisco.com/VIC_1225\n              name: hostDevices-jade-booby-35\n</code></pre>","tags":["cnv","kubevirt","v4.17"]},{"location":"kubevirt/storage/","title":"OpenShift Virtualization Storage","text":"<p>OpenShift Virtualization uses the Persistent Volume (PV) paradigm from Kubernetes to provide storage for virtual machines. More specifically, each VM disk is stored in a dedicated persistent volume provisioned and managed by a CSI (Container Storage Interface) driver provided by Red Hat or a supported storage vendor. This is different from the paradigm used by other virtualization offerings where a single storage device, e.g. an iSCSI/FC LUN or NFS export, is used to store many virtual machine disks.</p> <p>Live migration of virtual machines requires that all virtual machine disks for the VM being migrated use PVCs with the read-write-many (RWX) access mode.</p> Important <p>OCP Virtualization works differently for storage than vSphere and RHV: The reuse of existing storage is the aim by customers across the board but only little is known about the differences in the consumption of storage with OpenShift Virtualization. This reveals to be a major challenge for most of the vSphere to OCP-V projects.</p> <p>In vSphere plain storage consumption, the storage capacity is provided using large volumes (aka described as LUNs) as datastores and then the vmdk \"block\" devices carved out as files coming from vmfs residing on the datastores. A similar approach is used by RHV.</p> <p>In OCP Virtualization instead, every persistent volume for a virtual disk coming from a block storage must be provided as a real world block storage device. This usually leads to trouble on the OCP nodes as well as on the enterprise storage: the number of PVs used for VMs define the number of block volumes to create on the enterprise storage subsystems, to be handled in SAN mappings, and to be managed by FC/iSCSI and multi-pathing on the OCP nodes.</p>","tags":["cnv","kubevirt","storage","ocp-v","csi"]},{"location":"kubevirt/storage/#overview-of-the-csi-components","title":"Overview of the CSI Components","text":"<p>The diagram below provides a high-level overview of the CSI components in an OpenShift cluster. The CSI driver coordinates the creation and management of the volume on the storage device, mounting of the volume to the node hosting the virtual machine, and implementing features like snapshots and volume resizing at the storage volume level.</p> <ul> <li>Source: Docs Red Hat - OCP 4.17 Using CSI</li> </ul> <p></p>","tags":["cnv","kubevirt","storage","ocp-v","csi"]},{"location":"kubevirt/storage/#csi-drivers-supported-by-openshift-container-platform","title":"CSI drivers supported by OpenShift Container Platform","text":"<p>OpenShift Container Platform installs certain CSI drivers by default, giving users storage options that are not possible with in-tree volume plugins.</p> <p>The following table describes the CSI drivers that are installed with OpenShift Container Platform supported by OpenShift Container Platform and which CSI features they support, such as volume snapshots and resize.</p>","tags":["cnv","kubevirt","storage","ocp-v","csi"]},{"location":"kubevirt/storage/#applies-to-ocp-v-version-417","title":"Applies to OCP-V version 4.17","text":"<ul> <li>Source: Docs Red Hat - OCP 4.17 Using CSI</li> </ul> <ul> <li>(1) Requires vSphere version 7.0 Update 3 or later for both vCenter Server and ESXi.</li> <li> <p>Does not support fileshare volumes.</p> </li> <li> <p>(2) Offline volume expansion: minimum required vSphere version is 6.7 Update 3 P06</p> </li> <li> <p>Online volume expansion: minimum required vSphere version is 7.0 Update 2.</p> </li> <li> <p>(3) Does not support offline snapshots or resize. Volume must be attached to a running pod.</p> </li> <li> <p>(4) Azure File cloning does not supports NFS protocol. It supports the azurefile-csi storage class, which uses SMB protocol.</p> </li> <li>Azure File cloning and snapshot are Technology Preview features.</li> </ul>","tags":["cnv","kubevirt","storage","ocp-v","csi"]},{"location":"kubevirt/storage/#configuring-local-storage-for-virtual-machines","title":"Configuring local storage for virtual machines","text":"<p>When you install the OpenShift Virtualization Operator, the Hostpath Provisioner Operator is automatically installed. HPP is a local storage provisioner designed for OpenShift Virtualization that is created by the Hostpath Provisioner Operator.</p> <p>Official documentation: Creating a hostpath provisioner with a basic storage pool</p>","tags":["cnv","kubevirt","storage","ocp-v","csi"]},{"location":"kubevirt/storage/#using-locally-created-directories-as-local-storage","title":"Using locally created directories as local-storage","text":"<p>The described way below creates an directory and mounts it as a device (<code>/dev/vdb</code>) on each node. The appropriate resources must be created accordingly.</p>","tags":["cnv","kubevirt","storage","ocp-v","csi"]},{"location":"kubevirt/storage/#create-a-backing-directory-on-each-node","title":"Create a backing directory on each node","text":"<p>Label your CNV nodes:</p> <pre><code>oc label node/compute-0 node-role.kubernetes.io/cnv\n</code></pre>","tags":["cnv","kubevirt","storage","ocp-v","csi"]},{"location":"kubevirt/storage/#create-machineconfigpool","title":"Create MachineConfigPool","text":"<p>Important</p> <p>One node can only apply one MachineConfigPool! That's why you have to include all worker machineconfigurations.</p> <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfigPool\nmetadata:\n  name: cnv\nspec:\n  machineConfigSelector:\n    matchExpressions:\n      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,cnv]}\n  nodeSelector:\n    matchLabels:\n      node-role.kubernetes.io/cnv: \"\"\nEOF\n</code></pre> <p>Source: custom-pools</p>","tags":["cnv","kubevirt","storage","ocp-v","csi"]},{"location":"kubevirt/storage/#create-machineconfig","title":"Create MachineConfig","text":"<p>Note</p> <p>Machine Config Operator do not support the Ignition filesystem.directory method. Supported vs Unsupported Ignition config changes</p> <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfig\nmetadata:\n  name: 50-cnv-local-storage\n  labels:\n    machineconfiguration.openshift.io/role: cnv\nspec:\n  config:\n    ignition:\n      version: 2.2.0\n    storage:\n      filesystems:\n      - name: storage\n        mount:\n          device: /dev/vdb\n          format: xfs\n          wipe_filesystem: false\n    systemd:\n      units:\n        - contents: |\n            [Unit]\n            Description=Create mountpoint /var/srv/storage\n            Before=kubelet.service\n\n            [Service]\n            ExecStart=/bin/mkdir -p /var/srv/storage\n\n            [Install]\n            WantedBy=var-srv-storage.mount\n          enabled: true\n          name: create-mountpoint-var-srv-storage.service\n        - name: var-srv-storage.mount\n          enabled: true\n          contents: |\n            [Unit]\n            Before=local-fs.target\n            [Mount]\n            What=/dev/vdb\n            Where=/var/srv/storage\n            Type=xfs\n            [Install]\n            WantedBy=local-fs.target\n        - contents: |\n            [Unit]\n            Description=Set SELinux chcon for hostpath provisioner\n            Before=kubelet.service\n\n            [Service]\n            ExecStart=/usr/bin/chcon -Rt container_file_t /var/srv/storage\n\n            [Install]\n            WantedBy=multi-user.target\n          enabled: true\n          name: hostpath-provisioner.service\nEOF\n</code></pre>","tags":["cnv","kubevirt","storage","ocp-v","csi"]},{"location":"kubevirt/storage/#create-hostpathprovisioner","title":"Create HostPathProvisioner","text":"<pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: hostpathprovisioner.kubevirt.io/v1alpha1\nkind: HostPathProvisioner\nmetadata:\n  name: hostpath-provisioner\nspec:\n  imagePullPolicy: IfNotPresent\n  pathConfig:\n    path: \"/var/srv/storage\"\n    useNamingPrefix: \"false\"\nEOF\n</code></pre>","tags":["cnv","kubevirt","storage","ocp-v","csi"]},{"location":"kubevirt/storage/#create-storageclass","title":"Create StorageClass","text":"<pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: hostpath-provisioner\nprovisioner: kubevirt.io/hostpath-provisioner\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\nEOF\n</code></pre>","tags":["cnv","kubevirt","storage","ocp-v","csi"]},{"location":"kubevirt/template/","title":"Virtual Machine Templates in OpenShift Virtualization","text":"<ol> <li> <p>First create our own virtual machine image template.</p> </li> <li> <p>Upload the image into a PVC in namespace openshift-virtualization-os-images</p> <pre><code>oc project openshift-virtualization-os-images\n\nvirtctl image-upload pvc remote-mgmt-supporter-20231211 \\\n    --size 55Gi --storage-class coe-netapp-nas \\\n    --access-mode ReadWriteMany \\\n    --image-path remote-mgmt-supporter-20231211.qcow\n</code></pre> </li> <li> <p>Create DataSource to the new PVC</p> datasource.yamlDownload <pre><code>apiVersion: cdi.kubevirt.io/v1beta1\nkind: DataSource\nmetadata:\n  annotations:\n  name: remote-mgmt-supporter\n  namespace: openshift-virtualization-os-images\nspec:\n  source:\n    pvc:\n      name: remote-mgmt-supporter-20231211\n      namespace: openshift-virtualization-os-images\n</code></pre> <pre><code>curl -L -O https://examples.openshift.pub/pr-133/kubevirt/template/datasource.yaml\n</code></pre> </li> <li> <p>Adjust / Apply template</p> template.yamlDownload <pre><code>apiVersion: template.openshift.io/v1\nkind: Template\nmetadata:\n  annotations:\n    defaults.template.kubevirt.io/disk: rootdisk\n    description: |\n      32Bit Debian based bunsenlabs VM with old Browsers and Java.\n      To managed all systems at the COE Lab Munich.\n\n    iconClass: icon-debian\n    name.os.template.kubevirt.io/debian11: Based in Debian 11 Bullseye\n    openshift.io/display-name: COE Lab - Remote Mgmt Supporter\n    openshift.io/documentation-url: https://gitlab.consulting.redhat.com/coe-lab/remote-mgmt-supporter\n    openshift.io/provider-display-name: COE Lab\n    openshift.io/support-url: https://gitlab.consulting.redhat.com/coe-lab/remote-mgmt-supporter/-/issues\n    openshift.kubevirt.io/pronounceable-suffix-for-name-expression: \"true\"\n\n    tags: debian,linux,coe,remotemgmt\n\n    template.kubevirt.io/editable: |\n      /objects[0].spec.template.spec.domain.cpu.sockets\n      /objects[0].spec.template.spec.domain.cpu.cores\n      /objects[0].spec.template.spec.domain.cpu.threads\n      /objects[0].spec.template.spec.domain.memory.guest\n      /objects[0].spec.template.spec.domain.devices.disks\n      /objects[0].spec.template.spec.volumes\n      /objects[0].spec.template.spec.networks\n\n    template.kubevirt.io/images: |\n      https://drive.google.com/drive/folders/1YclzXWW56YZuTfCBotjU79CFLQ-X2WJG?usp=drive_link\n    template.kubevirt.io/provider: COE Lab\n    template.kubevirt.io/provider-support-level: Full\n    template.kubevirt.io/provider-url: https://gitlab.consulting.redhat.com/coe-lab/remote-mgmt-supporter\n    template.openshift.io/bindable: \"false\"\n  labels:\n    flavor.template.kubevirt.io/large: \"true\"\n    name.os.template.kubevirt.io/debian11: \"true\"\n    template.kubevirt.io/type: base\n    workload.template.kubevirt.io/desktop: \"true\"\n    template.kubevirt.io/default-os-variant: \"true\"\n  name: remote-mgmt-supporter\n  namespace: openshift\n\nobjects:\n  - apiVersion: kubevirt.io/v1\n    kind: VirtualMachine\n    metadata:\n      labels:\n        app: ${NAME}\n        kubevirt.io/dynamic-credentials-support: \"false\"\n      name: ${NAME}\n    spec:\n      dataVolumeTemplates:\n        - apiVersion: cdi.kubevirt.io/v1beta1\n          kind: DataVolume\n          metadata:\n            name: ${NAME}\n          spec:\n            sourceRef:\n              kind: DataSource\n              name: ${DATA_SOURCE_NAME}\n              namespace: ${DATA_SOURCE_NAMESPACE}\n            storage:\n              resources:\n                requests:\n                  storage: 60Gi\n      running: false\n      template:\n        metadata:\n          annotations:\n            vm.kubevirt.io/flavor: large\n            vm.kubevirt.io/os: other\n            vm.kubevirt.io/workload: server\n          labels:\n            kubevirt.io/domain: ${NAME}\n            kubevirt.io/size: large\n        spec:\n          domain:\n            cpu:\n              cores: 1\n              sockets: 2\n              threads: 1\n            devices:\n              disks:\n                - disk:\n                    bus: virtio\n                  name: rootdisk\n              interfaces:\n                - masquerade: {}\n                  model: virtio\n                  name: default\n              networkInterfaceMultiqueue: true\n              rng: {}\n            features:\n              smm:\n                enabled: true\n            firmware:\n              bootloader:\n                efi: {}\n            machine:\n              type: pc-q35-rhel9.2.0\n            memory:\n              guest: 8Gi\n          networks:\n            - name: default\n              pod: {}\n          terminationGracePeriodSeconds: 180\n          volumes:\n            - dataVolume:\n                name: ${NAME}\n              name: rootdisk\nparameters:\n  - description: VM name\n    from: remote-mgmt-supporter-[a-z0-9]{16}\n    generate: expression\n    name: NAME\n  - description: Name of the DataSource to clone\n    name: DATA_SOURCE_NAME\n    value: remote-mgmt-supporter\n  - description: Namespace of the DataSource\n    name: DATA_SOURCE_NAMESPACE\n    value: openshift-virtualization-os-images\n</code></pre> <pre><code>curl -L -O https://examples.openshift.pub/pr-133/kubevirt/template/template.yaml\n</code></pre> </li> </ol>","tags":["cnv","kubevirt","template","ocp-v"]},{"location":"kubevirt/application-aware-quota/","title":"Application Aware Quota","text":"<p>Documentation:</p> <ul> <li>https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/virtualization/index#virt-understanding-aaq-operator</li> <li>https://access.redhat.com/solutions/7063206</li> </ul> <p>Tested with:</p> Component Version OpenShift v4.18.19 OpenShift Virt v4.18.8","tags":["aaq","cnv","v4.18"]},{"location":"kubevirt/application-aware-quota/#enable-application-aware-quota-in-openshift-virt","title":"Enable Application Aware Quota in  OpenShift Virt","text":"<pre><code>oc edit hco -n openshift-cnv   kubevirt-hyperconverged\n</code></pre> <p>Change following settings</p> <pre><code>spec:\n  applicationAwareConfig:\n    allowApplicationAwareClusterResourceQuota: true\n    vmiCalcConfigName: DedicatedVirtualResources\n  featureGates:\n    enableApplicationAwareQuota: true\n</code></pre>","tags":["aaq","cnv","v4.18"]},{"location":"kubevirt/application-aware-quota/#lets-create-a-project-with-quota","title":"Let's create a project with quota","text":"<pre><code>oc new-project single-project-quota-test\n</code></pre> <p>Label project to enabel application aware quota:</p> <pre><code>oc label namespace single-project-quota-test application-aware-quota/enable-gating=\n</code></pre> <p>Apply quota:</p> OCexample-resource-quota.ApplicationAwareResourceQuota.yaml <pre><code>oc apply -f https://examples.openshift.pub/pr-133/kubevirt/application-aware-quota/example-resource-quota.ApplicationAwareResourceQuota.yaml\n</code></pre> <pre><code>apiVersion: aaq.kubevirt.io/v1alpha1\nkind: ApplicationAwareResourceQuota\nmetadata:\n  name: example-resource-quota\nspec:\n  hard:\n    requests.cpu/vmi: \"5\"\n    requests.memory/vmi: 10Gi\n    limits.cpu/vmi: \"10\"\n    limits.memory/vmi: 20Gi\n</code></pre> <p>Check the quota</p> <pre><code>$ oc describe arq  example-resource-quota\nName:         example-resource-quota\nNamespace:    single-project-quota-test\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  aaq.kubevirt.io/v1alpha1\nKind:         ApplicationAwareResourceQuota\nMetadata:\n  Creation Timestamp:  2025-07-30T20:15:36Z\n  Generation:          1\n  Resource Version:    8431156\n  UID:                 cb3dbe5a-a09e-4fa6-8c99-07f46290796d\nSpec:\n  Hard:\n    limits.cpu/vmi:       10\n    limits.memory/vmi:    20Gi\n    requests.cpu/vmi:     5\n    requests.memory/vmi:  10Gi\nStatus:\n  Hard:\n    limits.cpu/vmi:       10\n    limits.memory/vmi:    20Gi\n    requests.cpu/vmi:     5\n    requests.memory/vmi:  10Gi\n  Used:\n    requests.cpu/vmi:     0\n    requests.memory/vmi:  0\nEvents:                   &lt;none&gt;\n</code></pre> <p>Start some workload</p> <pre><code>oc apply -k git@github.com:openshift-examples/kustomize/components/vmpool-no-load\noc scale vmpool/no-load --replicas 10\n</code></pre> <p>Info</p> <p>Only five (5) will be scheduled.</p> List of VM's <pre><code>oc get vm,vmi\nNAME                                   AGE   STATUS     READY\nvirtualmachine.kubevirt.io/no-load-0   45s   Starting   False\nvirtualmachine.kubevirt.io/no-load-1   45s   Running    True\nvirtualmachine.kubevirt.io/no-load-2   45s   Starting   False\nvirtualmachine.kubevirt.io/no-load-3   45s   Starting   False\nvirtualmachine.kubevirt.io/no-load-4   45s   Running    True\nvirtualmachine.kubevirt.io/no-load-5   45s   Starting   False\nvirtualmachine.kubevirt.io/no-load-6   45s   Running    True\nvirtualmachine.kubevirt.io/no-load-7   45s   Running    True\nvirtualmachine.kubevirt.io/no-load-8   45s   Running    True\nvirtualmachine.kubevirt.io/no-load-9   45s   Starting   False\n\nNAME                                           AGE   PHASE        IP             NODENAME   READY\nvirtualmachineinstance.kubevirt.io/no-load-0   44s   Scheduling                             False\nvirtualmachineinstance.kubevirt.io/no-load-1   45s   Running      10.128.2.216   ucs55      True\nvirtualmachineinstance.kubevirt.io/no-load-2   44s   Scheduling                             False\nvirtualmachineinstance.kubevirt.io/no-load-3   43s   Scheduling                             False\nvirtualmachineinstance.kubevirt.io/no-load-4   44s   Running      10.128.2.217   ucs55      True\nvirtualmachineinstance.kubevirt.io/no-load-5   44s   Scheduling                             False\nvirtualmachineinstance.kubevirt.io/no-load-6   45s   Running      10.128.2.215   ucs55      True\nvirtualmachineinstance.kubevirt.io/no-load-7   44s   Running                     ucs56      True\nvirtualmachineinstance.kubevirt.io/no-load-8   44s   Running                     ucs56      True\nvirtualmachineinstance.kubevirt.io/no-load-9   43s   Scheduling                             False\n</code></pre> <p>Check the quota again:</p> <pre><code>oc describe arq  example-resource-quota\nName:         example-resource-quota\nNamespace:    single-project-quota-test\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  aaq.kubevirt.io/v1alpha1\nKind:         ApplicationAwareResourceQuota\nMetadata:\n  Creation Timestamp:  2025-07-30T20:15:36Z\n  Generation:          1\n  Resource Version:    8447197\n  UID:                 cb3dbe5a-a09e-4fa6-8c99-07f46290796d\nSpec:\n  Hard:\n    limits.cpu/vmi:       10\n    limits.memory/vmi:    20Gi\n    requests.cpu/vmi:     5\n    requests.memory/vmi:  10Gi\nStatus:\n  Hard:\n    limits.cpu/vmi:       10\n    limits.memory/vmi:    20Gi\n    requests.cpu/vmi:     5\n    requests.memory/vmi:  10Gi\n  Used:\n    requests.cpu/vmi:     5\n    requests.memory/vmi:  5120M\nEvents:                   &lt;none&gt;\n</code></pre>","tags":["aaq","cnv","v4.18"]},{"location":"kubevirt/application-aware-quota/#lets-configure-a-cluster-wide-quota","title":"Let's configure a cluster wide quota","text":"<p>Change project request template, to label all new project/namespaces. Documetation</p> <p>Create template</p> <pre><code>oc adm create-bootstrap-project-template -o yaml &gt; template.yaml\n</code></pre> <p>Adjust template:</p> Changes (diff) <pre><code># diff -Nuar &lt;(oc adm create-bootstrap-project-template -o yaml) template.yaml\n--- /dev/fd/11  2025-07-30 22:42:46.359683599 +0200\n+++ template.yaml       2025-07-30 22:42:26.314463561 +0200\n@@ -3,6 +3,7 @@\n metadata:\n   creationTimestamp: null\n   name: project-request\n+  namespace: openshift-config\n objects:\n - apiVersion: project.openshift.io/v1\n   kind: Project\n@@ -11,6 +12,8 @@\n       openshift.io/description: ${PROJECT_DESCRIPTION}\n       openshift.io/display-name: ${PROJECT_DISPLAYNAME}\n       openshift.io/requester: ${PROJECT_REQUESTING_USER}\n+    labels:\n+      application-aware-quota/enable-gating: \"\"\n     creationTimestamp: null\n     name: ${PROJECT_NAME}\n   spec: {}\n</code></pre> Apply via CLIFinal YAML <pre><code>oc apply -f https://examples.openshift.pub/pr-133/kubevirt/application-aware-quota/template.yaml\n</code></pre> <pre><code>apiVersion: template.openshift.io/v1\nkind: Template\nmetadata:\n  creationTimestamp: null\n  name: project-request\n  namespace: openshift-config\nobjects:\n  - apiVersion: project.openshift.io/v1\n    kind: Project\n    metadata:\n      annotations:\n        openshift.io/description: ${PROJECT_DESCRIPTION}\n        openshift.io/display-name: ${PROJECT_DISPLAYNAME}\n        openshift.io/requester: ${PROJECT_REQUESTING_USER}\n      labels:\n        application-aware-quota/enable-gating: \"\"\n      creationTimestamp: null\n      name: ${PROJECT_NAME}\n    spec: {}\n    status: {}\n  - apiVersion: rbac.authorization.k8s.io/v1\n    kind: RoleBinding\n    metadata:\n      creationTimestamp: null\n      name: admin\n      namespace: ${PROJECT_NAME}\n    roleRef:\n      apiGroup: rbac.authorization.k8s.io\n      kind: ClusterRole\n      name: admin\n    subjects:\n      - apiGroup: rbac.authorization.k8s.io\n        kind: User\n        name: ${PROJECT_ADMIN_USER}\nparameters:\n  - name: PROJECT_NAME\n  - name: PROJECT_DISPLAYNAME\n  - name: PROJECT_DESCRIPTION\n  - name: PROJECT_ADMIN_USER\n  - name: PROJECT_REQUESTING_USER\n</code></pre> <p>Configura template</p> <pre><code>oc edit project.config.openshift.io/cluster\n</code></pre> <p>Change</p> <pre><code>spec:\n  projectRequestTemplate:\n    name: project-request\n</code></pre> <p>Create cluster wide quota</p> OCexample-resource-quota.aacrq.yaml <pre><code>oc apply -f https://examples.openshift.pub/pr-133/kubevirt/application-aware-quota/example-resource-quota.aacrq.yaml\n</code></pre> <pre><code>apiVersion: aaq.kubevirt.io/v1alpha1\nkind: ApplicationAwareClusterResourceQuota\nmetadata:\n  name: example-resource-quota\nspec:\n  quota:\n    hard:\n      requests.cpu/vmi: \"20\"\n      requests.memory/vmi: 20Gi\n      limits.cpu/vmi: \"20\"\n      limits.memory/vmi: 20Gi\n  selector:\n    annotations: null\n    labels:\n      matchLabels:\n        application-aware-quota/enable-gating: \"\"\n</code></pre> <p>Let's check the quota</p> <pre><code>$ oc describe  aacrq example-resource-quota\nName:         example-resource-quota\nNamespace:    cluster-wide-quota-a\nLabels:       aaq.kubevirt.io=true\nAnnotations:  &lt;none&gt;\nAPI Version:  aaq.kubevirt.io/v1alpha1\nKind:         ApplicationAwareAppliedClusterResourceQuota\nMetadata:\n  Creation Timestamp:  2025-07-30T20:52:10Z\n  Generation:          6\n  Owner References:\n    API Version:           aaq.kubevirt.io/v1alpha1\n    Block Owner Deletion:  true\n    Controller:            true\n    Kind:                  ApplicationAwareClusterResourceQuota\n    Name:                  example-resource-quota\n    UID:                   fb9a1ec3-9940-4356-803d-d092d1f81486\n  Resource Version:        8478833\n  UID:                     b62ae8b1-05e6-42bb-b30e-811e19ab6fcb\nSpec:\n  Quota:\n    Hard:\n      limits.cpu/vmi:       20\n      limits.memory/vmi:    20Gi\n      requests.cpu/vmi:     20\n      requests.memory/vmi:  20Gi\n  Selector:\n    Annotations:  &lt;nil&gt;\n    Labels:\n      Match Labels:\n        application-aware-quota/enable-gating:\nStatus:\n  Namespaces:\n    Namespace:                single-project-quota-test\n    Status:\n      Hard:\n        limits.cpu/vmi:       20\n        limits.memory/vmi:    20Gi\n        requests.cpu/vmi:     20\n        requests.memory/vmi:  20Gi\n      Used:\n        requests.cpu/vmi:     5\n        requests.memory/vmi:  5120M\n  Total:\n    Hard:\n      limits.cpu/vmi:       20\n      limits.memory/vmi:    20Gi\n      requests.cpu/vmi:     20\n      requests.memory/vmi:  20Gi\n    Used:\n      requests.cpu/vmi:     5\n      requests.memory/vmi:  5120M\nEvents:                     &lt;none&gt;\n</code></pre> <p>Info</p> <p>We alread have some quota consumed, because the cluster quota includes our first project <code>single-project-quota-test</code></p> <p>Add more workload:</p> <pre><code>oc new-project cluster-wide-quota-a\noc apply -k git@github.com:openshift-examples/kustomize/components/vmpool-no-load\noc scale vmpool/no-load --replicas 10\n</code></pre> <p>Check quota again:</p> <pre><code>oc describe  aacrq example-resource-quota\nName:         example-resource-quota\nNamespace:    cluster-wide-quota-a\nLabels:       aaq.kubevirt.io=true\nAnnotations:  &lt;none&gt;\nAPI Version:  aaq.kubevirt.io/v1alpha1\nKind:         ApplicationAwareAppliedClusterResourceQuota\nMetadata:\n  Creation Timestamp:  2025-07-30T20:52:10Z\n  Generation:          13\n  Owner References:\n    API Version:           aaq.kubevirt.io/v1alpha1\n    Block Owner Deletion:  true\n    Controller:            true\n    Kind:                  ApplicationAwareClusterResourceQuota\n    Name:                  example-resource-quota\n    UID:                   fb9a1ec3-9940-4356-803d-d092d1f81486\n  Resource Version:        8483427\n  UID:                     b62ae8b1-05e6-42bb-b30e-811e19ab6fcb\nSpec:\n  Quota:\n    Hard:\n      limits.cpu/vmi:       20\n      limits.memory/vmi:    20Gi\n      requests.cpu/vmi:     20\n      requests.memory/vmi:  20Gi\n  Selector:\n    Annotations:  &lt;nil&gt;\n    Labels:\n      Match Labels:\n        application-aware-quota/enable-gating:\nStatus:\n  Namespaces:\n    Namespace:  single-project-quota-test\n    Status:\n      Hard:\n        limits.cpu/vmi:       20\n        limits.memory/vmi:    20Gi\n        requests.cpu/vmi:     20\n        requests.memory/vmi:  20Gi\n      Used:\n        requests.cpu/vmi:     5\n        requests.memory/vmi:  5120M\n    Namespace:                cluster-wide-quota-a\n    Status:\n      Hard:\n        limits.cpu/vmi:       20\n        limits.memory/vmi:    20Gi\n        requests.cpu/vmi:     20\n        requests.memory/vmi:  20Gi\n      Used:\n        requests.cpu/vmi:     10\n        requests.memory/vmi:  10240M\n  Total:\n    Hard:\n      limits.cpu/vmi:       20\n      limits.memory/vmi:    20Gi\n      requests.cpu/vmi:     20\n      requests.memory/vmi:  20Gi\n    Used:\n      requests.cpu/vmi:     15\n      requests.memory/vmi:  15360M\nEvents:                     &lt;none&gt;\n</code></pre>","tags":["aaq","cnv","v4.18"]},{"location":"kubevirt/application-aware-quota/#life-migration-still-works-with-100-quota-exited","title":"Life Migration still works with 100% quota exited","text":"<p>Check quota:</p> <pre><code>$ oc describe  aacrq example-resource-quota\n...\n  Total:\n    Hard:\n      limits.cpu/vmi:       20\n      limits.memory/vmi:    20Gi\n      requests.cpu/vmi:     20\n      requests.memory/vmi:  20Gi\n    Used:\n      requests.cpu/vmi:     20\n      requests.memory/vmi:  20480M\n</code></pre> <p>Check VMi's</p> <pre><code>$ oc get vmi\nNAME         AGE     PHASE        IP             NODENAME   READY\nno-load-0    16m     Running      10.128.2.232   ucs55      True\nno-load-1    16m     Running      10.128.2.228   ucs55      True\nno-load-10   9m15s   Running      10.128.2.234   ucs55      True\nno-load-11   9m15s   Running      10.129.0.190   ucs56      True\nno-load-12   9m13s   Scheduling                             False\nno-load-13   9m14s   Scheduling                             False\nno-load-14   9m14s   Running      10.129.0.191   ucs56      True\nno-load-15   9m14s   Running      10.128.2.236   ucs55      True\nno-load-16   9m14s   Scheduling                             False\nno-load-17   9m14s   Scheduling                             False\nno-load-18   9m14s   Running      10.128.2.235   ucs55      True\nno-load-19   9m14s   Scheduling                             False\nno-load-2    16m     Running      10.128.2.233   ucs55      True\nno-load-3    16m     Running      10.128.2.229   ucs55      True\nno-load-4    16m     Running      10.129.0.185   ucs56      True\nno-load-5    16m     Running      10.128.2.226   ucs55      True\nno-load-6    16m     Running      10.129.0.187   ucs56      True\nno-load-7    16m     Running      10.128.2.227   ucs55      True\nno-load-8    16m     Running      10.128.2.230   ucs55      True\nno-load-9    16m     Running      10.130.0.123   ucs57      True\n</code></pre> <p>Lets migrate</p> <pre><code>$ virtctl  migrate no-load-8\nVM no-load-8 was scheduled to migrate\n$ oc get vmi no-load-8 --watch\nNAME        AGE   PHASE     IP             NODENAME   READY\nno-load-8   17m   Running   10.128.2.230   ucs55      True\nno-load-8   17m   Running   10.128.2.230   ucs55      True\nno-load-8   17m   Running   10.128.2.230   ucs55      True\nno-load-8   17m   Running   10.128.2.230   ucs55      True\nno-load-8   17m   Running   10.128.2.230   ucs55      True\nno-load-8   17m   Running   10.128.2.230   ucs55      True\nno-load-8   17m   Running   10.128.2.230   ucs56      True\nno-load-8   17m   Running   10.129.0.193   ucs56      True\nno-load-8   17m   Running   10.129.0.193   ucs56      False\nno-load-8   17m   Running   10.129.0.193   ucs56      True\n</code></pre>","tags":["aaq","cnv","v4.18"]},{"location":"kubevirt/descheduler/","title":"Descheduler","text":"VM's need an annotion! <p>VM's / Pods need the annotation <code>descheduler.alpha.kubernetes.io/evict: true</code> to be included into the descheduler process! By default VM's createed via templates do  not have the annotation.</p> <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nspec:\n  template:\n    metadata:\n      annotations:\n        descheduler.alpha.kubernetes.io/evict: \"true\"\n</code></pre> <p>Applying the annotation to all VM's in the current namespace:</p> <pre><code>for VM in $(kubectl get vm -o jsonpath='{.items[*].metadata.name}'); do\n    echo \"Updating compute resources for VM: $VM\"\n    kubectl patch vm \"$VM\" --type='json' -p=\"[{'op': 'add', 'path': '/spec/template/metadata/annotations/descheduler.alpha.kubernetes.io~1evict', 'value': 'true'}]\"\ndone\n</code></pre>","tags":["ocp-v","kubevirt"]},{"location":"kubevirt/descheduler/#resources","title":"Resources","text":"<ul> <li>7.14.15. Enabling descheduler evictions on virtual machines</li> <li>4.9.1. Descheduler overview</li> </ul>","tags":["ocp-v","kubevirt"]},{"location":"kubevirt/descheduler/#installation","title":"Installation","text":"<ul> <li>Install Operator Kube Descheduler Operator</li> </ul>","tags":["ocp-v","kubevirt"]},{"location":"kubevirt/descheduler/#configuration","title":"Configuration","text":"<p>KubeDescheduler details</p> <pre><code>apiVersion: operator.openshift.io/v1\nkind: KubeDescheduler\nmetadata:\n  name: cluster\n  namespace: openshift-kube-descheduler-operator\nspec:\n  logLevel: Normal\n  mode: Automatic\n  operatorLogLevel: Normal\n  deschedulingIntervalSeconds: 60\n  profileCustomizations:\n    devEnableEvictionsInBackground: true\n    devLowNodeUtilizationThresholds: Medium\n</code></pre>","tags":["ocp-v","kubevirt"]},{"location":"kubevirt/livemigration/","title":"Live Migration","text":"","tags":["tagA","tagB","v4.17"]},{"location":"kubevirt/livemigration/#content","title":"Content","text":"","tags":["tagA","tagB","v4.17"]},{"location":"kubevirt/livemigration/cclm/","title":"Cross cluster live migration","text":"<p>Official documentation: 12.5. Configuring a cross-cluster live migration network</p> <p>Tested with:</p> Component Version OpenShift v4.20.4 OpenShift Virt v4.20.1 MTV v2.10.0 <p>Without ACM, just a pure cross cluster live migration with two OpenShift clusters.</p>","tags":["cnv","kubevirt","ocp-v","v4.12"]},{"location":"kubevirt/livemigration/cclm/#cluster-overview","title":"Cluster overview","text":"<p>We have two identicial clusters in terms of</p> <ul> <li>OpenShift Version</li> <li>CPU Type and Model</li> </ul> <p>Cluster one called OCP1 is the target cluster with mtv. Cluster two called OCP7 is the source cluster.</p> <p>This cluster are running on bare OpenShift Cluster called ISAR.</p> <p></p>","tags":["cnv","kubevirt","ocp-v","v4.12"]},{"location":"kubevirt/livemigration/cclm/#details-about-the-ocp1-ocp7-adjustments-at-isar","title":"Details about the OCP1 &amp; OCP7 adjustments at ISAR","text":"<p>OCP1 and OCP7 are provided via our stormshift automation</p> OCP1 &amp; OCP7 Infrastructure details","tags":["cnv","kubevirt","ocp-v","v4.12"]},{"location":"kubevirt/livemigration/cclm/#patch-the-cpu-model","title":"Patch the cpu model","text":"Command <pre><code>oc get vm -o name | xargs oc patch  --type=merge -p '{\"spec\":{\"template\":{\"spec\":{\"domain\":{\"cpu\":{\"model\":\"Haswell-v4\"}}}}}}'\n</code></pre>","tags":["cnv","kubevirt","ocp-v","v4.12"]},{"location":"kubevirt/livemigration/cclm/#enable-vt-xvmx-feature","title":"Enable VT-X/vmx feature","text":"Command <pre><code>oc get vm -o name | xargs oc patch  --type=merge -p '{\"spec\":{\"template\":{\"spec\":{\"domain\":{\"cpu\":{\"features\":[{\"name\":\"vmx\",\"policy\":\"require\"}]}}}}}}'\n</code></pre>","tags":["cnv","kubevirt","ocp-v","v4.12"]},{"location":"kubevirt/livemigration/cclm/#restart-all-vms","title":"Restart all VM's","text":"Command <pre><code>oc get vm --no-headers -o custom-columns=\"NAME:.metadata.name\" | xargs -n1 virtctl restart\n</code></pre>","tags":["cnv","kubevirt","ocp-v","v4.12"]},{"location":"kubevirt/livemigration/cclm/#check-setttings-as-isar","title":"Check setttings as ISAR","text":"CommandExample output <pre><code>oc get vm -o custom-columns=NAME:.metadata.name,CPU:.spec.template.spec.domain.cpu\n</code></pre> <pre><code>oc get vm -o custom-columns=NAME:.metadata.name,CPU:.spec.template.spec.domain.cpu\nNAME            CPU\nocp1-cp-0       map[cores:8 features:[map[name:vmx policy:require]] model:Haswell-v4 sockets:1 threads:1]\nocp1-cp-1       map[cores:8 features:[map[name:vmx policy:require]] model:Haswell-v4 sockets:1 threads:1]\nocp1-cp-2       map[cores:8 features:[map[name:vmx policy:require]] model:Haswell-v4 sockets:1 threads:1]\nocp1-worker-0   map[cores:8 features:[map[name:vmx policy:require]] model:Haswell-v4 sockets:1 threads:1]\nocp1-worker-1   map[cores:8 features:[map[name:vmx policy:require]] model:Haswell-v4 sockets:1 threads:1]\nocp1-worker-2   map[cores:8 features:[map[name:vmx policy:require]] model:Haswell-v4 sockets:1 threads:1]\n</code></pre>","tags":["cnv","kubevirt","ocp-v","v4.12"]},{"location":"kubevirt/livemigration/cclm/#add-second-interface-into-vlan-2001-for-the-vmsnodes","title":"Add second interface into vlan 2001 for the VM's/nodes","text":"oc apply -f ....isar-2001-net-attach-def.yaml <pre><code>oc apply -n stormshift-ocp1-infra -f https://examples.openshift.pub/pr-133/kubevirt/livemigration/cclm/cclm/isar-2001-net-attach-def.yaml\noc apply -n stormshift-ocp7-infra -f https://examples.openshift.pub/pr-133/kubevirt/livemigration/cclm/cclm/isar-2001-net-attach-def.yaml\n</code></pre> <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  annotations:\n    k8s.v1.cni.cncf.io/resourceName: bridge.network.kubevirt.io/coe-bridge\n  name: coe-bridge-2001\nspec:\n  config: |\n    {\n      \"name\": \"coe-bridge\",\n      \"type\": \"bridge\",\n      \"cniVersion\": \"0.3.1\",\n      \"bridge\": \"coe-bridge\",\n      \"macspoofchk\": false,\n      \"ipam\": {},\n      \"vlan\": 2001,\n      \"preserveDefaultVlan\": false\n    }\n</code></pre>","tags":["cnv","kubevirt","ocp-v","v4.12"]},{"location":"kubevirt/livemigration/cclm/#adjust-vms-to-add-second-interface-to-worker-nodes","title":"Adjust VM's to add second interface to worker nodes:","text":"","tags":["cnv","kubevirt","ocp-v","v4.12"]},{"location":"kubevirt/livemigration/cclm/#ocp1-and-ocp7-cluster-preperation","title":"OCP1 and OCP7 cluster preperation","text":"","tags":["cnv","kubevirt","ocp-v","v4.12"]},{"location":"kubevirt/livemigration/cclm/#install-following-operators","title":"Install following operators","text":"<ul> <li>Nmstate Operator (instantiate the operator now)</li> <li>OpenShift Virtualization (instantiate the operator LATER!)</li> <li>Migration Toolkit for Virtualization (instantiate the operator LATER!)</li> </ul>","tags":["cnv","kubevirt","ocp-v","v4.12"]},{"location":"kubevirt/livemigration/cclm/#prepare-required-live-migration-network","title":"Prepare required live migration network","text":"<p>Both clusters have to be connected via an L2 network. In my case it's vlan 2001 with `192.168.201.0/24 subnet</p> <p>Here an high level overview:</p> <p></p> There is an documetion bug in the offical docs <p>https://issues.redhat.com/browse/CNV-74609</p> NodeNetworkConfigurationPolicy for linux bridge into VLAN 2001 <p>Apply this to OCP1 and OCP7</p> coe-bridge-via-enp2s0.yamloc apply -f .... <pre><code>apiVersion: nmstate.io/v1\nkind: NodeNetworkConfigurationPolicy\nmetadata:\n  name: br-2001-via-enp2s0\nspec:\n  desiredState:\n    interfaces:\n      - bridge:\n          options:\n            stp:\n              enabled: false\n          port:\n            - name: enp2s0\n        description: Linux Brige info COE Network via enp2s0\n        ipv4:\n          enabled: false\n        name: br-2001\n        state: up\n        type: linux-bridge\n  nodeSelector:\n    node-role.kubernetes.io/worker: \"\"\n</code></pre> <pre><code>oc apply -f https://examples.openshift.pub/pr-133/kubevirt/livemigration/cclm/cclm/coe-bridge-via-enp2s0.yaml\n</code></pre> NetworkAttachmentDefinition for OCP1 and OCP7 <p>Little helper for find out the interfaces on the nodes:</p> <pre><code>oc get nodes -l node-role.kubernetes.io/worker -o name | while read line ; do echo \"# $line\";oc debug -q $line -- ip -br l | grep enp ; done\n</code></pre> <p>Apply this to OCP1</p> ocp1.net-attach-def.yamloc apply -f .... <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: livemigration-network\n  namespace: openshift-cnv\nspec:\n  config: |\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"name\": \"migration-bridge\",\n      \"type\": \"macvlan\",\n      \"bridge\": \"br-2001\",\n      \"mode\": \"bridge\",\n      \"ipam\": {\n        \"type\": \"whereabouts\",\n        \"range\": \"192.168.201.0/24\",\n        \"exclude\": [\n           \"192.168.201.0/25\"\n        ]\n      }\n    }\n</code></pre> <pre><code>oc apply -f https://examples.openshift.pub/pr-133/kubevirt/livemigration/cclm/cclm/ocp1.net-attach-def.yaml\n</code></pre> <p>Apply this to OCP7</p> ocp7.net-attach-def.yamloc apply -f .... <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: livemigration-network\n  namespace: openshift-cnv\nspec:\n  config: |\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"name\": \"migration-bridge\",\n      \"type\": \"macvlan\",\n      \"bridge\": \"br-2001\",\n      \"mode\": \"bridge\",\n      \"ipam\": {\n        \"type\": \"whereabouts\",\n        \"range\": \"192.168.201.0/24\",\n        \"exclude\": [\n           \"192.168.201.128/25\"\n        ]\n      }\n    }\n</code></pre> <pre><code>oc apply -f https://examples.openshift.pub/pr-133/kubevirt/livemigration/cclm/cclm/ocp7.net-attach-def.yaml\n</code></pre>","tags":["cnv","kubevirt","ocp-v","v4.12"]},{"location":"kubevirt/livemigration/cclm/#instantiate-the-operator","title":"Instantiate the operator","text":"","tags":["cnv","kubevirt","ocp-v","v4.12"]},{"location":"kubevirt/livemigration/cclm/#openshift-virtualization-on-ocp1-and-ocp7","title":"OpenShift Virtualization on OCP1 and OCP7","text":"<p>Instantiate with following changes:</p> <pre><code>spec:\n  liveMigrationConfig:\n    network: livemigration-network\n  featureGates:\n    decentralizedLiveMigration: true\n</code></pre> <p>Wait until <code>virt-synchronization-controller-xxx</code> pods are running:</p> <pre><code>oc get pods -n openshift-cnv -l kubevirt.io=virt-synchronization-controller\n</code></pre> <pre><code>% oc get pods -n openshift-cnv -l kubevirt.io=virt-synchronization-controller\nNAME                                               READY   STATUS    RESTARTS   AGE\nvirt-synchronization-controller-5b58bd4478-5l25n   1/1     Running   0          3d21h\nvirt-synchronization-controller-5b58bd4478-zwpn4   1/1     Running   0          3d21h\n</code></pre> <p>Optional: Check the virt-handler migration network configuration:</p> <pre><code>% oc project openshift-cnv\n% oc get pods -l kubevirt.io=virt-handler  -o name | while read line ; do oc exec -q $line -- /bin/sh -c 'echo -n \"$HOSTNAME $NODE_NAME \"; ip -4 -br a show dev migration0' ; done\nvirt-handler-dm5mh ocp1-worker-2 migration0@if9   UP             192.168.201.129/24\nvirt-handler-h6bn9 ocp1-worker-1 migration0@if9   UP             192.168.201.131/24\nvirt-handler-nndkq ocp1-worker-0 migration0@if9   UP             192.168.201.130/24\n</code></pre> <pre><code>% oc project openshift-cnv\n% oc get pods -l kubevirt.io=virt-handler  -o name | while read line ; do oc exec -q $line -- /bin/sh -c 'echo -n \"$HOSTNAME $NODE_NAME \"; ip -4 -br a show dev migration0' ; done\nvirt-handler-bjwvt ocp7-worker-1 migration0@if9   UP             192.168.201.3/24\nvirt-handler-gl8gs ocp7-worker-0 migration0@if9   UP             192.168.201.1/24\nvirt-handler-kxg7k ocp7-worker-2 migration0@if8   UP             192.168.201.4/24\n</code></pre>","tags":["cnv","kubevirt","ocp-v","v4.12"]},{"location":"kubevirt/livemigration/cclm/#migration-toolkit-for-virtualization-on-ocp1","title":"Migration toolkit for Virtualization on OCP1","text":"<p>Instantiate with following change:</p> <pre><code>spec:\n  feature_ocp_live_migration: 'true'\n</code></pre>","tags":["cnv","kubevirt","ocp-v","v4.12"]},{"location":"kubevirt/livemigration/cclm/#migration-toolkit-for-virtualization","title":"Migration toolkit for Virtualization","text":"","tags":["cnv","kubevirt","ocp-v","v4.12"]},{"location":"kubevirt/livemigration/cclm/#create-provide-at-ocp1","title":"Create provide at OCP1","text":"","tags":["cnv","kubevirt","ocp-v","v4.12"]},{"location":"kubevirt/livemigration/cclm/#create-service-account-and-token-at-ocp7","title":"Create service account and token at OCP7","text":"<p>Create clusterrole <code>live-migration-role</code></p> <pre><code>oc apply  -f https://examples.openshift.pub/pr-133/kubevirt/livemigration/cclm/cclm/clusterrole.yaml\n</code></pre> <pre><code>oc create namespace openshift-mtv\noc create serviceaccount cclm -n openshift-mtv\noc create clusterrolebinding cclm --clusterrole=live-migration-role --serviceaccount=openshift-mtv:cclm\noc apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: cclm\n  namespace: openshift-mtv\n  annotations:\n    kubernetes.io/service-account.name: cclm\ntype: kubernetes.io/service-account-token\nEOF\n</code></pre> <p>Get the token:</p> <pre><code>oc get secret \"cclm\" -n \"openshift-mtv\" -o  go-template='{{ .data.token | base64decode}}{{\"\\n\"}}'\n</code></pre>","tags":["cnv","kubevirt","ocp-v","v4.12"]},{"location":"kubevirt/livemigration/cclm/#add-provider-ocp7-to-ocp1","title":"Add provider <code>ocp7</code> to OCP1","text":"","tags":["cnv","kubevirt","ocp-v","v4.12"]},{"location":"kubevirt/livemigration/cclm/#run-a-cross-cluster-live-migration","title":"Run a cross cluster live migration","text":"","tags":["cnv","kubevirt","ocp-v","v4.12"]},{"location":"kubevirt/livemigration/cclm/#resources","title":"Resources","text":"<ul> <li>https://access.redhat.com/solutions/7130438</li> <li>https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/virtualization/networking#virt-dedicated-network-live-migration</li> <li>https://github.com/k8snetworkplumbingwg/whereabouts</li> <li>https://docs.google.com/document/d/1x4r9gJVXVGe8ef6lMcciOqNbywJ5HKtR7E9kHVzIjAQ/edit?tab=t.0</li> <li>https://github.com/openshift/runbooks/pull/362</li> </ul>","tags":["cnv","kubevirt","ocp-v","v4.12"]},{"location":"kubevirt/networking/","title":"Networking","text":"<p>An OpenShift cluster is configured using an overlay software-defined network (SDN) for both the Pod and Service networks. By default, VMs are configured with connectivity to the SDN and have the same features/connectivity as Pod-based applications.</p> <p>Host-level networking configurations are created and applied using the NMstate operator. This includes the ability to report the current configuration options, such as bonds, bridges, and VLAN tags to help segregate networking resources, as well as apply desired-state configuration for those entities.</p> <ul> <li>Source: Red Hat Architecting OpenShift Virtualization</li> </ul> <p></p>","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#bonded-nics-for-management-and-sdn","title":"Bonded NICs for Management and SDN","text":"<p>The initial bond interface, consisting of two adapters bonded together with an IP address on the machine network specified and configured at install time, is used for the SDN, management traffic between the node and the control plane (and administrator access), and live migration traffic. During installation, use the host network interface configuration options to configure the bond and set the IP address needed.</p>","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#additional-dedicated-network-interfaces-for-traffic-types","title":"Additional dedicated Network Interfaces for traffic types","text":"<p>The following is a sample NMstate configuration making use of two adapters on the host to create a bonded interface in the LACP (802.3ad) run mode. The bonds are intended to be used for isolating network traffic for different purposes. This provides the advantage of avoiding noisy neighbor scenarios for some interfaces that may have a large impact, for example a backup for a virtual machine consuming significant network throughput impacting ODF or etcd traffic on a shared interface.</p> <pre><code>apiVersion: nmstate.io/v1\nkind: NodeNetworkConfigurationPolicy\nmetadata:\n  annotations:\n    description: a bond for VM traffic and VLANs\n  name: bonding-policy\nspec:\n  desiredState:\n    interfaces:\n      - link-aggregation:\n          mode: 802.3ad\n          port:\n            - enp6s0f0\n            - enp6s0f1\n        name: bond1\n        state: up\n        type: bond\n</code></pre>","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#example-vm-network-configuration","title":"Example VM Network Configuration","text":"<p>An example configuration for VM network connectivity is below, note that the bond configuration should be a part of the same NodeNetworkConfigurationPolicy to ensure they are configured together.</p> <pre><code>apiVersion: nmstate.io/v1\nkind: NodeNetworkConfigurationPolicy\nmetadata:\n  name: ovs-br1-vlan-trunk\nspec:\n  nodeSelector:\n    node-role.kubernetes.io/worker: ''\n  desiredState:\n    interfaces:\n    - name: ovs-br1\n      description: |-\n        A dedicated OVS bridge with bond2 as a port\n        allowing all VLANs and untagged traffic\n      type: ovs-bridge\n      state: up\n      bridge:\n        allow-extra-patch-ports: true\n        options:\n          stp: true\n        port:\n        - name: bond2\n    ovn:\n      bridge-mappings:\n      - localnet: vlan-2024\n        bridge: ovs-br1\n        state: present\n      - localnet: vlan-1993\n        bridge: ovs-br1\n        state: present\n---\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  annotations:\n    description: VLAN 2024 connection for VMs\n  name: vlan-2024\n  namespace: default\nspec:\n  config: |-\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"name\": \"vlan-2024\",\n      \"type\": \"ovn-k8s-cni-overlay\",\n      \"topology\": \"localnet\",\n      \"netAttachDefName\": \"default/vlan-2024\",\n      \"vlanID\": 2024,\n      \"ipam\": {}\n    }\n---\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  annotations:\n    description: VLAN 1993 connection for VMs\n  name: vlan-1993\n  namespace: default\nspec:\n  config: |-\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"name\": \"vlan-1993\",\n      \"type\": \"ovn-k8s-cni-overlay\",\n      \"topology\": \"localnet\",\n      \"netAttachDefName\": \"default/vlan-1993\",\n      \"vlanID\": 1993,\n      \"ipam\": {}\n    }\n</code></pre>","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#create-a-bridge-on-the-main-interface","title":"Create a bridge on the main interface","text":"<p>All nodes on which the configuration is executed are restarted.</p> <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: nmstate.io/v1alpha1\nkind: NodeNetworkConfigurationPolicy\nmetadata:\n  name: br1-ens3-policy-workers\nspec:\n  nodeSelector:\n    node-role.kubernetes.io/worker: \"\"\n  desiredState:\n    interfaces:\n      - name: br1\n        description: Linux bridge with ens3 as a port\n        type: linux-bridge\n        state: up\n        ipv4:\n          enabled: true\n          dhcp: true\n        bridge:\n          options:\n            stp:\n              enabled: false\n          port:\n            - name: ens3\nEOF\n</code></pre>","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#create-network-attachment-definition","title":"Create Network Attachment Definition","text":"<pre><code>cat &lt;&lt; EOF | oc apply -f -\napiVersion: \"k8s.cni.cncf.io/v1\"\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: tuning-bridge-fixed\n  annotations:\n    k8s.v1.cni.cncf.io/resourceName: bridge.network.kubevirt.io/br1\nspec:\n  config: '{\n    \"cniVersion\": \"0.3.1\",\n    \"name\": \"br1\",\n    \"plugins\": [\n      {\n        \"type\": \"cnv-bridge\",\n        \"bridge\": \"br1\"\n      },\n      {\n        \"type\": \"cnv-tuning\"\n      }\n    ]\n  }'\nEOF\n</code></pre>","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#example-localnet","title":"Example: Localnet","text":"<ul> <li>Tested with OpenShift 4.17.0</li> <li>Blog post: Red Hat OpenShift Virtualization: Configuring virtual machines to use external networks</li> </ul>","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#configure-localnet-via-nncp","title":"Configure localnet via NNCP","text":"OClocalnet-nncp.yaml <pre><code>oc apply -f https://examples.openshift.pub/pr-133/kubevirt/networking/localnet-nncp.yaml\n</code></pre> <pre><code>% oc get nncp,nnce\nNAME                                                     STATUS      REASON\nnodenetworkconfigurationpolicy.nmstate.io/localnet-coe   Available   SuccessfullyConfigured\n\nNAME                                                                      STATUS      STATUS AGE   REASON\nnodenetworkconfigurationenactment.nmstate.io/ocp1-worker-1.localnet-coe   Available   2s           SuccessfullyConfigured\nnodenetworkconfigurationenactment.nmstate.io/ocp1-worker-2.localnet-coe   Available   9s           SuccessfullyConfigured\nnodenetworkconfigurationenactment.nmstate.io/ocp1-worker-3.localnet-coe   Available   8s           SuccessfullyConfigured\n</code></pre> <pre><code>apiVersion: nmstate.io/v1\nkind: NodeNetworkConfigurationPolicy\nmetadata:\n  name: localnet-coe\nspec:\n  nodeSelector:\n    node-role.kubernetes.io/worker: ''\n  desiredState:\n    ovn:\n      bridge-mappings:\n        - localnet: localnet-coe\n          bridge: br-ex\n          state: present\n</code></pre>","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#apply-localnet-demo","title":"Apply localnet-demo","text":"","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#create-new-project","title":"Create new project","text":"<pre><code>oc new-project localnet-demo\n</code></pre>","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#create-net-attach-def","title":"Create net-attach-def","text":"OClocalnet-net-attach-def.yaml <pre><code>oc apply -f https://examples.openshift.pub/pr-133/kubevirt/networking/localnet-net-attach-def.yaml\n</code></pre> <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: coe\n  namespace: localnet-demo\nspec:\n  config: '{\n            \"name\":\"localnet-coe\",\n            \"type\":\"ovn-k8s-cni-overlay\",\n            \"cniVersion\":\"0.4.0\",\n            \"topology\":\"localnet\",\n            \"netAttachDefName\":\"localnet-demo/coe\"\n          }'\n</code></pre>","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#attach-fedora-vm","title":"Attach Fedora VM","text":"OClocalnet-fedora-vm.yaml <pre><code>oc apply -f https://examples.openshift.pub/pr-133/kubevirt/networking/localnet-fedora-vm.yaml\n</code></pre> <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: fedora\nspec:\n  runStrategy: Always\n  template:\n    spec:\n      domain:\n        devices:\n          disks:\n            - disk:\n                bus: virtio\n              name: containerdisk\n            - disk:\n                bus: virtio\n              name: cloudinit\n          rng: {}\n          interfaces:\n            - bridge: {}\n              model: virtio\n              name: coe\n        features:\n          acpi: {}\n          smm:\n            enabled: true\n        firmware:\n          bootloader:\n            efi:\n              secureBoot: true\n        resources:\n          requests:\n            memory: 1Gi\n      terminationGracePeriodSeconds: 180\n      networks:\n        - multus:\n            networkName: coe\n          name: coe\n      volumes:\n        - name: containerdisk\n          containerDisk:\n            image: quay.io/containerdisks/fedora:41\n        - name: cloudinit\n          cloudInitNoCloud:\n            networkData: |\n              version: 2\n              ethernets:\n                eth0:\n                  dhcp4: true\n            userData: |-\n              #cloud-config\n\n              users:\n                - name: coe\n                  lock_passwd: false\n                  # redhat // mkpasswd --method=SHA-512 --rounds=4096\n                  hashed_passwd: \"$6$rounds=4096$kmUERoUZHwzYfQMJ$G70T2Qg24d0XUhu.GTCH7Ia1F0B/B48JqIFdzVfigeMgfG5nsxp3dEWFKokfXGmhuetFXl4l41L8t1AZgEDW0.\"\n                  sudo: ['ALL=(ALL) NOPASSWD:ALL']\n                  chpasswd: { expire: False }\n                  groups: wheel\n                  shell: /bin/bash\n                  ssh_authorized_keys:\n                    - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIEQM82o2imwpHyGVO7DxCNbdE0ZWnkp6oxdawb7/MOCT coe-muc\n\n              packages:\n                - httpd\n\n              # install puppet (and dependencies); make sure apache and postgres\n              # both start at boot-time\n              runcmd:\n                - [ systemctl, enable, httpd.service ]\n                - [ systemctl, start, httpd.service ]\n</code></pre>","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#example-bonding-vlan-localnet-bridge","title":"Example: Bonding -&gt; VLAN -&gt; LocalNet &amp; Bridge","text":"<ul> <li>Tested with OpenShift 4.17.11</li> </ul> NMState for initial setup / add-node <pre><code>hosts:\n- hostname: inf49\n  rootDeviceHints:\n    deviceName: /dev/sda\n  interfaces:\n    - macAddress: b4:99:ba:b4:49:d2\n      name: enp3s0f0\n    - macAddress: 00:1b:21:b5:6a:20\n      name: ens2f0\n    - macAddress: 00:1b:21:b5:6a:21\n      name: ens2f1\n  networkConfig:\n    interfaces:\n      - name: enp3s0f0\n        type: ethernet\n        ipv6:\n          enabled: false\n        ipv4:\n          enabled: false\n      - name: bond0.32\n        type: vlan\n        state: up\n        ipv4:\n          enabled: true\n          dhcp: true\n        ipv6:\n          enabled: false\n        vlan:\n          base-iface: bond0\n          id: 32\n      - name: bond0\n        type: bond\n        state: up\n        link-aggregation:\n          mode: active-backup\n          options:\n            primary: ens2f0\n            miimon: '140'\n          port:\n          - ens2f0\n          - ens2f1\n</code></pre> NodeNetworkConfigurationPolicy (NNCP), create linux bridge connected to bond0 <pre><code>apiVersion: nmstate.io/v1\nkind: NodeNetworkConfigurationPolicy\nmetadata:\n  name: coe-bridge\nspec:\n  desiredState:\n    interfaces:\n    - bridge:\n        options:\n          stp:\n            enabled: false\n        port:\n        - name: bond0\n      name: coe-bridge\n      state: up\n      type: linux-bridge\n  nodeSelector:\n    bond0-available: \"true\"\n</code></pre> net-attach-def connect to bridge <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  annotations:\n    k8s.v1.cni.cncf.io/resourceName: bridge.network.kubevirt.io/coe-bridge\n  name: vlan1004\n  namespace: coe-bridge-test\nspec:\n  config: |-\n    {\n        \"cniVersion\": \"0.3.1\",\n        \"name\": \"vlan1004\",\n        \"type\": \"bridge\",\n        \"bridge\": \"coe-bridge\",\n        \"ipam\": {},\n        \"macspoofchk\": false,\n        \"preserveDefaultVlan\": false,\n        \"vlan\": 1004\n    }\n</code></pre>","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#example-ovn-bonding-balance-slb-ovn","title":"Example: OVN Bonding (balance-slb) (OVN)","text":"<ul> <li>Tested with OpenShift 4.18.13</li> </ul> <p>Warning</p> <p>Balance-slb is only supported with \"OVN Bonding\" and not with Linux Bonds!</p> NodeNetworkConfigurationPolicy, bond1 <pre><code>apiVersion: nmstate.io/v1\nkind: NodeNetworkConfigurationPolicy\nmetadata:\n  name: bond1\nspec:\n  desiredState:\n    interfaces:\n    - ipv4:\n        enabled: false\n      ipv6:\n        enabled: false\n      name: enp2s0\n      state: up\n      type: ethernet\n    - ipv4:\n        enabled: false\n      ipv6:\n        enabled: false\n      name: enp3s0\n      state: up\n      type: ethernet\n    - bridge:\n        allow-extra-patch-ports: true\n        port:\n        - name: patch-phy-to-ex\n        - link-aggregation:\n            mode: balance-slb\n            port:\n            - name: enp2s0\n            - name: enp3s0\n          name: ovs-bond\n      ipv4:\n        dhcp: false\n        enabled: false\n      ipv6:\n        dhcp: false\n        enabled: false\n      name: br-pub\n      state: up\n      type: ovs-bridge\n    ovn:\n      bridge-mappings:\n      - bridge: br-pub\n        localnet: localnet-pub\n        state: present\n  nodeSelector:\n    kubernetes.io/hostname: ocp1-worker-0\n</code></pre> NetworkAttachmentDefinition <p>Via YAML or WebUI</p> <p></p> <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  annotations:\n    k8s.ovn.org/network-id: '7'\n    k8s.ovn.org/network-name: localnet-pub\n  name: nad-localnet-pub\n  namespace: bonding-test\nspec:\n  config: |-\n    {\n        \"cniVersion\": \"0.4.0\",\n        \"name\": \"localnet-pub\",\n        \"type\": \"ovn-k8s-cni-overlay\",\n        \"netAttachDefName\": \"bonding-test/nad-localnet-pub\",\n        \"topology\": \"localnet\"\n    }\n</code></pre>","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#example-firewalling-isolation","title":"Example: Firewalling (Isolation)","text":"","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#enable-multinetworkpolicy","title":"Enable MultiNetworkPolicy","text":"<p>https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/networking/multiple-networks#nw-multi-network-policy-enable_configuring-multi-network-policy</p> <pre><code>oc patch network.operator.openshift.io cluster \\\n  --type=merge \\\n  -p '{\"spec\":{\"useMultiNetworkPolicy\":true}}'\n</code></pre> <p>Wait for the rollout / configuration</p> <pre><code>$ oc get co/network\nNAME      VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE\nnetwork   4.18.17   True        True          False      3d17h   DaemonSet \"/openshift-ovn-kubernetes/ovnkube-node\" update is rolling out (3 out of 6 updated)\n</code></pre>","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#create-two-vms-with-coe-network-connect","title":"Create two VM's with coe network connect","text":"OClocalnet-fedora-vm.yaml <pre><code>oc apply -f https://examples.openshift.pub/pr-133/kubevirt/networking/localnet-fedora-vm.yaml\n</code></pre> <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: fedora\nspec:\n  runStrategy: Always\n  template:\n    spec:\n      domain:\n        devices:\n          disks:\n            - disk:\n                bus: virtio\n              name: containerdisk\n            - disk:\n                bus: virtio\n              name: cloudinit\n          rng: {}\n          interfaces:\n            - bridge: {}\n              model: virtio\n              name: coe\n        features:\n          acpi: {}\n          smm:\n            enabled: true\n        firmware:\n          bootloader:\n            efi:\n              secureBoot: true\n        resources:\n          requests:\n            memory: 1Gi\n      terminationGracePeriodSeconds: 180\n      networks:\n        - multus:\n            networkName: coe\n          name: coe\n      volumes:\n        - name: containerdisk\n          containerDisk:\n            image: quay.io/containerdisks/fedora:41\n        - name: cloudinit\n          cloudInitNoCloud:\n            networkData: |\n              version: 2\n              ethernets:\n                eth0:\n                  dhcp4: true\n            userData: |-\n              #cloud-config\n\n              users:\n                - name: coe\n                  lock_passwd: false\n                  # redhat // mkpasswd --method=SHA-512 --rounds=4096\n                  hashed_passwd: \"$6$rounds=4096$kmUERoUZHwzYfQMJ$G70T2Qg24d0XUhu.GTCH7Ia1F0B/B48JqIFdzVfigeMgfG5nsxp3dEWFKokfXGmhuetFXl4l41L8t1AZgEDW0.\"\n                  sudo: ['ALL=(ALL) NOPASSWD:ALL']\n                  chpasswd: { expire: False }\n                  groups: wheel\n                  shell: /bin/bash\n                  ssh_authorized_keys:\n                    - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIEQM82o2imwpHyGVO7DxCNbdE0ZWnkp6oxdawb7/MOCT coe-muc\n\n              packages:\n                - httpd\n\n              # install puppet (and dependencies); make sure apache and postgres\n              # both start at boot-time\n              runcmd:\n                - [ systemctl, enable, httpd.service ]\n                - [ systemctl, start, httpd.service ]\n</code></pre>","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#lets-apply-some-multinetworkpolicy","title":"Let's apply some MultiNetworkPolicy","text":"deny-by-defaultallow-dns-and-default-gatewayallow-ingressallow-egress <pre><code>apiVersion: k8s.cni.cncf.io/v1beta1\nkind: MultiNetworkPolicy\nmetadata:\n  name: deny-by-default\n  namespace: localnet-demo\n  annotations:\n    k8s.v1.cni.cncf.io/policy-for: coe\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress: []\n  egress:  []\n</code></pre> <pre><code>apiVersion: k8s.cni.cncf.io/v1beta1\nkind: MultiNetworkPolicy\nmetadata:\n  name: allow-dns-and-default-gateway\n  namespace: localnet-demo\n  annotations:\n    k8s.v1.cni.cncf.io/policy-for: coe\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - ipBlock:\n        cidr: 10.32.96.1/32\n    - ipBlock:\n        cidr: 10.32.96.31/32\n    - ipBlock:\n        cidr: 10.32.111.254/32\n</code></pre> <pre><code>apiVersion: k8s.cni.cncf.io/v1beta1\nkind: MultiNetworkPolicy\nmetadata:\n  name: allow-ingress\n  namespace: localnet-demo\n  annotations:\n    k8s.v1.cni.cncf.io/policy-for: coe\nspec:\n  podSelector: {}\n  policyTypes:\n    - Ingress\n  ingress:\n    - from:\n      - ipBlock:\n          cidr: 0.0.0.0/0\n</code></pre> <pre><code>apiVersion: k8s.cni.cncf.io/v1beta1\nkind: MultiNetworkPolicy\nmetadata:\n  name: allow-egress\n  namespace: localnet-demo\n  annotations:\n    k8s.v1.cni.cncf.io/policy-for: coe\nspec:\n  podSelector: {}\n  policyTypes:\n    - Egress\n  egress:\n    - to:\n      - ipBlock:\n          cidr: 0.0.0.0/0\n</code></pre>","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#debugging-purpose","title":"Debugging purpose","text":"","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#create-br1-via-nmcli","title":"Create br1 via nmcli","text":"<pre><code>nmcli con show --active\nnmcli con add type bridge ifname br1 con-name br1\nnmcli con add type bridge-slave ifname ens3 master br1\nnmcli con modify br1 bridge.stp no\nnmcli con down 'Wired connection 1'\nnmcli con up br1\nnmcli con mod br1 connection.autoconnect yes\nnmcli con mod 'Wired connection 1' connection.autoconnect no\n</code></pre> <pre><code>[root@compute-0 ~]# nmcli con show\nNAME                UUID                                  TYPE      DEVICE\nbr1                 2ae82518-2ff3-4d49-b95c-fc8fbf029d48  bridge    br1\nbridge-slave-ens3   faac459f-ce51-4ce9-8616-ea9d23aff675  ethernet  ens3\nWired connection 1  e158d160-1743-3b00-9f67-258849993562  ethernet  --\n[root@compute-0 ~]# nmcli -f bridge con show br1\nbridge.mac-address:                     --\nbridge.stp:                             no\nbridge.priority:                        32768\nbridge.forward-delay:                   15\nbridge.hello-time:                      2\nbridge.max-age:                         20\nbridge.ageing-time:                     300\nbridge.group-forward-mask:              0\nbridge.multicast-snooping:              yes\nbridge.vlan-filtering:                  no\nbridge.vlan-default-pvid:               1\nbridge.vlans:                           --\n[root@compute-0 ~]# ip a show dev ens3\n2: ens3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel master br1 state UP group default qlen 1000\n    link/ether 52:54:00:a8:34:0d brd ff:ff:ff:ff:ff:ff\n[root@compute-0 ~]# ip a show dev br1\n17: br1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether 52:54:00:a8:34:0d brd ff:ff:ff:ff:ff:ff\n    inet 192.168.52.13/24 brd 192.168.52.255 scope global dynamic noprefixroute br1\n       valid_lft 3523sec preferred_lft 3523sec\n    inet6 fe80::70f0:71c5:53ea:71ee/64 scope link noprefixroute\n       valid_lft forever preferred_lft forever\n</code></pre>","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#connection-problem-with-kubevirtioallow-pod-bridge-network-live-migration-after-live-migration","title":"Connection problem with <code>kubevirt.io/allow-pod-bridge-network-live-migration</code> after live migration","text":"","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#hcp-cluster-sendling","title":"HCP Cluster sendling","text":"<pre><code>oc get nodes\nNAME                      STATUS   ROLES    AGE   VERSION\nsendling-d0c14274-6nbvl   Ready    worker   11d   v1.27.8+4fab27b\nsendling-d0c14274-sz7rb   Ready    worker   11d   v1.27.8+4fab27b\n</code></pre> Ping check details node/sendling-d0c14274-6nbvl <pre><code>oc debug node/sendling-d0c14274-6nbvl\nStarting pod/sendling-d0c14274-6nbvl-debug ...\nTo use host binaries, run `chroot /host`\nPod IP: 10.128.8.133\nIf you don't see a command prompt, try pressing enter.\nsh-4.4# ping www.google.de\nPING www.google.de (172.253.62.94) 56(84) bytes of data.\n64 bytes from bc-in-f94.1e100.net (172.253.62.94): icmp_seq=1 ttl=99 time=112 ms\n64 bytes from bc-in-f94.1e100.net (172.253.62.94): icmp_seq=2 ttl=99 time=98.3 ms\n^C\n--- www.google.de ping statistics ---\n2 packets transmitted, 2 received, 0% packet loss, time 1000ms\nrtt min/avg/max/mdev = 98.310/105.047/111.785/6.745 ms\nsh-4.4# exit\nexit\n\nRemoving debug pod ...\n</code></pre> Ping check details node/sendling-d0c14274-sz7rb <pre><code>$ oc debug node/sendling-d0c14274-sz7rb\nStarting pod/sendling-d0c14274-sz7rb-debug ...\nTo use host binaries, run `chroot /host`\nPod IP: 10.131.9.28\nIf you don't see a command prompt, try pressing enter.\nsh-4.4# ping www.google.de\nPING www.google.de (172.253.62.94) 56(84) bytes of data.\n</code></pre> <ul> <li>Node sendling-d0c14274-6nbvl - Ping google \u2705</li> <li>Node sendling-d0c14274-sz7rb - Ping google \u274c</li> </ul> <pre><code>$ oc get pods -l kubevirt.io=virt-launcher -o wide -n rbohne-hcp-sendling\nNAME                                          READY   STATUS      RESTARTS   AGE     IP             NODE                 NOMINATED NODE   READINESS GATES\nvirt-launcher-sendling-d0c14274-6nbvl-pb6zd   1/1     Running     0          6d2h    10.128.8.133   inf8                 &lt;none&gt;           1/1\nvirt-launcher-sendling-d0c14274-sz7rb-cw5vj   1/1     Running     0          3d20h   10.131.9.28    ucs-blade-server-1   &lt;none&gt;           1/1\nvirt-launcher-sendling-d0c14274-sz7rb-mbmv8   0/1     Completed   0          3d20h   10.131.9.28    ucs-blade-server-3   &lt;none&gt;           1/1\nvirt-launcher-sendling-d0c14274-sz7rb-nb25r   0/1     Completed   0          6d2h    10.131.9.28    ucs-blade-server-1   &lt;none&gt;           1/1\n$\n</code></pre>","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"kubevirt/networking/#checkout-node-routing","title":"Checkout node routing","text":"<p>Host subnets:</p> <pre><code>$ oc get nodes -o custom-columns=\"NODE:.metadata.name,node-subnets:.metadata.annotations.k8s\\.ovn\\.org/node-subnets\"\nNODE                 node-subnets\n...\ninf8                 {\"default\":[\"10.131.8.0/21\"]}\nucs-blade-server-1   {\"default\":[\"10.131.0.0/21\"]}\nucs-blade-server-3   {\"default\":[\"10.130.8.0/21\"]}\n...\n\n$ oc get pods -n openshift-ovn-kubernetes -o wide -l  app=ovnkube-node\nNAME                 READY   STATUS    RESTARTS       AGE    IP             NODE                 NOMINATED NODE   READINESS GATES\n...\novnkube-node-9xt5n   8/8     Running   8              2d7h   10.32.96.101   ucs-blade-server-1   &lt;none&gt;           &lt;none&gt;\novnkube-node-hhsx5   8/8     Running   8              2d7h   10.32.96.8     inf8                 &lt;none&gt;           &lt;none&gt;\novnkube-node-qx9bh   8/8     Running   9 (2d6h ago)   2d7h   10.32.96.103   ucs-blade-server-3   &lt;none&gt;           &lt;none&gt;\n...\n\n$ oc exec -n openshift-ovn-kubernetes -c ovn-controller ovnkube-node-9xt5n -- ovn-nbctl lr-route-list ovn_cluster_router\nIPv4 Routes\nRoute Table &lt;main&gt;:\n             10.128.8.133                100.88.0.9 dst-ip\n             10.129.8.107              10.129.8.107 dst-ip rtos-ucs-blade-server-1 ecmp\n             10.129.8.107                100.88.0.8 dst-ip ecmp\n             10.130.10.29              10.130.10.29 dst-ip rtos-ucs-blade-server-1\n              10.131.8.41               10.131.8.41 dst-ip rtos-ucs-blade-server-1\n              10.131.9.28               10.131.9.28 dst-ip rtos-ucs-blade-server-1 ecmp\n              10.131.9.28                100.88.0.8 dst-ip ecmp\n              10.131.9.44               10.131.9.44 dst-ip rtos-ucs-blade-server-1\n               100.64.0.2                100.88.0.2 dst-ip\n               100.64.0.3                100.88.0.3 dst-ip\n               100.64.0.4                100.88.0.4 dst-ip\n               100.64.0.5                100.64.0.5 dst-ip\n               100.64.0.6                100.88.0.6 dst-ip\n               100.64.0.8                100.88.0.8 dst-ip\n               100.64.0.9                100.88.0.9 dst-ip\n              100.64.0.10               100.88.0.10 dst-ip\n            10.128.0.0/21                100.88.0.2 dst-ip\n            10.128.8.0/21                100.88.0.6 dst-ip\n           10.128.16.0/21               100.88.0.10 dst-ip\n            10.129.0.0/21                100.88.0.3 dst-ip\n            10.130.0.0/21                100.88.0.4 dst-ip\n            10.130.8.0/21                100.88.0.8 dst-ip\n            10.131.8.0/21                100.88.0.9 dst-ip\n            10.128.0.0/14                100.64.0.5 src-ip\n\n$ oc exec -n openshift-ovn-kubernetes -c ovn-controller ovnkube-node-hhsx5   -- ovn-nbctl lr-route-list ovn_cluster_router\nIPv4 Routes\nRoute Table &lt;main&gt;:\n             10.128.8.133              10.128.8.133 dst-ip rtos-inf8\n             10.129.8.107                100.88.0.5 dst-ip ecmp\n             10.129.8.107                100.88.0.8 dst-ip ecmp\n             10.130.10.29                100.88.0.5 dst-ip\n              10.131.8.41                100.88.0.5 dst-ip\n              10.131.9.28                100.88.0.5 dst-ip ecmp\n              10.131.9.28                100.88.0.8 dst-ip ecmp\n              10.131.9.44                100.88.0.5 dst-ip\n               100.64.0.2                100.88.0.2 dst-ip\n               100.64.0.3                100.88.0.3 dst-ip\n               100.64.0.4                100.88.0.4 dst-ip\n               100.64.0.5                100.88.0.5 dst-ip\n               100.64.0.6                100.88.0.6 dst-ip\n               100.64.0.8                100.88.0.8 dst-ip\n               100.64.0.9                100.64.0.9 dst-ip\n              100.64.0.10               100.88.0.10 dst-ip\n            10.128.0.0/21                100.88.0.2 dst-ip\n            10.128.8.0/21                100.88.0.6 dst-ip\n           10.128.16.0/21               100.88.0.10 dst-ip\n            10.129.0.0/21                100.88.0.3 dst-ip\n            10.130.0.0/21                100.88.0.4 dst-ip\n            10.130.8.0/21                100.88.0.8 dst-ip\n            10.131.0.0/21                100.88.0.5 dst-ip\n            10.128.0.0/14                100.64.0.9 src-ip\n$\n\n$ oc exec -n openshift-ovn-kubernetes -c ovn-controller ovnkube-node-qx9bh -- ovn-nbctl lr-route-list ovn_cluster_router\nIPv4 Routes\nRoute Table &lt;main&gt;:\n             10.128.8.133                100.88.0.9 dst-ip\n             10.129.8.107                100.88.0.5 dst-ip\n             10.130.10.29                100.88.0.5 dst-ip\n              10.131.8.41                100.88.0.5 dst-ip\n              10.131.9.28                100.88.0.5 dst-ip\n              10.131.9.44                100.88.0.5 dst-ip\n               100.64.0.2                100.88.0.2 dst-ip\n               100.64.0.3                100.88.0.3 dst-ip\n               100.64.0.4                100.88.0.4 dst-ip\n               100.64.0.5                100.88.0.5 dst-ip\n               100.64.0.6                100.88.0.6 dst-ip\n               100.64.0.8                100.64.0.8 dst-ip\n               100.64.0.9                100.88.0.9 dst-ip\n              100.64.0.10               100.88.0.10 dst-ip\n            10.128.0.0/21                100.88.0.2 dst-ip\n            10.128.8.0/21                100.88.0.6 dst-ip\n           10.128.16.0/21               100.88.0.10 dst-ip\n            10.129.0.0/21                100.88.0.3 dst-ip\n            10.130.0.0/21                100.88.0.4 dst-ip\n            10.131.0.0/21                100.88.0.5 dst-ip\n            10.131.8.0/21                100.88.0.9 dst-ip\n            10.128.0.0/14                100.64.0.8 src-ip\n</code></pre>","tags":["v4.17","cnv","kubevirt","ocp-v","networking"]},{"location":"my-lab/","title":"My Lab notes","text":""},{"location":"my-lab/#content","title":"Content","text":"<ul> <li> <p>Hetzner Storage Box</p> </li> <li> <p>Local fedora</p> </li> <li> <p>SSH</p> </li> <li> <p>OCP Remote worker</p> </li> <li> <p>Workstation</p> </li> </ul>"},{"location":"my-lab/fedora-workstation/","title":"Fedora Workstation","text":"","tags":["laptop","fedora","workstation"]},{"location":"my-lab/fedora-workstation/#ssh-agent","title":"SSH Agent","text":"<pre><code>eval $(ssh-agent)\nssh-add ~/.ssh/..\n\nssh host\n</code></pre>","tags":["laptop","fedora","workstation"]},{"location":"my-lab/fedora-workstation/#ssh-agent-tmux","title":"SSH Agent &amp; tmux","text":"<p>Update some env. variables.</p> <pre><code>eval $(tmux show-env -s )\n</code></pre>","tags":["laptop","fedora","workstation"]},{"location":"my-lab/fedora-workstation/#google-chrome-sway-and-file-opener-problem","title":"Google Chrome, Sway and file opener problem","text":"<ul> <li>https://bugzilla.redhat.com/show_bug.cgi?id=2241173</li> <li>https://discussion.fedoraproject.org/t/blink-based-browsers-dont-open-file-dialog-in-sway-fedora-38/90403/2</li> </ul>","tags":["laptop","fedora","workstation"]},{"location":"my-lab/local-fedora/","title":"My local fedora VM - with VirtualBox","text":""},{"location":"my-lab/local-fedora/#install-virtualbox-guest-tools","title":"Install VirtualBox guest tools","text":"<pre><code>dnf -y update kernel*\ndnf -y install gcc kernel-devel kernel-headers dkms make bzip2 perl libxcrypt-compat\n</code></pre> <p>Attach VirtualBoxGuestAdditions ISO <pre><code>mkdir /media/VirtualBoxGuestAdditions\nmount -r /dev/cdrom /media/VirtualBoxGuestAdditions\n</code></pre></p> <p>Build <pre><code>export KERN_DIR=/usr/src/kernels/$(uname -r )\ncd /media/VirtualBoxGuestAdditions\n./VBoxLinuxAdditions.run\nreboot\n</code></pre></p>"},{"location":"my-lab/local-fedora/#setup-podman-remote","title":"Setup Podman remote","text":"<p>Source https://www.redhat.com/sysadmin/podman-clients-macos-windows</p>"},{"location":"my-lab/local-fedora/#on-fedora-as-root","title":"On Fedora as root","text":"<p>Sadly I had some problems to run with podman.socket: Durring a long running build I got <code>Error: unexpected EOF</code> :-(</p> <pre><code>dnf -y install podman\nsystemctl --now podman.socket\n\npodman --remote info\n</code></pre>"},{"location":"my-lab/local-fedora/#on-macos","title":"On MacOs","text":""},{"location":"my-lab/local-fedora/#install-podman-remote","title":"Install podman remote","text":"<pre><code>cd ~/bin/\n\nexport PODMAN_VERSION=$(ssh -q fedora podman version -f json | jq -r '.Client.Version')\ncurl -L -O https://github.com/containers/podman/releases/download/v${PODMAN_VERSION}/podman-remote-release-darwin.zip\nunzip podman-remote-release-darwin.zip podman\n</code></pre>"},{"location":"my-lab/local-fedora/#setup-connection","title":"Setup connection","text":"<pre><code>podman system connection add --identity ~/.ssh/id_ed25519 --port 1984 fedora root@127.0.0.1\n</code></pre>"},{"location":"my-lab/remote-worker/","title":"OpenShift Remote worker","text":"<p>These are my personal nodes, nothing more nothing less and of course not perfect.</p> <p></p>"},{"location":"my-lab/remote-worker/#install-wireguard-vpn-on-all-sites","title":"Install WireGuard VPN on all sites","text":"<pre><code>sudo yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm https://www.elrepo.org/elrepo-release-8.el8.elrepo.noarch.rpm\n\nsudo yum install kmod-wireguard wireguard-tools\n\n\nreboot # because of kernel modules\n\n\nwg genkey &gt; wg-private-key-$(hostname -f )\nchmod 600 wg-private-key-$(hostname -f )\ncat wg-private-key-$(hostname -f )\nwg pubkey &lt; wg-private-key-$(hostname -f ) &gt;  wg-public-key-$(hostname -f )\ncat wg-public-key-$(hostname -f )\n</code></pre> <p>Don't forgot to open the UDP port at your hetzner firewall and linux firewall: <code>firewall-cmd --zone=public --add-port 51820/udp --permanent &amp;&amp; firewall-cmd --reload</code></p> <p>Wireguard config on Hetzner server <pre><code>cat &gt; /etc/wireguard/wg0.conf &lt;&lt;EOF\n# host01.openshift.pub\n[Interface]\nAddress = 192.168.200.1/24\nPrivateKey = xxxx # Private key from hetzner\nListenPort = 51820\n\n\n# skull.egamting.bohne.io\n[Peer]\nPublicKey = xxxx # PubKey of your HomeLab\nAllowedIPs = 192.168.200.2/32,192.168.121.0/24\nPersistentKeepalive = 25\nEOF\n</code></pre></p> <p>Wireguard config on HomeLab</p> <pre><code>cat &gt; /etc/wireguard/wg0.conf &lt;&lt;EOF\n[Interface]\nAddress = 192.168.200.2/24\nPrivateKey =\nListenPort = 51820\n\n[Peer]\nPublicKey = xxxxx\nEndpoint = $HETZER_SERVER_IP$:51820\nAllowedIPs = 192.168.200.1/32,192.168.52.0/24\nPersistentKeepalive = 25\nEOF\n</code></pre> <p>Start service on both sites: <pre><code>systemctl enable --now wg-quick@wg0.service\n</code></pre></p> <p>Check Wireguard status wih <code>wg</code> commad.</p> <p>Add wg0 interface to trusted zone on all sites: <pre><code>firewall-cmd --zone=trusted --add-interface=wg0 --permanent\nfirewall-cmd --reload\n</code></pre></p>"},{"location":"my-lab/remote-worker/#prepare-hetzner-host-host01openshiftpub","title":"Prepare Hetzner Host (host01.openshift.pub)","text":"<p>Setup your cluster with hetzner-ocp4</p> <p>Stop cluster</p> <p>Adjust network, add forwarder <pre><code>&lt;forwarder domain='skull.egmating.bohne.io' addr='192.168.200.2'/&gt;\n</code></pre></p> <p>Setup DNS forwarder</p> <pre><code>cat &gt; /etc/dnsmasq.d/wireguard-dns-forwarder.conf &lt;&lt;EOF\ninterface=wg0\nlisten-address=192.168.200.1\nserver=/compute.local/192.168.52.1\nserver=/api-int.demo.openshift.pub/192.168.52.1\nno-resolv\nstrict-order\nexpand-hosts\nexcept-interface=lo\nEOF\n\nsystemctl enable --now dnsmasq\n</code></pre>"},{"location":"my-lab/remote-worker/#prepare-homelab-host-skull","title":"Prepare HomeLab Host (skull)","text":"<p>Install libvirt <code>yum install...</code></p> <p>Setup DNS forwarder <pre><code>cat &gt; /etc/dnsmasq.d/wireguard-dns-forwarder.conf &lt;&lt;EOF\nbind-interfaces\ninterface=wg0\nlisten-address=192.168.200.2\nserver=/skull.egmating.bohne.io/192.168.121.1\nno-resolv\nstrict-order\nexpand-hosts\nexcept-interface=lo\nEOF\n\nsystemctl enable --now dnsmasq\n</code></pre></p> <p>Create remote worker network</p> <pre><code>cat - &gt; network-remote-worker.xml &lt;&lt;EOF\n\n&lt;network&gt;\n  &lt;name&gt;remote-worker&lt;/name&gt;\n  &lt;uuid&gt;3f22931e-fcf7-437f-9085-d8adcab53451&lt;/uuid&gt;\n  &lt;forward mode='nat'&gt;\n    &lt;nat&gt;\n      &lt;port start='1024' end='65535'/&gt;\n    &lt;/nat&gt;\n  &lt;/forward&gt;\n  &lt;bridge name='virbr1' stp='on' delay='0'/&gt;\n  &lt;mac address='52:54:00:4d:92:bf'/&gt;\n  &lt;domain name='skull.egmating.bohne.io'/&gt;\n  &lt;dns&gt;\n    &lt;forwarder domain='compute.local' addr='192.168.200.1'/&gt;\n    &lt;forwarder domain='api-int.demo.openshift.pub' addr='192.168.200.1'/&gt;\n    &lt;host ip='192.168.121.2'&gt;\n      &lt;hostname&gt;remote-worker.skull.egmating.bohne.io&lt;/hostname&gt;\n    &lt;/host&gt;\n  &lt;/dns&gt;\n  &lt;ip address='192.168.121.1' netmask='255.255.255.0'&gt;\n    &lt;dhcp&gt;\n      &lt;range start='192.168.121.200' end='192.168.121.254'/&gt;\n      &lt;host mac='52:54:00:a8:79:02' name='remote-worker.skull.egmating.bohne.io' ip='192.168.121.2'/&gt;\n    &lt;/dhcp&gt;\n  &lt;/ip&gt;\n&lt;/network&gt;\nEOF\n\nvirsh net-create network-remote-worker.xml\n</code></pre> <p>Download CoreOS <pre><code>cd /var/lib/libvirt/images/\ncurl -L -O https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.7/latest/rhcos-4.7.0-x86_64-live.x86_64.iso\n</code></pre></p> <p>Create remote worker VM boot into live iso</p> <pre><code>qemu-img create -f qcow2 /var/lib/libvirt/images/remote-worker.qcow2 120G\nvirt-install \\\n  --name remote-worker \\\n  --memory 8192 --vcpus 4 \\\n  --disk /var/lib/libvirt/images/remote-worker.qcow2 \\\n  --os-variant rhel8.0 \\\n  --cdrom /var/lib/libvirt/images/rhcos-4.7.0-x86_64-live.x86_64.iso \\\n  --network network=remote-worker,mac=52:54:00:a8:79:02 \\\n  --console pty,target_type=virtio\n</code></pre> <p>At CoreOS Live ISO <pre><code># Use curl to avoid SSL chizzel\n\ncurl -k -o worker.ign https://api-int.demo.openshift.pub:22623/config/worker\n\nsudo coreos-installer install /dev/vda \\\n  --ignition-file worker.ign\n\nreboot\n</code></pre></p> <p>Approve CSR and debug errors :-)</p>"},{"location":"my-lab/ssh/","title":"X-Forward","text":""},{"location":"my-lab/ssh/#server","title":"Server","text":"<ul> <li>X11Forwarding enabled?</li> <li>xauth installed?</li> </ul> <pre><code>grep X11Forwarding /etc/ssh/sshd_config\nX11Forwarding yes\n\n# type xauth\nxauth is /usr/bin/xauth\n</code></pre>"},{"location":"my-lab/ssh/#client","title":"Client","text":""},{"location":"my-lab/ssh/#ssh-agent","title":"SSH Agent","text":"<pre><code>eval $(ssh-agent)\nssh-add ~/.ssh/..\n\nssh host\n</code></pre>"},{"location":"my-lab/ssh/#ssh-agent-tmux","title":"SSH Agent &amp; tmux","text":"<p>Update some env. variables.</p> <pre><code>eval $(tmux show-env -s )\n</code></pre>"},{"location":"my-lab/storage-box/","title":"Hetzner Storage Box","text":"<p>For all example we use <code>HETZNER_STORAGE_USERNAME</code> and <code>HETZNER_STORAGE_PASSWORD</code> environment variables.</p> <pre><code>export HETZNER_STORAGE_USERNAME=..\nexport HETZNER_STORAGE_PASSWORD=..\n</code></pre>"},{"location":"my-lab/storage-box/#via-webdav","title":"via WebDav","text":"Action Command List <code>curl -u ${HETZNER_STORAGE_USERNAME}:${HETZNER_STORAGE_PASSWORD} https://${HETZNER_STORAGE_USERNAME}.your-storagebox.de/</code> Upload <code>curl -u ${HETZNER_STORAGE_USERNAME}:${HETZNER_STORAGE_PASSWORD}  -T '/path/to/local/file.txt' https://${HETZNER_STORAGE_USERNAME}.your-storagebox.de/</code>"},{"location":"my-lab/storage-box/#via-sftp","title":"via SFTP","text":"<pre><code>sftp -P 23 ${HETZNER_STORAGE_USERNAME}@${HETZNER_STORAGE_USERNAME}.your-storagebox.de\n</code></pre>"},{"location":"my-lab/storage-box/#via-rsync","title":"via rsync","text":"<pre><code>rsync --progress -e 'ssh -p23' --recursive ${HETZNER_STORAGE_USERNAME}@${HETZNER_STORAGE_USERNAME}.your-storagebox.de:homer/root/hetzner-ocp4 /root/\n\nrsync --progress -e 'ssh -p23' --recursive ${HETZNER_STORAGE_USERNAME}@${HETZNER_STORAGE_USERNAME}.your-storagebox.de:homer/root/hetzner-ocp4 /root/\n\nrsync --progress -e 'ssh -p23' --recursive ${HETZNER_STORAGE_USERNAME}@${HETZNER_STORAGE_USERNAME}.your-storagebox.de:homer/images /var/lib/libvirt/images\n\nrsync --progress -e 'ssh -p23' --recursive ${HETZNER_STORAGE_USERNAME}@${HETZNER_STORAGE_USERNAME}.your-storagebox.de:homer/root/cka /var/lib/libvirt/images\n</code></pre>"},{"location":"my-lab/storage-box/#backup-example","title":"Backup example","text":"<p>Setup SSH key-auth: https://docs.hetzner.com/de/robot/storage-box/backup-space-ssh-keys/</p> <pre><code>echo -e \"mkdir /.ssh \\n chmod 700 .ssh \\n put /root/.ssh/id_rsa.pub .ssh/authorized_keys \\n chmod 600 .ssh/authorized_keys\" | sftp -P 23 ${HETZNER_STORAGE_USERNAME}@${HETZNER_STORAGE_USERNAME}.your-storagebox.de\n</code></pre> <p>Prep for backup <pre><code>echo -e \"mkdir $(hostname)\" | sftp -P 23 ${HETZNER_STORAGE_USERNAME}@${HETZNER_STORAGE_USERNAME}.your-storagebox.de\n</code></pre></p> <p>Run backup</p> <pre><code>rsync --progress -e 'ssh -p23' --recursive \\\n  --exclude '.vscode-server' \\\n  --exclude '.cache' \\\n  --exclude '.kube/cache' \\\n  /root \\\n  ${HETZNER_STORAGE_USERNAME}@${HETZNER_STORAGE_USERNAME}.your-storagebox.de:$(hostname)-$(date +%F)\n</code></pre>"},{"location":"my-lab/storage-box/#restore","title":"Restore","text":"<pre><code># Select backup\n$ echo -e \"ls\"  | sftp -P 23 ${HETZNER_STORAGE_USERNAME}@${HETZNER_STORAGE_USERNAME}.your-storagebox.de\nsftp&gt; ls\nhomer               host01              host01-2020-12-18\n\n$ export RESTORE_FROM=host01-2020-12-18\n\n$ rsync --progress -e 'ssh -p23' --recursive \\\n  --exclude '.vscode-server' \\\n  --exclude '.cache' \\\n  --exclude '.ssh/authorized_keys' \\\n  ${HETZNER_STORAGE_USERNAME}@${HETZNER_STORAGE_USERNAME}.your-storagebox.de:${RESTORE_FROM}/root/ \\\n  /root/\n</code></pre>"},{"location":"my-lab/storage-box/#partial-restore","title":"Partial restore:","text":"<p>Rsync filter example: <pre><code>\"*\"         means everything\n\"dir1\"      transfers empty directory [dir1]\n\"dir*\"      transfers empty directories like: \"dir1\", \"dir2\", \"dir3\", etc...\n\"file*\"     transfers files whose names start with [file]\n\"dir**\"     transfers every path that starts with [dir] like \"dir1/file.txt\", \"dir2/bar/ffaa.html\", etc...\n\"dir***\"    same as above\n\"dir1/*\"    does nothing\n\"dir1/**\"   does nothing\n\"dir1/***\"  transfers [dir1] directory and all its contents like \"dir1/file.txt\", \"dir1/fooo.sh\", \"dir1/fold/baar.py\", etc...\n</code></pre></p> <p>The exclude and include order is very important!</p>"},{"location":"my-lab/storage-box/#ssh-keys-only","title":"SSH Keys only","text":"<pre><code>export RESTORE_FROM=host01-2020-12-18\nrsync --progress -avz -e 'ssh -p23' --recursive \\\n  --exclude='known_hosts' \\\n  --exclude='authorized_keys' \\\n  --include='.ssh**' \\\n  --exclude='*' \\\n  ${HETZNER_STORAGE_USERNAME}@${HETZNER_STORAGE_USERNAME}.your-storagebox.de:${RESTORE_FROM}/root/ \\\n  /root/\n\nchown -R root:root ~\nchmod -R 600 ~/.ssh/\n</code></pre>"},{"location":"my-lab/storage-box/#cluster-configs","title":"Cluster-configs","text":"<pre><code>export RESTORE_FROM=host01-2020-12-18\nrsync --progress -avz -e 'ssh -p23' --recursive  \\\n  --include='hetzner-ocp4/cluster.yml' \\\n  --include='hetzner-ocp4/cluster-*.yaml' \\\n  --include='hetzner-ocp4/' \\\n  --exclude='*' \\\n  ${HETZNER_STORAGE_USERNAME}@${HETZNER_STORAGE_USERNAME}.your-storagebox.de:${RESTORE_FROM}/root/ \\\n  /root/\n</code></pre>"},{"location":"my-lab/storage-box/#create-mirror-of-mirroropenshiftpub","title":"Create mirror of mirror.openshift.pub","text":"<pre><code>export\n\nOCP Mirrror:\n\nexport HETZNER_STORAGE_USERNAME=u221214-sub5\nexport HETZNER_STORAGE_PASSWORD=2a51mq474OG2Lo6b\n\nsftp -P 23 ${HETZNER_STORAGE_USERNAME}@${HETZNER_STORAGE_USERNAME}.your-storagebox.de\n\nmkdir -p https://mirror.openshift.com/pub/openshift-v4/clients/ocp\nmkdir -p openshift-v4/dependencies/rhcos\nmkdir -p openshift-v4/clients/helm\n\n\nyour-storagebox.de\n\ncurl -T '/path/to/local/file.txt' 'https://example.com/test/'\n\ncurl -XGET -u $HUSER:$HPASS https://${HUSER}.your-storagebox.de/\n\ncurl -XGET -u $HUSER:$HPASS https://${HUSER}.your-storagebox.de/\n</code></pre> <p>export CLIENT=4.6.1 export RHCOS=4.6.1</p> <p>wget --mirror   https://mirror.openshift.com/pub/openshift-v4/clients/ocp/${CLIENT}/openshift-install-linux-${CLIENT}.tar.gz   https://mirror.openshift.com/pub/openshift-v4/clients/ocp/${CLIENT}/openshift-install-linux-${CLIENT}.tar.gz openshift-client-linux-4.6.1.tar.gz)</p>"},{"location":"networking/","title":"Networking","text":""},{"location":"networking/#content","title":"Content","text":"<ul> <li> <p>User Defined Networks</p> </li> <li> <p>Services &amp; Routes</p> </li> <li> <p>Multus</p> </li> <li> <p>Network Policy</p> </li> <li> <p>Router sharding</p> </li> <li> <p>Egress IP</p> </li> <li> <p>Kubernetes iptables chains</p> </li> <li> <p>LLDPd</p> </li> </ul>"},{"location":"networking/k8s-iptables-chains/","title":"Kubernetes iptables chains","text":"<p>Documentation</p>"},{"location":"networking/k8s-iptables-chains/#kube-services-chain-containts-all-services","title":"KUBE-SERVICES (CHAIN) Containts all Services","text":"<pre><code>   -A PREROUTING -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\n   -A OUTPUT -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\n\n   -A KUBE-SERVICES ! -s 10.30.0.0/16 -d 172.30.229.64/32 -p tcp -m comment --comment \"simpson/homer:8080-tcp cluster IP\" -m tcp --dport 8080 -j KUBE-MARK-MASQ\n   -A KUBE-SERVICES -d 172.30.229.64/32 -p tcp -m comment --comment \"simpson/homer:8080-tcp cluster IP\" -m tcp --dport 8080 -j KUBE-SVC-UNWFIWS7624VL6XI\n</code></pre>"},{"location":"networking/k8s-iptables-chains/#kube-svc-the-loadbalance-every-services-has-an-kube-svc-entry","title":"KUBE-SVC-* the \"LoadBalance\" Every services has an KUBE-SVC entry","text":"<pre><code>   -A KUBE-SVC-UNWFIWS7624VL6XI -m comment --comment \"simpson/homer:8080-tcp\" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-HKFKWO5PFFO7Z3EC\n   -A KUBE-SVC-UNWFIWS7624VL6XI -m comment --comment \"simpson/homer:8080-tcp\" -j KUBE-SEP-N3H7ASEQYKKEG23L\n</code></pre>"},{"location":"networking/k8s-iptables-chains/#kube-sep-the-service-endpoint-every-endpoint-of-an-service-oc-get-endpoints","title":"KUBE-SEP-* the Service EndPoint Every endpoint of an service. (<code>oc get endpoints</code>)","text":"<pre><code>-A KUBE-SEP-HKFKWO5PFFO7Z3EC -s 10.30.2.88/32 -m comment --comment \"simpson/homer:8080-tcp\" -j KUBE-MARK-MASQ\n-A KUBE-SEP-HKFKWO5PFFO7Z3EC -p tcp -m comment --comment \"simpson/homer:8080-tcp\" -m tcp -j DNAT --to-destination 10.30.2.88:8080\n\n-A KUBE-SEP-N3H7ASEQYKKEG23L -s 10.30.3.62/32 -m comment --comment \"simpson/homer:8080-tcp\" -j KUBE-MARK-MASQ\n-A KUBE-SEP-N3H7ASEQYKKEG23L -p tcp -m comment --comment \"simpson/homer:8080-tcp\" -m tcp -j DNAT --to-destination 10.30.3.62:8080\n</code></pre>"},{"location":"networking/k8s-iptables-chains/#netfilter-iptables-package-flow","title":"NetFilter / IPTables Package flow","text":"<p>Source: http://xkr47.outerspace.dyndns.org/netfilter/packet_flow/</p>"},{"location":"networking/multus/","title":"Multus","text":"<p>Notice</p> <p>Not comlete yet....</p> <ul> <li>Introduction to Linux interfaces for virtual networking</li> <li>OpenShift Documentation: Understanding multiple networks</li> </ul>"},{"location":"networking/multus/#ipvlan-example","title":"ipvlan example","text":"<p>Example based on my lab build with hetzner-ocp4</p>"},{"location":"networking/multus/#configure-additional-network","title":"Configure additional network","text":"<pre><code>oc edit networks.operator.openshift.io cluster\n</code></pre> <p>Add <code>additionalNetworks</code>: <pre><code>  additionalNetworks:\n  - name: extra-network-1\n    namespace: cni-test\n    simpleMacvlanConfig:\n      ipamConfig:\n        type: DHCP\n    type: SimpleMacvlan\n</code></pre></p> <p>Check the network attachment definitions: <pre><code>$ oc get network-attachment-definitions/extra-network-1 -n cni-test\nNAME              AGE\nextra-network-1   14h\n</code></pre></p>"},{"location":"networking/multus/#create-a-pod","title":"Create a pod","text":"<pre><code>oc create -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: multus\n  namespace: cni-test\n  annotations:\n    k8s.v1.cni.cncf.io/networks: extra-network-1\nspec:\n  containers:\n    - name: rhel\n      image: registry.access.redhat.com/rhel7/rhel-tools\n      command: [ \"/bin/sh\", \"-c\", \"while true ; do date; sleep 10; done;\" ]\n  restartPolicy: Never\nEOF\n</code></pre>"},{"location":"networking/egress-ip/","title":"Some information","text":"<p>Official documentation: https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/ovn-kubernetes_network_plugin/configuring-egress-ips-ovn</p> <p>Tested with:</p> Component Version OpenShift v4.19.14","tags":["Egress","v4.19"]},{"location":"networking/egress-ip/#prepare-cluster","title":"Prepare cluster","text":"<pre><code>% oc get nodes -l node-role.kubernetes.io/worker\nNAME                       STATUS   ROLES    AGE   VERSION\nocp1-worker-0              Ready    worker   79d   v1.32.5\nocp1-worker-1              Ready    worker   79d   v1.32.5\nocp1-worker-2              Ready    worker   79d   v1.32.5\n\n% oc label node/ocp1-worker-0  k8s.ovn.org/egress-assignable=\"\"\nnode/ocp1-worker-0 labeled\n% oc label node/ocp1-worker-1  k8s.ovn.org/egress-assignable=\"\"\nnode/ocp1-worker-1 labeled\n% oc label node/ocp1-worker-2  k8s.ovn.org/egress-assignable=\"\"\nnode/ocp1-worker-2 labeled\n\noc apply -f - &lt;&lt;EOF\nheredoc&gt; apiVersion: k8s.ovn.org/v1\nkind: EgressIP\nmetadata:\n  name: egress-coe\nspec:\n  egressIPs:\n  - 10.32.105.72\n  - 10.32.105.73\n  namespaceSelector:\n    matchLabels:\n      egress: coe\nheredoc&gt; EOF\negressip.k8s.ovn.org/egress-coe created\n</code></pre>","tags":["Egress","v4.19"]},{"location":"networking/egress-ip/#deployment","title":"Deployment","text":"<pre><code>oc new-project rbohne-egress\noc deploy -k simple-nginx...\noc rsh deployment/simple-nginx\ncurl $WEBSERVER\n\noc label namespace/rbohne-egress egress=coe\n</code></pre> <pre><code>10.32.96.44 - - [31/Aug/2025:11:54:00 +0200] \"GET / HTTP/1.1\" 301 247 \"-\" \"curl/7.76.1\"\n10.32.105.72 - - [31/Aug/2025:11:56:31 +0200] \"GET / HTTP/1.1\" 301 247 \"-\" \"curl/7.76.1\"\n</code></pre>","tags":["Egress","v4.19"]},{"location":"networking/egress-ip/#check-ip-on-the-node","title":"Check IP on the node","text":"<pre><code>sh-5.1# ip -br a show dev br-ex\nbr-ex            UNKNOWN        10.32.105.69/20 169.254.0.2/17 10.32.105.72/32\nsh-5.1#\n</code></pre>","tags":["Egress","v4.19"]},{"location":"networking/lldpd/","title":"Run LLDPd as DaemonSet","text":"<p>Tested with:</p> Component Version OpenShift v4.18.14","tags":["lldp","v4.18"]},{"location":"networking/lldpd/#create-projectnamespace","title":"Create project/namespace","text":"<p>Info</p> <p>Empty node selector, to run the lldpd on all nodes</p> OCnamespace.yaml <pre><code>oc apply -f https://examples.openshift.pub/pr-133/networking/lldpd/namespace.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  annotations:\n    openshift.io/node-selector: \"\"\n  name: infra-lldpd\nspec: {}\n</code></pre>","tags":["lldp","v4.18"]},{"location":"networking/lldpd/#build-the-lldpd-image-rhel-image","title":"Build the LLDPd Image (RHEL Image)","text":"<ul> <li>To install and configure entitelt builds, please follo: Entitled builds and OpenShift 4</li> </ul> OCbuild.yaml <pre><code>oc apply -f https://examples.openshift.pub/pr-133/networking/lldpd/build.yaml\n</code></pre> <pre><code>apiVersion: shipwright.io/v1beta1\nkind: Build\nmetadata:\n  name: lldpd\n  namespace: infra-lldpd\nspec:\n  source:\n    type: Git\n    git:\n      url: https://github.com/openshift-examples/web\n    contextDir: content/networking/lldp\n  strategy:\n    name: buildah\n    kind: ClusterBuildStrategy\n  paramValues:\n    - name: dockerfile\n      value: Containerfile\n  volumes:\n    - csi:\n        driver: csi.sharedresource.openshift.io\n        readOnly: true\n        volumeAttributes:\n          sharedSecret: etc-pki-entitlement\n      name: etc-pki-entitlement\n  output:\n    image: image-registry.openshift-image-registry.svc:5000/infra-lldpd/lldpd:latest\n</code></pre>","tags":["lldp","v4.18"]},{"location":"networking/lldpd/#run-lldpd","title":"Run lldpd","text":"OCdaemonset.yaml <pre><code>oc apply -f https://examples.openshift.pub/pr-133/networking/lldpd/daemonset.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: lldpd\n  namespace: infra-lldpd\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: system:openshift:scc:privileged\n  namespace: infra-lldpd\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:openshift:scc:privileged\nsubjects:\n  - kind: ServiceAccount\n    name: lldpd\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: lldpd\n  namespace: infra-lldpd\nspec:\n  selector:\n    matchLabels:\n      name: lldpd\n  template:\n    metadata:\n      annotations:\n        openshift.io/required-scc: \"privileged\"\n      labels:\n        name: lldpd\n    spec:\n      imagePullSecrets:\n        - name: rbohne-rhel-lldp-pull-secret\n      serviceAccountName: lldpd\n      tolerations:\n        - key: \"node-role.kubernetes.io/master\"\n          operator: \"Exists\"\n          effect: \"NoSchedule\"\n      containers:\n        - name: lldpd\n          image: image-registry.openshift-image-registry.svc:5000/infra-lldpd/lldpd:latest\n          env:\n            - name: DAEMON_ARGS\n              value: \"-l -c\"\n          securityContext:\n            capabilities:\n              add: [\"NET_RAW\", \"NET_ADMIN\", \"SYS_CHROOT\"]\n      hostPID: true\n      hostNetwork: true\n</code></pre>","tags":["lldp","v4.18"]},{"location":"networking/lldpd/#check-lldpd","title":"Check lldpd","text":"Run daemonset/lldpd lldpcli show neighbors <pre><code>oc rsh daemonset/lldpd lldpcli show neighbors\n-------------------------------------------------------------------------------\nLLDP neighbors:\n-------------------------------------------------------------------------------\nInterface:    enp79s0f0, via: LLDP, RID: 1, Time: 0 day, 00:01:16\nChassis:\n    ChassisID:    mac 00:1c:73:7f:b4:d7\n    SysName:      arista-rj45.coe.muc.redhat.com\n    SysDescr:     Arista Networks EOS version 4.27.0F running on an Arista Networks DCS-7050TX-64\n    MgmtIP:       10.32.104.2\n    MgmtIface:    999001\n    Capability:   Bridge, on\n    Capability:   Router, off\nPort:\n    PortID:       ifname Ethernet20\n    TTL:          120\n-------------------------------------------------------------------------------\nInterface:    enp79s0f1, via: LLDP, RID: 1, Time: 0 day, 00:01:17\nChassis:\n    ChassisID:    mac 00:1c:73:7f:b4:d7\n    SysName:      arista-rj45.coe.muc.redhat.com\n    SysDescr:     Arista Networks EOS version 4.27.0F running on an Arista Networks DCS-7050TX-64\n    MgmtIP:       10.32.104.2\n    MgmtIface:    999001\n    Capability:   Bridge, on\n    Capability:   Router, off\nPort:\n    PortID:       ifname Ethernet15\n    TTL:          120\n-------------------------------------------------------------------------------\nInterface:    veth543afbbd, via: CDPv2, RID: 3, Time: 0 day, 00:01:05\nChassis:\n    ChassisID:    local ocp1-worker-0.ocp1.stormshift.coe.muc.redhat.com\n    SysName:      ocp1-worker-0.ocp1.stormshift.coe.muc.redhat.com\n    SysDescr:     Linux running on\n                Red Hat Enterprise Linux 9.6 (Plow) Linux 5.14.0-427.72.1.el9_4.x86_64 #1 SMP PREEMPT_DYNAMIC Fri May 30 09:53:21 EDT 2025 x86_64\n    MgmtIP:       10.131.0.2\n    Capability:   Station, on\nPort:\n    PortID:       ifname enp1s0\n    PortDescr:    enp1s0\n    TTL:          120\n-------------------------------------------------------------------------------\n</code></pre> Run lldp neighbors at switch <pre><code>arista-rj45#show lldp neighbors\nLast table change time   : 0:02:13 ago\nNumber of table inserts  : 80\nNumber of table deletes  : 53\nNumber of table drops    : 0\nNumber of table age-outs : 1\n\nPort            Neighbor Device ID                   Neighbor Port ID    TTL\n------------ ------------------------------------ ---------------------- ---\nEt1             sw01-dist-lab.muc.redhat.com         ge-0/0/1            120\nEt9             ucs56.isar.coe.muc.redhat.com        80e0.1d36.ffac      120\nEt11            ecf4.bbf1.0470                       00f4.bbf1.0470      120\nEt13            storm2.isar.coe.muc.redhat.com       ecf4.bbf1.6b28      120\nEt15            ucs55.isar.coe.muc.redhat.com        70e4.22c4.290c      120\nEt17            ceph10.isar.coe.muc.redhat.com       a036.9f07.ffff      120\nEt19            ceph11.isar.coe.muc.redhat.com       a036.9f07.ffff      120\nEt20            ucs55.isar.coe.muc.redhat.com        70e4.22c4.290a      120\nEt21            ceph12.isar.coe.muc.redhat.com       a036.9f06.8e2e      120\nEt22            ucs56.isar.coe.muc.redhat.com        80e0.1d36.ffae      120\nEt24            storm6.isar.coe.muc.redhat.com       a036.9fe5.0d9e      120\nEt31            storm3.isar.coe.muc.redhat.com       ecf4.bbf1.4ef2      120\nEt32            storm5.isar.coe.muc.redhat.com       a036.9fe5.14bc      120\nEt33            storm2.isar.coe.muc.redhat.com       ecf4.bbf1.6b2a      120\nEt34            storm5.isar.coe.muc.redhat.com       a036.9fe5.14be      120\nEt35            ucs57.isar.coe.muc.redhat.com        58ac.7803.18b2      120\nEt36            ucs57.isar.coe.muc.redhat.com        58ac.7803.18b0      120\nEt37            storm6.isar.coe.muc.redhat.com       a036.9fe5.0d9c      120\nEt40            storm3.isar.coe.muc.redhat.com       ecf4.bbf1.4ef0      120\nEt41            ceph11.isar.coe.muc.redhat.com       a036.9f07.032c      120\nEt45            ceph10.isar.coe.muc.redhat.com       a036.9f07.04c0      120\nEt47            ceph12.isar.coe.muc.redhat.com       a036.9f06.8e2c      120\nEt52/1          arista-sfp.coe.muc.redhat.com        Ethernet19/1        120\nEt52/2          arista-sfp.coe.muc.redhat.com        Ethernet19/2        120\nEt52/3          arista-sfp.coe.muc.redhat.com        Ethernet19/3        120\nEt52/4          arista-sfp.coe.muc.redhat.com        Ethernet19/4        120\nMa1             sw01-dist-lab.muc.redhat.com         ge-0/0/43           120\n</code></pre>","tags":["lldp","v4.18"]},{"location":"networking/network-policy/","title":"Network Policy","text":"<p>Official documentation: About network policy </p>"},{"location":"networking/network-policy/#basics","title":"Basics","text":"<ol> <li>Based on labeling or annotations</li> <li>Empty label selector match all</li> <li>Rules for allowing<ul> <li>Ingress -&gt; who can connect to this POD</li> <li>Egress -&gt; where can this POD connect to</li> </ul> </li> <li>Rules<ul> <li>traffic is allowed unless a Network Policy selecting the POD</li> <li>traffic is denied if pod is selected in policie but none of them have any rules allowing it</li> <li>=&gt; You can only write rules that allow traffic!</li> <li>Scope: Namespace</li> </ul> </li> </ol>"},{"location":"networking/network-policy/#demo-network-policies","title":"Demo Network Policies","text":"<p>Please check</p> <ul> <li>OVNKubernetes</li> <li>OpenShiftSDN</li> </ul>"},{"location":"networking/network-policy/OVNKubernetes/","title":"Network Policy with OVNKubernetes","text":"<p>Info</p> <p>Work in progress not ready yet!</p>","tags":["NetworkPolicy","OVNKubernetes"]},{"location":"networking/network-policy/OVNKubernetes/#nice-to-know-basics","title":"Nice to know / Basics","text":"<ol> <li>Based on labeling or annotations<ul> <li>project / namespaces seldom have labels :-/</li> </ul> </li> <li>Empty label selector match all</li> <li>Rules for allowing<ul> <li>Ingress -&gt; who can connect to this POD</li> <li>Egress -&gt; where can this POD connect to</li> </ul> </li> <li>Rules<ul> <li>traffic is allowed unless a Network Policy selecting the POD</li> <li>traffic is denied if pod is selected in policie but none of them have any rules allowing it</li> <li>=  You can only write rules that allow traffic!</li> <li>Scope: Namespace</li> </ul> </li> </ol>","tags":["NetworkPolicy","OVNKubernetes"]},{"location":"networking/network-policy/OVNKubernetes/#tutorial-demo-openshift-v4","title":"Tutorial / Demo - OpenShift v4!","text":"","tags":["NetworkPolicy","OVNKubernetes"]},{"location":"networking/network-policy/OVNKubernetes/#deploy-demo-environment","title":"Deploy demo environment","text":"<pre><code>oc new-project bouvier\noc new-app quay.io/openshift-examples/simple-http-server:micro --name patty\noc create route edge patty --service=patty\noc new-app quay.io/openshift-examples/simple-http-server:micro --name selma\noc create route edge selma --service=selma\n\noc new-project simpson\noc new-app quay.io/openshift-examples/simple-http-server:micro --name homer\noc create route edge homer --service=homer\n\noc new-app quay.io/openshift-examples/simple-http-server:micro --name marge\noc create route edge marge --service=marge\n</code></pre>","tags":["NetworkPolicy","OVNKubernetes"]},{"location":"networking/network-policy/OVNKubernetes/#download-some-helper-scripts","title":"Download some helper scripts","text":"<pre><code>git clone https://github.com/openshift-examples/network-policies-tests.git\ncd network-policies-tests/\n</code></pre>","tags":["NetworkPolicy","OVNKubernetes"]},{"location":"networking/network-policy/OVNKubernetes/#run-connection-overview","title":"Run connection overview","text":"<p><pre><code>./run-tmux.sh apps.&lt;cluster_name&gt;.&lt;base_domain&gt;\n</code></pre> </p>","tags":["NetworkPolicy","OVNKubernetes"]},{"location":"networking/network-policy/OVNKubernetes/#discover-the-environment","title":"Discover the environment","text":"","tags":["NetworkPolicy","OVNKubernetes"]},{"location":"networking/network-policy/OVNKubernetes/#list-pods","title":"List POD's","text":"<pre><code>$ oc get pods -o wide -n simpson\nNAME                     READY   STATUS    RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES\nhomer-789b78ddf5-89dqp   1/1     Running   0          142m   10.129.0.113   master-0   &lt;none&gt;           &lt;none&gt;\nmarge-5887b4985f-td9qx   1/1     Running   0          142m   10.130.0.207   master-1   &lt;none&gt;           &lt;none\n\n$ oc get pods -o wide -n bouvier\nNAME                     READY   STATUS    RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES\npatty-7c674bc58c-tvtnc   1/1     Running   0          142m   10.129.0.111   master-0   &lt;none&gt;           &lt;none&gt;\nselma-6787cf669f-bnrj5   1/1     Running   0          142m   10.129.0.112   master-0   &lt;none&gt;           &lt;none&gt;\n</code></pre>","tags":["NetworkPolicy","OVNKubernetes"]},{"location":"networking/network-policy/OVNKubernetes/#lets-start-with-the-network-policy-demonstration","title":"Let's start with the Network Policy demonstration","text":"<p>Every one can connect to each other</p> <p></p> <pre><code>$ ./OVNKubernetes/dump-net.sh --all case0\nRun dump at\nmaster-1;ovs-node-c97cv\nmaster-0;ovs-node-t84dc\nmaster-2;ovs-node-xs2fm\nWrite: case0.2020-12-29-12-07-39.1609240059.master-1.OpenFlow13.br-ex\nWrite: case0.2020-12-29-12-07-39.1609240059.master-1.OpenFlow13.br-int\nWrite: case0.2020-12-29-12-07-39.1609240059.master-1.OpenFlow13.br-local\nWrite: case0.2020-12-29-12-07-39.1609240059.master-1.iptables\nWrite: case0.2020-12-29-12-07-39.1609240059.master-0.OpenFlow13.br-ex\nWrite: case0.2020-12-29-12-07-39.1609240059.master-0.OpenFlow13.br-int\nWrite: case0.2020-12-29-12-07-39.1609240059.master-0.OpenFlow13.br-local\nWrite: case0.2020-12-29-12-07-39.1609240059.master-0.iptables\nWrite: case0.2020-12-29-12-07-39.1609240059.master-2.OpenFlow13.br-ex\nWrite: case0.2020-12-29-12-07-39.1609240059.master-2.OpenFlow13.br-int\nWrite: case0.2020-12-29-12-07-39.1609240059.master-2.OpenFlow13.br-local\nWrite: case0.2020-12-29-12-07-39.1609240059.master-2.iptables\n</code></pre>","tags":["NetworkPolicy","OVNKubernetes"]},{"location":"networking/network-policy/OVNKubernetes/#case-1-simpson-default-deny","title":"Case 1 - Simpson - default-deny","text":"<pre><code>oc create -n simpson  -f - &lt;&lt;EOF\nkind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: default-deny\nspec:\n  podSelector: {}\nEOF\n</code></pre> <pre><code>$ ./OVNKubernetes/dump-net.sh master-0 master-0.case1\nRun dump at\nmaster-0;ovs-node-t84dc\nWrite: master-0.case1.master-0.OpenFlow13.br-ex\nWrite: master-0.case1.master-0.OpenFlow13.br-int\nWrite: master-0.case1.master-0.OpenFlow13.br-local\nWrite: master-0.case1.master-0.iptables\n</code></pre> <p>Diff of OpenFlow13 <pre><code>$ diff -Nuar case0.2020-12-29-12-07-39.1609240059.master-0.OpenFlow13.br-ex master-0.case1.master-0.OpenFlow13.br-ex\n$ diff -Nuar case0.2020-12-29-12-07-39.1609240059.master-0.OpenFlow13.br-local master-0.case1.master-0.OpenFlow13.br-local\n$ diff -Nuar case0.2020-12-29-12-07-39.1609240059.master-0.OpenFlow13.br-int master-0.case1.master-0.OpenFlow13.br-int\n--- case0.2020-12-29-12-07-39.1609240059.master-0.OpenFlow13.br-int 2020-12-29 12:07:49.000000000 +0100\n+++ master-0.case1.master-0.OpenFlow13.br-int   2020-12-29 12:08:58.000000000 +0100\n@@ -3327,6 +3327,15 @@\n  cookie=0x5ba0d9b7, table=44, priority=2001,ct_state=-new+est-rpl+trk,ct_label=0x1/0x1,ip,metadata=0x3,nw_src=10.128.0.2 actions=load:0x1-&gt;NXM_NX_XXREG0[97],resubmit(,45)\n  cookie=0x56194a07, table=44, priority=2001,ct_state=-new+est-rpl+trk,ct_label=0x1/0x1,ip,metadata=0x5,nw_src=10.129.0.2 actions=load:0x1-&gt;NXM_NX_XXREG0[97],resubmit(,45)\n  cookie=0xb258993d, table=44, priority=2001,ct_state=-new+est-rpl+trk,ct_label=0/0x1,ip,metadata=0x5,nw_src=10.129.0.2 actions=resubmit(,45)\n+ cookie=0xa3134bb9, table=44, priority=2001,ct_state=-new+est-rpl+trk,ct_label=0x1/0x1,arp,reg15=0x49,metadata=0x5 actions=load:0x1-&gt;NXM_NX_XXREG0[97],resubmit(,45)\n+ cookie=0x8951af69, table=44, priority=2001,ct_state=-new+est-rpl+trk,ct_label=0/0x1,arp,reg15=0x49,metadata=0x5 actions=resubmit(,45)\n+ cookie=0xa3134bb9, table=44, priority=2001,ct_state=+new-est+trk,arp,reg15=0x49,metadata=0x5 actions=load:0x1-&gt;NXM_NX_XXREG0[97],resubmit(,45)\n+ cookie=0x8951af69, table=44, priority=2001,ct_state=-trk,arp,reg15=0x49,metadata=0x5 actions=resubmit(,45)\n+ cookie=0x52df188c, table=44, priority=2000,ct_state=+est+trk,ct_label=0x1/0x1,reg15=0x49,metadata=0x5 actions=drop\n+ cookie=0x6ca413a8, table=44, priority=2000,ct_state=+est+trk,ct_label=0/0x1,ipv6,reg15=0x49,metadata=0x5 actions=ct(commit,zone=NXM_NX_REG13[0..15],exec(load:0x1-&gt;NXM_NX_CT_LABEL[0]))\n+ cookie=0x6ca413a8, table=44, priority=2000,ct_state=+est+trk,ct_label=0/0x1,ip,reg15=0x49,metadata=0x5 actions=ct(commit,zone=NXM_NX_REG13[0..15],exec(load:0x1-&gt;NXM_NX_CT_LABEL[0]))\n+ cookie=0x52df188c, table=44, priority=2000,ct_state=-trk,reg15=0x49,metadata=0x5 actions=drop\n+ cookie=0x52df188c, table=44, priority=2000,ct_state=-est+trk,reg15=0x49,metadata=0x5 actions=drop\n  cookie=0x93b0b827, table=44, priority=1,ct_state=-est+trk,ip,metadata=0x4 actions=load:0x1-&gt;NXM_NX_XXREG0[97],resubmit(,45)\n  cookie=0x93b0b827, table=44, priority=1,ct_state=-est+trk,ipv6,metadata=0x4 actions=load:0x1-&gt;NXM_NX_XXREG0[97],resubmit(,45)\n  cookie=0xb723d24c, table=44, priority=1,ct_state=-est+trk,ipv6,metadata=0x3 actions=load:0x1-&gt;NXM_NX_XXREG0[97],resubmit(,45)\n</code></pre></p>","tags":["NetworkPolicy","OVNKubernetes"]},{"location":"networking/network-policy/OVNKubernetes/#2-simpson-allow-from-openshift-ingress-namespaces-because-of-router","title":"2) Simpson allow from openshift-ingress namespaces, because of router","text":"<pre><code>cat &lt;&lt; EOF| oc create -f -\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-from-openshift-ingress\nspec:\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          network.openshift.io/policy-group: ingress\n  podSelector: {}\n  policyTypes:\n  - Ingress\nEOF\n</code></pre> <p>Because of HostNetwork access of the OpenShift Ingress you have to apply a label to the default namespace: <pre><code>oc label namespace default 'network.openshift.io/policy-group=ingress'\n</code></pre> Documentation: 2. If the default Ingress Controller configuration has the...</p> <pre><code>$ ./OVNKubernetes/dump-net.sh master-0 master-0.case2\nRun dump at\nmaster-0;ovs-node-t84dc\nWrite: master-0.case2.master-0.OpenFlow13.br-ex\nWrite: master-0.case2.master-0.OpenFlow13.br-int\nWrite: master-0.case2.master-0.OpenFlow13.br-local\nWrite: master-0.case2.master-0.iptables\n</code></pre> <p>Diff: <pre><code>$ diff -Nuar master-0.case1.master-0.OpenFlow13.br-int master-0.case2.master-0.OpenFlow13.br-int\n$ diff -Nuar master-0.case1.master-0.OpenFlow13.br-ex master-0.case2.master-0.OpenFlow13.br-ex\n$ diff -Nuar master-0.case1.master-0.OpenFlow13.br-local master-0.case2.master-0.OpenFlow13.br-local\n</code></pre></p> <p>Warning</p> <p>Problem - did not work! Bug 1909777 - Setting up multitenant netwotk policy does not work with OVN-Kubernetes network plugin.</p>","tags":["NetworkPolicy","OVNKubernetes"]},{"location":"networking/network-policy/OVNKubernetes/#3-simpson-allow-internal-communcation","title":"3) Simpson allow internal communcation","text":"<pre><code>cat &lt;&lt; EOF| oc create -f -\nkind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: allow-same-namespace\nspec:\n  podSelector:\n  ingress:\n  - from:\n    - podSelector: {}\nEOF\n</code></pre> <pre><code>$ ./OVNKubernetes/dump-net.sh master-0 master-0.case3\n</code></pre> <p>Diff <pre><code>$ diff -Nuar master-0.case2.master-0.OpenFlow13.br-int master-0.case3.master-0.OpenFlow13.br-int\n--- master-0.case2.master-0.OpenFlow13.br-int   2020-12-29 13:22:34.000000000 +0100\n+++ master-0.case3.master-0.OpenFlow13.br-int   2020-12-29 13:27:13.000000000 +0100\n@@ -3331,6 +3331,14 @@\n  cookie=0x8951af69, table=44, priority=2001,ct_state=-new+est-rpl+trk,ct_label=0/0x1,arp,reg15=0x49,metadata=0x5 actions=resubmit(,45)\n  cookie=0xa3134bb9, table=44, priority=2001,ct_state=+new-est+trk,arp,reg15=0x49,metadata=0x5 actions=load:0x1-&gt;NXM_NX_XXREG0[97],resubmit(,45)\n  cookie=0x8951af69, table=44, priority=2001,ct_state=-trk,arp,reg15=0x49,metadata=0x5 actions=resubmit(,45)\n+ cookie=0x25d2c138, table=44, priority=2001,ct_state=-new+est-rpl+trk,ct_label=0/0x1,ip,reg15=0x49,metadata=0x5,nw_src=10.129.0.113 actions=resubmit(,45)\n+ cookie=0x4b57e23f, table=44, priority=2001,ct_state=-new+est-rpl+trk,ct_label=0x1/0x1,ip,reg15=0x49,metadata=0x5,nw_src=10.129.0.113 actions=load:0x1-&gt;NXM_NX_XXREG0[97],resubmit(,45)\n+ cookie=0x4b57e23f, table=44, priority=2001,ct_state=-new+est-rpl+trk,ct_label=0x1/0x1,ip,reg15=0x49,metadata=0x5,nw_src=10.130.0.207 actions=load:0x1-&gt;NXM_NX_XXREG0[97],resubmit(,45)\n+ cookie=0x25d2c138, table=44, priority=2001,ct_state=-new+est-rpl+trk,ct_label=0/0x1,ip,reg15=0x49,metadata=0x5,nw_src=10.130.0.207 actions=resubmit(,45)\n+ cookie=0x25d2c138, table=44, priority=2001,ct_state=-trk,ip,reg15=0x49,metadata=0x5,nw_src=10.129.0.113 actions=resubmit(,45)\n+ cookie=0x25d2c138, table=44, priority=2001,ct_state=-trk,ip,reg15=0x49,metadata=0x5,nw_src=10.130.0.207 actions=resubmit(,45)\n+ cookie=0x4b57e23f, table=44, priority=2001,ct_state=+new-est+trk,ip,reg15=0x49,metadata=0x5,nw_src=10.129.0.113 actions=load:0x1-&gt;NXM_NX_XXREG0[97],resubmit(,45)\n+ cookie=0x4b57e23f, table=44, priority=2001,ct_state=+new-est+trk,ip,reg15=0x49,metadata=0x5,nw_src=10.130.0.207 actions=load:0x1-&gt;NXM_NX_XXREG0[97],resubmit(,45)\n  cookie=0x52df188c, table=44, priority=2000,ct_state=+est+trk,ct_label=0x1/0x1,reg15=0x49,metadata=0x5 actions=drop\n  cookie=0x6ca413a8, table=44, priority=2000,ct_state=+est+trk,ct_label=0/0x1,ipv6,reg15=0x49,metadata=0x5 actions=ct(commit,zone=NXM_NX_REG13[0..15],exec(load:0x1-&gt;NXM_NX_CT_LABEL[0]))\n  cookie=0x6ca413a8, table=44, priority=2000,ct_state=+est+trk,ct_label=0/0x1,ip,reg15=0x49,metadata=0x5 actions=ct(commit,zone=NXM_NX_REG13[0..15],exec(load:0x1-&gt;NXM_NX_CT_LABEL[0]))\n$ diff -Nuar master-0.case2.master-0.OpenFlow13.br-ex master-0.case3.master-0.OpenFlow13.br-ex\n$ diff -Nuar master-0.case3.master-0.OpenFlow13.br-local master-0.case3.master-0.OpenFlow13.br-local\n</code></pre></p>","tags":["NetworkPolicy","OVNKubernetes"]},{"location":"networking/network-policy/OVNKubernetes/#4-selma-and-patty-wants-to-talk-with-marge","title":"4) Selma and Patty want's to talk with Marge!","text":"<p>1) First label the namespace bouvier:    <pre><code>oc label namespace/bouvier name=bouvier\n</code></pre></p> <p>2) Apply Network Policy    <pre><code>oc create -n simpson -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-from-bouviers-to-marge\nspec:\n  podSelector:\n    matchLabels:\n      deployment: marge\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: bouvier\nEOF\n</code></pre></p> <pre><code>$ ./OVNKubernetes/dump-net.sh master-0 master-0.case4\n</code></pre> <p>Diff <pre><code>$ diff -Nuar master-0.case3.master-0.OpenFlow13.br-int master-0.case4.master-0.OpenFlow13.br-int\n$ diff -Nuar master-0.case3.master-0.OpenFlow13.br-ex master-0.case4.master-0.OpenFlow13.br-ex\n$ diff -Nuar master-0.case3.master-0.OpenFlow13.br-local master-0.case4.master-0.OpenFlow13.br-local\n</code></pre></p> <p>Nothing??</p>","tags":["NetworkPolicy","OVNKubernetes"]},{"location":"networking/network-policy/OVNKubernetes/#destroy-demo-env","title":"Destroy demo env","text":"<pre><code>oc delete project simpson bouvier\n</code></pre>","tags":["NetworkPolicy","OVNKubernetes"]},{"location":"networking/network-policy/OVNKubernetes/#useful-commands","title":"Useful commands","text":"Info Command Dump northbound db <code>oc rsh -n openshift-ovn-kubernetes -c northd ovnkube-master-6s6bw ovn-nbctl -C /ovn-ca/ca-bundle.crt -p /ovn-cert/tls.key -c /ovn-cert/tls.crt --db=ssl:192.168.51.10:9641 --pretty show</code>","tags":["NetworkPolicy","OVNKubernetes"]},{"location":"networking/network-policy/OpenShiftSDN/","title":"Network Policy with OpenShiftSDN","text":"","tags":["NetworkPolicy","OpenShiftSDN"]},{"location":"networking/network-policy/OpenShiftSDN/#nice-to-know-basics","title":"Nice to know / Basics","text":"<ol> <li>Based on labeling or annotations<ul> <li>project / namespaces seldom have labels :-/</li> </ul> </li> <li>Empty label selector match all</li> <li>Rules for allowing<ul> <li>Ingress -&gt; who can connect to this POD</li> <li>Egress -&gt; where can this POD connect to</li> </ul> </li> <li>Rules<ul> <li>traffic is allowed unless a Network Policy selecting the POD</li> <li>traffic is denied if pod is selected in policie but none of them have any rules allowing it</li> <li>=  You can only write rules that allow traffic!</li> <li>Scope: Namespace</li> </ul> </li> </ol>","tags":["NetworkPolicy","OpenShiftSDN"]},{"location":"networking/network-policy/OpenShiftSDN/#tutorial-demo-openshift-v4","title":"Tutorial / Demo - OpenShift v4!","text":"","tags":["NetworkPolicy","OpenShiftSDN"]},{"location":"networking/network-policy/OpenShiftSDN/#deploy-demo-environment","title":"Deploy demo environment","text":"<p>Install OpenShift OpenShift 3 with redhat/openshift-ovs-networkpolicy OpenShift 4 network policy is the default</p> <p></p> <pre><code>oc new-project bouvier\noc new-app quay.io/openshift-examples/simple-http-server:micro --name patty\noc create route edge patty --service=patty\noc new-app quay.io/openshift-examples/simple-http-server:micro --name selma\noc create route edge selma --service=selma\n\noc new-project simpson\noc new-app quay.io/openshift-examples/simple-http-server:micro --name homer\noc create route edge homer --service=homer\n\noc new-app quay.io/openshift-examples/simple-http-server:micro --name marge\noc create route edge marge --service=marge\n</code></pre>","tags":["NetworkPolicy","OpenShiftSDN"]},{"location":"networking/network-policy/OpenShiftSDN/#download-some-helper-scripts","title":"Download some helper scripts","text":"<pre><code>git clone https://github.com/openshift-examples/network-policies-tests.git\ncd network-policies-tests/\n</code></pre>","tags":["NetworkPolicy","OpenShiftSDN"]},{"location":"networking/network-policy/OpenShiftSDN/#run-connection-overview","title":"Run connection overview","text":"<p><pre><code>./run-tmux.sh apps.&lt;cluster_name&gt;.&lt;base_domain&gt;\n</code></pre> </p>","tags":["NetworkPolicy","OpenShiftSDN"]},{"location":"networking/network-policy/OpenShiftSDN/#discover-the-environment","title":"Discover the environment","text":"","tags":["NetworkPolicy","OpenShiftSDN"]},{"location":"networking/network-policy/OpenShiftSDN/#list-network-namespaces","title":"List network namespaces","text":"<pre><code>$ oc get netnamespaces | grep -E '(NAME|simpson|bouvier|default|openshift-ingress)'\nNAME                                               NETID      EGRESS IPS\nbouvier                                            7511589\ndefault                                            0\nopenshift-ingress                                  3002648\nopenshift-ingress-operator                         13811611\nsimpson                                            12446308\n</code></pre>","tags":["NetworkPolicy","OpenShiftSDN"]},{"location":"networking/network-policy/OpenShiftSDN/#list-pods","title":"List POD's","text":"<pre><code>$ oc get pods -o wide -n simpson\nNAME                     READY   STATUS    RESTARTS   AGE   IP            NODE       NOMINATED NODE   READINESS GATES\nhomer-789b78ddf5-njfpf   1/1     Running   0          86m   10.128.0.63   master-0   &lt;none&gt;           &lt;none&gt;\nmarge-5887b4985f-4b7md   1/1     Running   0          86m   10.128.0.64   master-0   &lt;none&gt;           &lt;none&gt;\n\n$ oc get pods -o wide -n bouvier\nNAME                     READY   STATUS    RESTARTS   AGE   IP            NODE       NOMINATED NODE   READINESS GATES\npatty-7c674bc58c-bc8fj   1/1     Running   0          87m   10.128.0.62   master-0   &lt;none&gt;           &lt;none&gt;\nselma-6787cf669f-rwjwh   1/1     Running   0          87m   10.129.1.8    master-2   &lt;none&gt;           &lt;none&gt;\n</code></pre>","tags":["NetworkPolicy","OpenShiftSDN"]},{"location":"networking/network-policy/OpenShiftSDN/#lets-start-with-the-network-policy-demonstration","title":"Let's start with the Network Policy demonstration","text":"<p>Every one can connect to each other</p> <p></p> <pre><code>$ ./OpenShiftSDN/dump-net.sh --all case0\nRun dump at\nmaster-0;sdn-ht7pl\nmaster-2;sdn-kv9l5\nmaster-1;sdn-qh2wm\nWrite: case0.2020-12-29-08-43-00.1609227780.master-0.OpenFlow13\nWrite: case0.2020-12-29-08-43-00.1609227780.master-0.iptables\nWrite: case0.2020-12-29-08-43-00.1609227780.master-2.OpenFlow13\nWrite: case0.2020-12-29-08-43-00.1609227780.master-2.iptables\nWrite: case0.2020-12-29-08-43-00.1609227780.master-1.OpenFlow13\nWrite: case0.2020-12-29-08-43-00.1609227780.master-1.iptables\n</code></pre>","tags":["NetworkPolicy","OpenShiftSDN"]},{"location":"networking/network-policy/OpenShiftSDN/#case-1-simpson-default-deny","title":"Case 1 - Simpson - default-deny","text":"<pre><code>oc create -n simpson  -f - &lt;&lt;EOF\nkind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: default-deny\nspec:\n  podSelector: {}\nEOF\n</code></pre> <pre><code>$ ./OpenShiftSDN/dump-net.sh master-0 master-0.case1\nRun dump at\nmaster-0;sdn-ht7pl\nWrite: master-0.case1.master-0.OpenFlow13\nWrite: master-0.case1.master-0.iptables\n</code></pre> <p>Diff of OpenFlow13 <pre><code>$ diff -Nuar case0.2020-12-29-08-43-00.1609227780.master-0.OpenFlow13 master-0.case1.OpenFlow13\n--- case0.2020-12-29-08-43-00.1609227780.master-0.OpenFlow13    2020-12-29 08:43:01.000000000 +0100\n+++ master-0.case1.OpenFlow13   1970-01-01 01:00:00.000000000 +0100\n@@ -1,190 +0,0 @@\n- priority=1000,ct_state=-trk,ip actions=ct(table=0)\n- priority=400,ip,in_port=tun0,nw_src=10.128.0.1 actions=goto_table:30\n- priority=300,ip,in_port=tun0,nw_src=10.128.0.0/23,nw_dst=10.128.0.0/14 actions=goto_table:25\n- priority=250,ip,in_port=tun0,nw_dst=224.0.0.0/4 actions=drop\n- priority=200,arp,in_port=vxlan0,arp_spa=10.128.0.0/14,arp_tpa=10.128.0.0/23 actions=move:NXM_NX_TUN_ID[0..31]-&gt;NXM_NX_REG0[],goto_table:10\n- priority=200,ip,in_port=vxlan0,nw_src=10.128.0.0/14 actions=move:NXM_NX_TUN_ID[0..31]-&gt;NXM_NX_REG0[],goto_table:10\n- priority=200,ip,in_port=vxlan0,nw_dst=10.128.0.0/14 actions=move:NXM_NX_TUN_ID[0..31]-&gt;NXM_NX_REG0[],goto_table:10\n- priority=200,arp,in_port=tun0,arp_spa=10.128.0.1,arp_tpa=10.128.0.0/14 actions=goto_table:30\n- priority=200,ip,in_port=tun0 actions=goto_table:30\n- priority=150,in_port=vxlan0 actions=drop\n- priority=150,in_port=tun0 actions=drop\n- priority=100,arp actions=goto_table:20\n- priority=100,ip actions=goto_table:20\n- priority=0 actions=drop\n- cookie=0x1d71af99, table=10, priority=100,tun_src=192.168.50.12 actions=goto_table:30\n- cookie=0x161a7f59, table=10, priority=100,tun_src=192.168.50.11 actions=goto_table:30\n- table=10, priority=0 actions=drop\n- table=20, priority=100,arp,in_port=vethd46201d2,arp_spa=10.128.0.2,arp_sha=00:00:0a:80:00:02/00:00:ff:ff:ff:ff actions=load:0x475327-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=veth4e414189,arp_spa=10.128.0.3,arp_sha=00:00:0a:80:00:03/00:00:ff:ff:ff:ff actions=load:0x475327-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=veth0025790d,arp_spa=10.128.0.4,arp_sha=00:00:0a:80:00:04/00:00:ff:ff:ff:ff actions=load:0xc46324-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=veth25fb1f17,arp_spa=10.128.0.5,arp_sha=00:00:0a:80:00:05/00:00:ff:ff:ff:ff actions=load:0x9230fb-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=veth9ed8d346,arp_spa=10.128.0.6,arp_sha=00:00:0a:80:00:06/00:00:ff:ff:ff:ff actions=load:0xd8a3cf-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=vethbd80adef,arp_spa=10.128.0.7,arp_sha=00:00:0a:80:00:07/00:00:ff:ff:ff:ff actions=load:0x7819d6-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=veth6d10aca6,arp_spa=10.128.0.8,arp_sha=00:00:0a:80:00:08/00:00:ff:ff:ff:ff actions=load:0x71b55a-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=vethf4d750d6,arp_spa=10.128.0.9,arp_sha=00:00:0a:80:00:09/00:00:ff:ff:ff:ff actions=load:0xc1d466-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=veth31039f7e,arp_spa=10.128.0.10,arp_sha=00:00:0a:80:00:0a/00:00:ff:ff:ff:ff actions=load:0xc1d466-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=vetha565fc85,arp_spa=10.128.0.12,arp_sha=00:00:0a:80:00:0c/00:00:ff:ff:ff:ff actions=load:0xc46324-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=veth3e1b4c0d,arp_spa=10.128.0.19,arp_sha=00:00:0a:80:00:13/00:00:ff:ff:ff:ff actions=load:0xc1d466-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=veth2ef77857,arp_spa=10.128.0.22,arp_sha=00:00:0a:80:00:16/00:00:ff:ff:ff:ff actions=load:0x6b2011-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=vethbdf38f86,arp_spa=10.128.0.26,arp_sha=00:00:0a:80:00:1a/00:00:ff:ff:ff:ff actions=load:0xeda37e-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=veth7ef4c743,arp_spa=10.128.0.32,arp_sha=00:00:0a:80:00:20/00:00:ff:ff:ff:ff actions=load:0x802940-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=veth3f643c57,arp_spa=10.128.0.34,arp_sha=00:00:0a:80:00:22/00:00:ff:ff:ff:ff actions=load:0x61697e-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=veth98d3cbc2,arp_spa=10.128.0.37,arp_sha=00:00:0a:80:00:25/00:00:ff:ff:ff:ff actions=load:0xca8cd4-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=veth1e8a5ed2,arp_spa=10.128.0.43,arp_sha=00:00:0a:80:00:2b/00:00:ff:ff:ff:ff actions=load:0xc1d466-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=vethbd015c31,arp_spa=10.128.0.44,arp_sha=00:00:0a:80:00:2c/00:00:ff:ff:ff:ff actions=load:0xc1d466-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=vethe69cad58,arp_spa=10.128.0.45,arp_sha=00:00:0a:80:00:2d/00:00:ff:ff:ff:ff actions=load:0xc1d466-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=veth24794758,arp_spa=10.128.0.47,arp_sha=00:00:0a:80:00:2f/00:00:ff:ff:ff:ff actions=load:0xc1d466-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=vethf72ff6f7,arp_spa=10.128.0.52,arp_sha=00:00:0a:80:00:34/00:00:ff:ff:ff:ff actions=load:0x15e784-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=vethad55f19e,arp_spa=10.128.0.62,arp_sha=00:00:0a:80:00:3e/00:00:ff:ff:ff:ff actions=load:0x729e25-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=veth21dbd829,arp_spa=10.128.0.63,arp_sha=00:00:0a:80:00:3f/00:00:ff:ff:ff:ff actions=load:0xbdea64-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,arp,in_port=vethcbfe86ee,arp_spa=10.128.0.64,arp_sha=00:00:0a:80:00:40/00:00:ff:ff:ff:ff actions=load:0xbdea64-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=vethd46201d2,nw_src=10.128.0.2 actions=load:0x475327-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=veth4e414189,nw_src=10.128.0.3 actions=load:0x475327-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=veth0025790d,nw_src=10.128.0.4 actions=load:0xc46324-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=veth25fb1f17,nw_src=10.128.0.5 actions=load:0x9230fb-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=veth9ed8d346,nw_src=10.128.0.6 actions=load:0xd8a3cf-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=vethbd80adef,nw_src=10.128.0.7 actions=load:0x7819d6-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=veth6d10aca6,nw_src=10.128.0.8 actions=load:0x71b55a-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=vethf4d750d6,nw_src=10.128.0.9 actions=load:0xc1d466-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=veth31039f7e,nw_src=10.128.0.10 actions=load:0xc1d466-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=vetha565fc85,nw_src=10.128.0.12 actions=load:0xc46324-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=veth3e1b4c0d,nw_src=10.128.0.19 actions=load:0xc1d466-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=veth2ef77857,nw_src=10.128.0.22 actions=load:0x6b2011-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=vethbdf38f86,nw_src=10.128.0.26 actions=load:0xeda37e-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=veth7ef4c743,nw_src=10.128.0.32 actions=load:0x802940-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=veth3f643c57,nw_src=10.128.0.34 actions=load:0x61697e-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=veth98d3cbc2,nw_src=10.128.0.37 actions=load:0xca8cd4-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=veth1e8a5ed2,nw_src=10.128.0.43 actions=load:0xc1d466-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=vethbd015c31,nw_src=10.128.0.44 actions=load:0xc1d466-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=vethe69cad58,nw_src=10.128.0.45 actions=load:0xc1d466-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=veth24794758,nw_src=10.128.0.47 actions=load:0xc1d466-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=vethf72ff6f7,nw_src=10.128.0.52 actions=load:0x15e784-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=vethad55f19e,nw_src=10.128.0.62 actions=load:0x729e25-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=veth21dbd829,nw_src=10.128.0.63 actions=load:0xbdea64-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=100,ip,in_port=vethcbfe86ee,nw_src=10.128.0.64 actions=load:0xbdea64-&gt;NXM_NX_REG0[],goto_table:21\n- table=20, priority=0 actions=drop\n- table=21, priority=200,ip,nw_dst=10.128.0.0/14 actions=ct(commit,table=30)\n- table=21, priority=0 actions=goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.2 actions=load:0x475327-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.3 actions=load:0x475327-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.4 actions=load:0xc46324-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.5 actions=load:0x9230fb-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.6 actions=load:0xd8a3cf-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.7 actions=load:0x7819d6-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.8 actions=load:0x71b55a-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.9 actions=load:0xc1d466-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.10 actions=load:0xc1d466-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.12 actions=load:0xc46324-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.19 actions=load:0xc1d466-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.22 actions=load:0x6b2011-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.26 actions=load:0xeda37e-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.32 actions=load:0x802940-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.34 actions=load:0x61697e-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.37 actions=load:0xca8cd4-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.43 actions=load:0xc1d466-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.44 actions=load:0xc1d466-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.45 actions=load:0xc1d466-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.47 actions=load:0xc1d466-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.52 actions=load:0x15e784-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.62 actions=load:0x729e25-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.63 actions=load:0xbdea64-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=100,ip,nw_src=10.128.0.64 actions=load:0xbdea64-&gt;NXM_NX_REG0[],goto_table:30\n- table=25, priority=0 actions=drop\n- table=30, priority=300,arp,arp_tpa=10.128.0.1 actions=output:tun0\n- table=30, priority=300,ip,nw_dst=10.128.0.1 actions=output:tun0\n- table=30, priority=300,ct_state=+rpl,ip,nw_dst=10.128.0.0/23 actions=ct(table=70,nat)\n- table=30, priority=200,arp,arp_tpa=10.128.0.0/23 actions=goto_table:40\n- table=30, priority=200,ip,nw_dst=10.128.0.0/23 actions=goto_table:70\n- table=30, priority=100,arp,arp_tpa=10.128.0.0/14 actions=goto_table:50\n- table=30, priority=100,ip,nw_dst=10.128.0.0/14 actions=goto_table:90\n- table=30, priority=100,ip,nw_dst=172.30.0.0/16 actions=goto_table:60\n- table=30, priority=50,ip,in_port=vxlan0,nw_dst=224.0.0.0/4 actions=goto_table:120\n- table=30, priority=25,ip,nw_dst=224.0.0.0/4 actions=goto_table:110\n- table=30, priority=0,ip actions=goto_table:100\n- table=30, priority=0,arp actions=drop\n- table=40, priority=100,arp,arp_tpa=10.128.0.2 actions=output:vethd46201d2\n- table=40, priority=100,arp,arp_tpa=10.128.0.3 actions=output:veth4e414189\n- table=40, priority=100,arp,arp_tpa=10.128.0.4 actions=output:veth0025790d\n- table=40, priority=100,arp,arp_tpa=10.128.0.5 actions=output:veth25fb1f17\n- table=40, priority=100,arp,arp_tpa=10.128.0.6 actions=output:veth9ed8d346\n- table=40, priority=100,arp,arp_tpa=10.128.0.7 actions=output:vethbd80adef\n- table=40, priority=100,arp,arp_tpa=10.128.0.8 actions=output:veth6d10aca6\n- table=40, priority=100,arp,arp_tpa=10.128.0.9 actions=output:vethf4d750d6\n- table=40, priority=100,arp,arp_tpa=10.128.0.10 actions=output:veth31039f7e\n- table=40, priority=100,arp,arp_tpa=10.128.0.12 actions=output:vetha565fc85\n- table=40, priority=100,arp,arp_tpa=10.128.0.19 actions=output:veth3e1b4c0d\n- table=40, priority=100,arp,arp_tpa=10.128.0.22 actions=output:veth2ef77857\n- table=40, priority=100,arp,arp_tpa=10.128.0.26 actions=output:vethbdf38f86\n- table=40, priority=100,arp,arp_tpa=10.128.0.32 actions=output:veth7ef4c743\n- table=40, priority=100,arp,arp_tpa=10.128.0.34 actions=output:veth3f643c57\n- table=40, priority=100,arp,arp_tpa=10.128.0.37 actions=output:veth98d3cbc2\n- table=40, priority=100,arp,arp_tpa=10.128.0.43 actions=output:veth1e8a5ed2\n- table=40, priority=100,arp,arp_tpa=10.128.0.44 actions=output:vethbd015c31\n- table=40, priority=100,arp,arp_tpa=10.128.0.45 actions=output:vethe69cad58\n- table=40, priority=100,arp,arp_tpa=10.128.0.47 actions=output:veth24794758\n- table=40, priority=100,arp,arp_tpa=10.128.0.52 actions=output:vethf72ff6f7\n- table=40, priority=100,arp,arp_tpa=10.128.0.62 actions=output:vethad55f19e\n- table=40, priority=100,arp,arp_tpa=10.128.0.63 actions=output:veth21dbd829\n- table=40, priority=100,arp,arp_tpa=10.128.0.64 actions=output:vethcbfe86ee\n- table=40, priority=0 actions=drop\n- cookie=0x1d71af99, table=50, priority=100,arp,arp_tpa=10.129.0.0/23 actions=move:NXM_NX_REG0[]-&gt;NXM_NX_TUN_ID[0..31],set_field:192.168.50.12-&gt;tun_dst,output:vxlan0\n- cookie=0x161a7f59, table=50, priority=100,arp,arp_tpa=10.130.0.0/23 actions=move:NXM_NX_REG0[]-&gt;NXM_NX_TUN_ID[0..31],set_field:192.168.50.11-&gt;tun_dst,output:vxlan0\n- table=50, priority=0 actions=drop\n- table=60, priority=200 actions=output:tun0\n- table=60, priority=0 actions=drop\n- table=70, priority=100,ip,nw_dst=10.128.0.2 actions=load:0x475327-&gt;NXM_NX_REG1[],load:0x3-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.3 actions=load:0x475327-&gt;NXM_NX_REG1[],load:0x4-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.4 actions=load:0xc46324-&gt;NXM_NX_REG1[],load:0x5-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.5 actions=load:0x9230fb-&gt;NXM_NX_REG1[],load:0x6-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.6 actions=load:0xd8a3cf-&gt;NXM_NX_REG1[],load:0x7-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.7 actions=load:0x7819d6-&gt;NXM_NX_REG1[],load:0x8-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.8 actions=load:0x71b55a-&gt;NXM_NX_REG1[],load:0x9-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.9 actions=load:0xc1d466-&gt;NXM_NX_REG1[],load:0xa-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.10 actions=load:0xc1d466-&gt;NXM_NX_REG1[],load:0xb-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.12 actions=load:0xc46324-&gt;NXM_NX_REG1[],load:0xd-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.19 actions=load:0xc1d466-&gt;NXM_NX_REG1[],load:0x14-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.22 actions=load:0x6b2011-&gt;NXM_NX_REG1[],load:0x17-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.26 actions=load:0xeda37e-&gt;NXM_NX_REG1[],load:0x1b-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.32 actions=load:0x802940-&gt;NXM_NX_REG1[],load:0x21-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.34 actions=load:0x61697e-&gt;NXM_NX_REG1[],load:0x23-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.37 actions=load:0xca8cd4-&gt;NXM_NX_REG1[],load:0x26-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.43 actions=load:0xc1d466-&gt;NXM_NX_REG1[],load:0x2c-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.44 actions=load:0xc1d466-&gt;NXM_NX_REG1[],load:0x2d-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.45 actions=load:0xc1d466-&gt;NXM_NX_REG1[],load:0x2e-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.47 actions=load:0xc1d466-&gt;NXM_NX_REG1[],load:0x30-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.52 actions=load:0x15e784-&gt;NXM_NX_REG1[],load:0x35-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.62 actions=load:0x729e25-&gt;NXM_NX_REG1[],load:0x3f-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.63 actions=load:0xbdea64-&gt;NXM_NX_REG1[],load:0x40-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=100,ip,nw_dst=10.128.0.64 actions=load:0xbdea64-&gt;NXM_NX_REG1[],load:0x41-&gt;NXM_NX_REG2[],goto_table:80\n- table=70, priority=0 actions=drop\n- table=80, priority=300,ip,nw_src=10.128.0.1 actions=output:NXM_NX_REG2[]\n- table=80, priority=200,ct_state=+rpl,ip actions=output:NXM_NX_REG2[]\n- table=80, priority=50,reg1=4674343 actions=output:NXM_NX_REG2[]\n- table=80, priority=50,reg1=12870436 actions=output:NXM_NX_REG2[]\n- table=80, priority=50,reg1=9580795 actions=output:NXM_NX_REG2[]\n- table=80, priority=50,reg1=14197711 actions=output:NXM_NX_REG2[]\n- table=80, priority=50,reg1=7870934 actions=output:NXM_NX_REG2[]\n- table=80, priority=50,reg1=7451994 actions=output:NXM_NX_REG2[]\n- table=80, priority=50,reg1=12702822 actions=output:NXM_NX_REG2[]\n- table=80, priority=50,reg1=6383998 actions=output:NXM_NX_REG2[]\n- table=80, priority=50,reg1=7020561 actions=output:NXM_NX_REG2[]\n- table=80, priority=50,reg1=13274324 actions=output:NXM_NX_REG2[]\n- table=80, priority=50,reg1=15573886 actions=output:NXM_NX_REG2[]\n- table=80, priority=50,reg1=8399168 actions=output:NXM_NX_REG2[]\n- table=80, priority=50,reg1=1435524 actions=output:NXM_NX_REG2[]\n- table=80, priority=50,reg1=7511589 actions=output:NXM_NX_REG2[]\n- table=80, priority=50,reg1=12446308 actions=output:NXM_NX_REG2[]\n- table=80, priority=0 actions=drop\n- cookie=0x1d71af99, table=90, priority=100,ip,nw_dst=10.129.0.0/23 actions=move:NXM_NX_REG0[]-&gt;NXM_NX_TUN_ID[0..31],set_field:192.168.50.12-&gt;tun_dst,output:vxlan0\n- cookie=0x161a7f59, table=90, priority=100,ip,nw_dst=10.130.0.0/23 actions=move:NXM_NX_REG0[]-&gt;NXM_NX_TUN_ID[0..31],set_field:192.168.50.11-&gt;tun_dst,output:vxlan0\n- table=90, priority=0 actions=drop\n- table=100, priority=300,udp,tp_dst=4789 actions=drop\n- table=100, priority=200,tcp,nw_dst=192.168.50.10,tp_dst=53 actions=output:tun0\n- table=100, priority=200,udp,nw_dst=192.168.50.10,tp_dst=53 actions=output:tun0\n- table=100, priority=0 actions=goto_table:101\n- table=101, priority=0 actions=output:tun0\n- table=110, priority=0 actions=drop\n- table=111, priority=100 actions=move:NXM_NX_REG0[]-&gt;NXM_NX_TUN_ID[0..31],set_field:192.168.50.11-&gt;tun_dst,output:vxlan0,set_field:192.168.50.12-&gt;tun_dst,output:vxlan0,goto_table:120\n- table=120, priority=0 actions=drop\n- table=253, actions=note:02.08.00.00.00.00\n</code></pre></p>","tags":["NetworkPolicy","OpenShiftSDN"]},{"location":"networking/network-policy/OpenShiftSDN/#2-simpson-allow-from-openshift-ingress-namespaces-because-of-router","title":"2) Simpson allow from openshift-ingress namespaces, because of router","text":"<pre><code>cat &lt;&lt; EOF| oc create -f -\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-from-openshift-ingress\nspec:\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          network.openshift.io/policy-group: ingress\n  podSelector: {}\n  policyTypes:\n  - Ingress\nEOF\n</code></pre> <p>Because of HostNetwork access of the OpenShift Ingress you have to apply a label to the default namespace: <pre><code>oc label namespace default 'network.openshift.io/policy-group=ingress'\n</code></pre> Documentation: 2. If the default Ingress Controller configuration has the...</p> <pre><code>$ ./OpenShiftSDN/dump-net.sh master-0 master-0.case2\nRun dump at\nmaster-0;sdn-ht7pl\nWrite: master-0.case2.master-0.OpenFlow13\nWrite: master-0.case2.master-0.iptables\n</code></pre> <p>Diff: <pre><code>$ diff -Nuar master-0.case1.master-0.OpenFlow13 master-0.case2.master-0.OpenFlow13\n--- master-0.case1.master-0.OpenFlow13  2020-12-29 08:44:30.000000000 +0100\n+++ master-0.case2.master-0.OpenFlow13  2020-12-29 08:55:26.000000000 +0100\n@@ -160,6 +160,8 @@\n  table=70, priority=0 actions=drop\n  table=80, priority=300,ip,nw_src=10.128.0.1 actions=output:NXM_NX_REG2[]\n  table=80, priority=200,ct_state=+rpl,ip actions=output:NXM_NX_REG2[]\n+ table=80, priority=150,reg0=0,reg1=12446308 actions=output:NXM_NX_REG2[]\n+ table=80, priority=150,reg0=3002648,reg1=12446308 actions=output:NXM_NX_REG2[]\n  table=80, priority=50,reg1=4674343 actions=output:NXM_NX_REG2[]\n  table=80, priority=50,reg1=12870436 actions=output:NXM_NX_REG2[]\n  table=80, priority=50,reg1=9580795 actions=output:NXM_NX_REG2[]\n@@ -174,7 +176,6 @@\n  table=80, priority=50,reg1=8399168 actions=output:NXM_NX_REG2[]\n  table=80, priority=50,reg1=1435524 actions=output:NXM_NX_REG2[]\n  table=80, priority=50,reg1=7511589 actions=output:NXM_NX_REG2[]\n- table=80, priority=50,reg1=12446308 actions=output:NXM_NX_REG2[]\n  table=80, priority=0 actions=drop\n  cookie=0x1d71af99, table=90, priority=100,ip,nw_dst=10.129.0.0/23 actions=move:NXM_NX_REG0[]-&gt;NXM_NX_TUN_ID[0..31],set_field:192.168.50.12-&gt;tun_dst,output:vxlan0\n  cookie=0x161a7f59, table=90, priority=100,ip,nw_dst=10.130.0.0/23 actions=move:NXM_NX_REG0[]-&gt;NXM_NX_TUN_ID[0..31],set_field:192.168.50.11-&gt;tun_dst,output:vxlan0\n</code></pre></p> <p></p>","tags":["NetworkPolicy","OpenShiftSDN"]},{"location":"networking/network-policy/OpenShiftSDN/#3-simpson-allow-internal-communcation","title":"3) Simpson allow internal communcation","text":"<pre><code>$ cat &lt;&lt; EOF| oc create -f -\nkind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: allow-same-namespace\nspec:\n  podSelector:\n  ingress:\n  - from:\n    - podSelector: {}\nEOF\n</code></pre> <pre><code>$ ./OpenShiftSDN/dump-net.sh master-0 master-0.case3\nRun dump at\nmaster-0;sdn-ht7pl\nWrite: master-0.case3.master-0.OpenFlow13\nWrite: master-0.case3.master-0.iptables\n</code></pre> <p>Diff <pre><code>$ diff -Nuar master-0.case2.master-0.OpenFlow13 master-0.case3.master-0.OpenFlow13\n--- master-0.case2.master-0.OpenFlow13  2020-12-29 08:55:26.000000000 +0100\n+++ master-0.case3.master-0.OpenFlow13  2020-12-29 08:57:27.000000000 +0100\n@@ -162,6 +162,7 @@\n  table=80, priority=200,ct_state=+rpl,ip actions=output:NXM_NX_REG2[]\n  table=80, priority=150,reg0=0,reg1=12446308 actions=output:NXM_NX_REG2[]\n  table=80, priority=150,reg0=3002648,reg1=12446308 actions=output:NXM_NX_REG2[]\n+ table=80, priority=150,reg0=12446308,reg1=12446308 actions=output:NXM_NX_REG2[]\n  table=80, priority=50,reg1=4674343 actions=output:NXM_NX_REG2[]\n  table=80, priority=50,reg1=12870436 actions=output:NXM_NX_REG2[]\n  table=80, priority=50,reg1=9580795 actions=output:NXM_NX_REG2[]\n</code></pre></p> <p></p>","tags":["NetworkPolicy","OpenShiftSDN"]},{"location":"networking/network-policy/OpenShiftSDN/#4-selma-and-patty-wants-to-talk-with-marge","title":"4) Selma and Patty want's to talk with Marge!","text":"<p>1) First label the namespace bouvier:    <pre><code>oc label namespace/bouvier name=bouvier\n</code></pre></p> <p>2) Apply Network Policy    <pre><code>oc create -n simpson -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-from-bouviers-to-marge\nspec:\n  podSelector:\n    matchLabels:\n      deployment: marge\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: bouvier\nEOF\n</code></pre></p> <pre><code>$ ./OpenShiftSDN/dump-net.sh master-0 master-0.case4\nRun dump at\nmaster-0;sdn-ht7pl\nWrite: master-0.case4.master-0.OpenFlow13\nWrite: master-0.case4.master-0.iptables\n</code></pre> <p>Diff <pre><code>$ diff -Nuar master-0.case3.master-0.OpenFlow13 master-0.case4.master-0.OpenFlow13\n--- master-0.case3.master-0.OpenFlow13  2020-12-29 08:57:27.000000000 +0100\n+++ master-0.case4.master-0.OpenFlow13  2020-12-29 08:59:41.000000000 +0100\n@@ -163,6 +163,7 @@\n  table=80, priority=150,reg0=0,reg1=12446308 actions=output:NXM_NX_REG2[]\n  table=80, priority=150,reg0=3002648,reg1=12446308 actions=output:NXM_NX_REG2[]\n  table=80, priority=150,reg0=12446308,reg1=12446308 actions=output:NXM_NX_REG2[]\n+ table=80, priority=150,ip,reg0=7511589,reg1=12446308,nw_dst=10.128.0.64 actions=output:NXM_NX_REG2[]\n  table=80, priority=50,reg1=4674343 actions=output:NXM_NX_REG2[]\n  table=80, priority=50,reg1=12870436 actions=output:NXM_NX_REG2[]\n  table=80, priority=50,reg1=9580795 actions=output:NXM_NX_REG2[]\n</code></pre></p> <p></p>","tags":["NetworkPolicy","OpenShiftSDN"]},{"location":"networking/network-policy/OpenShiftSDN/#bonus-one-egressnetworkpolicy","title":"Bonus, one EgressNetworkPolicy","text":"<pre><code>oc create -n simpson -f - &lt;&lt;EOF\napiVersion: network.openshift.io/v1\nkind: EgressNetworkPolicy\nmetadata:\n  name: default\nspec:\n  egress:\n  - to:\n      cidrSelector: 1.2.3.0/24\n    type: Allow\n  - to:\n      dnsName: www.foo.com\n    type: Allow\n  - to:\n      cidrSelector: 0.0.0.0/0\n    type: Deny\nEOF\n</code></pre> <pre><code>$ ./OpenShiftSDN/dump-net.sh master-0 master-0.case5\nRun dump at\nmaster-0;sdn-ht7pl\nWrite: master-0.case5.master-0.OpenFlow13\nWrite: master-0.case5.master-0.iptables\n</code></pre> <p>Diff <pre><code>$ diff -Nuar master-0.case4.master-0.OpenFlow13 master-0.case5.master-0.OpenFlow13\n--- master-0.case4.master-0.OpenFlow13  2020-12-29 08:59:41.000000000 +0100\n+++ master-0.case5.master-0.OpenFlow13  2020-12-29 09:01:28.000000000 +0100\n@@ -186,6 +186,9 @@\n  table=100, priority=200,tcp,nw_dst=192.168.50.10,tp_dst=53 actions=output:tun0\n  table=100, priority=200,udp,nw_dst=192.168.50.10,tp_dst=53 actions=output:tun0\n  table=100, priority=0 actions=goto_table:101\n+ table=101, priority=3,ip,reg0=12446308,nw_dst=1.2.3.0/24 actions=output:tun0\n+ table=101, priority=2,ip,reg0=12446308,nw_dst=34.206.39.153 actions=output:tun0\n+ table=101, priority=1,ip,reg0=12446308 actions=drop\n  table=101, priority=0 actions=output:tun0\n  table=110, priority=0 actions=drop\n  table=111, priority=100 actions=move:NXM_NX_REG0[]-&gt;NXM_NX_TUN_ID[0..31],set_field:192.168.50.11-&gt;tun_dst,output:vxlan0,set_field:192.168.50.12-&gt;tun_dst,output:vxlan0,goto_table:120\n</code></pre></p>","tags":["NetworkPolicy","OpenShiftSDN"]},{"location":"networking/network-policy/OpenShiftSDN/#destroy-demo-env","title":"Destroy demo env","text":"<pre><code>oc delete project simpson bouvier\n</code></pre>","tags":["NetworkPolicy","OpenShiftSDN"]},{"location":"networking/network-policy/OpenShiftSDN/#useful-commands","title":"Useful commands","text":"Info Command Convert HEX -&gt; DEC <code>printf \"%d\\n\" 0xcc4bb1</code> Convert DEC -&gt; HEX <code>printf \"0x%x\\n\" 13388721</code> Dump OpenFlow rules <code>oc rsh -n openshift-sdn PODNAME ovs-ofctl -O OpenFlow13 --no-stats dump-flows br0</code> List bridges and ports <code>oc rsh -n openshift-sdn PODNAME ovs-vsctl show</code> Convert HEX to DEC in OpenFlow dump <code>perl -pe 's/(reg0\\|reg1)=([^ ,]+)/sprintf(\"%s=%d\",$1, hex($2))/eg'</code> Clean stats in OpenFlow dump <code>sed -E 's!(duration\\|n_packets\\|n_bytes)=[^ ]+!\\1=CLEANED!g'</code>   please consider: <code>--no-stats</code> List contract table <code>conntrack -L</code> List iptables <code>iptables-save</code> List NETID's <code>oc get netnamespaces</code> List host subnets <code>oc get hostsubnets</code> List cluster network <code>oc get clusternetwork</code>","tags":["NetworkPolicy","OpenShiftSDN"]},{"location":"networking/services-routes/route-encryption/","title":"Route encryption","text":""},{"location":"networking/services-routes/route-encryption/#edge","title":"Edge","text":"<pre><code>kind: Route\napiVersion: route.openshift.io/v1\nmetadata:\n  name: test2\nspec:\n  to:\n    kind: Service\n    name: test\n    weight: 100\n  port:\n    targetPort: 80\n  tls:\n    termination: edge\n    insecureEdgeTerminationPolicy: Redirect\n</code></pre>"},{"location":"networking/services-routes/router-sharding/","title":"Router sharding example","text":""},{"location":"networking/services-routes/router-sharding/#create-a-new-router-shared","title":"Create a new router shared","text":"<p>Take all routes with label <code>type=sharded</code></p> <pre><code>oc create -f - &lt;&lt;EOF\napiVersion: operator.openshift.io/v1\nkind: IngressController\nmetadata:\n  name: sharded\n  namespace: openshift-ingress-operator\nspec:\n  domain: external.demo.openshift.pub\n  endpointPublishingStrategy:\n    type: NodePortService\n  nodePlacement:\n    nodeSelector:\n      matchLabels:\n        node-role.kubernetes.io/worker: \"\"\n  routeSelector:\n    matchLabels:\n      type: sharded\nstatus: {}\nEOF\n</code></pre>"},{"location":"networking/services-routes/router-sharding/#adjust-default-router-to-skip-type-sharded","title":"Adjust default router to skip <code>type: sharded</code>","text":"<pre><code>oc patch \\\n  -n openshift-ingress-operator \\\n  IngressController/default \\\n  --type='merge' \\\n  -p '{\"spec\":{\"routeSelector\":{\"matchExpressions\":[{\"key\":\"type\",\"operator\":\"NotIn\",\"values\":[\"sharded\"]}]}}}'\n</code></pre>"},{"location":"networking/services-routes/router-sharding/#dummy-routes","title":"Dummy routes","text":"<pre><code>oc new-project demo\noc create service clusterip dummy --tcp=8080:8080\noc expose svc/dummy -l type=sharded --name dummy-shared\noc expose svc/dummy --name dummy-default\n</code></pre> <p>Check routes</p> <pre><code>$ oc describe route -n demo dummy-default\nName:           dummy-default\nNamespace:      demo\nCreated:        2 minutes ago\nLabels:         app=dummy\nAnnotations:        openshift.io/host.generated=true\nRequested Host:     dummy-default-demo.apps.demo.openshift.pub\n              exposed on router default (host apps.demo.openshift.pub) 2 minutes ago\nPath:           &lt;none&gt;\nTLS Termination:    &lt;none&gt;\nInsecure Policy:    &lt;none&gt;\nEndpoint Port:      8080-8080\n\nService:    dummy\nWeight:     100 (100%)\nEndpoints:  &lt;none&gt;\n\n$ oc describe route -n demo dummy-shared\nName:           dummy-shared\nNamespace:      demo\nCreated:        3 minutes ago\nLabels:         type=sharded\nAnnotations:        openshift.io/host.generated=true\nRequested Host:     dummy-shared-demo.apps.demo.openshift.pub\n              exposed on router sharded (host external.demo.openshift.pub) 2 minutes ago\nPath:           &lt;none&gt;\nTLS Termination:    &lt;none&gt;\nInsecure Policy:    &lt;none&gt;\nEndpoint Port:      8080-8080\n\nService:    dummy\nWeight:     100 (100%)\nEndpoints:  &lt;none&gt;\n\n\n\n$ oc rsh -n openshift-ingress deployment/router-sharded cat os_http_be.map\n^dummy-shared-demo\\.apps\\.demo\\.openshift\\.pub(:[0-9]+)?(/.*)?$ be_http:demo:dummy-shared\n\n$ oc rsh -n openshift-ingress deployment/router-default cat os_http_be.map\n^dummy-default-demo\\.apps\\.demo\\.openshift\\.pub(:[0-9]+)?(/.*)?$ be_http:demo:dummy-default\n</code></pre> <p>One downside: all routes auto generated with default domain:</p> <pre><code>$ oc get ingresses.config.openshift.io/cluster -o jsonpath=\"{.spec.domain}\"\napps.demo.openshift.pub\n</code></pre>"},{"location":"networking/services-routes/service-serving-certificate-secrets/","title":"Service Serving Certificate Secrets Example","text":"<p>Documentation</p>","tags":["certificates"]},{"location":"networking/services-routes/service-serving-certificate-secrets/#example-service","title":"Example Service","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: service-serving-cert\n  annotations:\n    service.alpha.openshift.io/serving-cert-secret-name: service-serving-cert\nspec:\n  ports:\n  - name: service-serving-cert\n    port: 443\n    targetPort: 8443\n  selector:\n    app: service-serving-cert\n</code></pre>","tags":["certificates"]},{"location":"networking/services-routes/service-serving-certificate-secrets/#check-certificate","title":"Check certificate","text":"<pre><code>oc get secret service-serving-cert -o json | jq -r '.data.\"tls.crt\"' | base64 --decode &gt; service-serving-cert.pem\n\nopenssl crl2pkcs7 -nocrl -certfile service-serving-cert.pem | openssl pkcs7 -print_certs  -noout\n</code></pre>","tags":["certificates"]},{"location":"networking/services-routes/service-serving-certificate-secrets/#example-commands","title":"Example commands","text":"<pre><code>$ echo \"apiVersion: v1\nkind: Service\nmetadata:\n  name: service-serving-cert\n  annotations:\n    service.alpha.openshift.io/serving-cert-secret-name: service-serving-cert\nspec:\n  ports:\n  - name: service-serving-cert\n    port: 443\n    targetPort: 8443\n  selector:\n    app: service-serving-cert\" | oc create -f -\nservice/service-serving-cert created\n$ oc get secret service-serving-cert -o json | jq -r '.data.\"tls.crt\"' | base64 --decode &gt; service-serving-cert.pem\n$ openssl crl2pkcs7 -nocrl -certfile service-serving-cert.pem | openssl pkcs7 -print_certs  -noout\nsubject=/CN=service-serving-cert.rbo.svc\nissuer=/CN=openshift-service-serving-signer@1545507973\n\nsubject=/CN=openshift-service-serving-signer@1545507973\nissuer=/CN=openshift-service-serving-signer@1545507973\n</code></pre>","tags":["certificates"]},{"location":"networking/services-routes/service-serving-certificate-secrets/#create-config-map-with-service-serving-root-ca","title":"Create config map with service serving root ca","text":"<pre><code>$ oc create -f - &lt;&lt;EOF\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: service-trustbundle-ca\n  annotations:\n    service.beta.openshift.io/inject-cabundle: \"true\"\ndata: {}\nEOF\n\n$ oc get configmap/service-trustbundle-ca -o jsonpath=\"{.data.service-ca\\.crt}\"  | openssl x509 -noout -subject -issuer -dates\nsubject= /CN=openshift-service-serving-signer@1593524307\nissuer= /CN=openshift-service-serving-signer@1593524307\nnotBefore=Jun 30 13:38:26 2020 GMT\nnotAfter=Aug 29 13:38:27 2022 GMT\n</code></pre>","tags":["certificates"]},{"location":"networking/sr-iov/deployment-sriov-eno4/","title":"Deployment sriov eno4","text":"<pre><code>          dpdk-devbind.py --status-dev net\n\n          dpdk-hugepages.py -s\n\n          sleep infinity\n</code></pre> <p>triggers:   - type: ConfigChange</p>"},{"location":"networking/sr-iov/sriov/","title":"Sriov","text":"<p><pre><code>oc get sriovnetworknodestates.sriovnetwork.openshift.io/storm5-10g.ocp5.stormshift.coe.muc.redhat.com -o yaml\napiVersion: sriovnetwork.openshift.io/v1\nkind: SriovNetworkNodeState\nmetadata:\n  creationTimestamp: \"2022-02-04T15:47:10Z\"\n  generation: 2\n  name: storm5-10g.ocp5.stormshift.coe.muc.redhat.com\n  namespace: openshift-sriov-network-operator\n  ownerReferences:\n  - apiVersion: sriovnetwork.openshift.io/v1\n    blockOwnerDeletion: true\n    controller: true\n    kind: SriovNetworkNodePolicy\n    name: default\n    uid: 28a1d113-c809-4e49-9ae8-1c315b548375\n  resourceVersion: \"1415300823\"\n  uid: 12ca11b3-30b5-4b17-b219-257d023c8d49\nspec:\n  dpConfigVersion: \"1415276305\"\n  interfaces:\n  - mtu: 1500\n    name: eno4\n    numVfs: 7\n    pciAddress: \"0000:01:00.3\"\n    vfGroups:\n    - deviceType: vfio-pci\n      mtu: 1500\n      policyName: storm5\n      resourceName: storm5\n      vfRange: 0-6\nstatus:\n  interfaces:\n  - deviceID: \"1521\"\n    driver: igb\n    linkSpeed: 1000 Mb/s\n    linkType: ETH\n    mac: 24:6e:96:5a:64:64\n    mtu: 1500\n    name: eno1\n    pciAddress: \"0000:01:00.0\"\n    totalvfs: 7\n    vendor: \"8086\"\n  - deviceID: \"1521\"\n    driver: igb\n    linkSpeed: 1000 Mb/s\n    linkType: ETH\n    mac: 24:6e:96:5a:64:65\n    mtu: 1500\n    name: eno2\n    pciAddress: \"0000:01:00.1\"\n    totalvfs: 7\n    vendor: \"8086\"\n  - deviceID: \"1521\"\n    driver: igb\n    linkSpeed: 1000 Mb/s\n    linkType: ETH\n    mac: 24:6e:96:5a:64:66\n    mtu: 1500\n    name: eno3\n    pciAddress: \"0000:01:00.2\"\n    totalvfs: 7\n    vendor: \"8086\"\n  - Vfs:\n    - deviceID: \"1520\"\n      driver: vfio-pci\n      pciAddress: \"0000:01:10.3\"\n      vendor: \"8086\"\n      vfID: 0\n    - deviceID: \"1520\"\n      driver: vfio-pci\n      pciAddress: \"0000:01:10.7\"\n      vendor: \"8086\"\n      vfID: 1\n    - deviceID: \"1520\"\n      driver: vfio-pci\n      pciAddress: \"0000:01:11.3\"\n      vendor: \"8086\"\n      vfID: 2\n    - deviceID: \"1520\"\n      driver: vfio-pci\n      pciAddress: \"0000:01:11.7\"\n      vendor: \"8086\"\n      vfID: 3\n    - deviceID: \"1520\"\n      driver: vfio-pci\n      pciAddress: \"0000:01:12.3\"\n      vendor: \"8086\"\n      vfID: 4\n    - deviceID: \"1520\"\n      driver: vfio-pci\n      pciAddress: \"0000:01:12.7\"\n      vendor: \"8086\"\n      vfID: 5\n    - deviceID: \"1520\"\n      driver: vfio-pci\n      pciAddress: \"0000:01:13.3\"\n      vendor: \"8086\"\n      vfID: 6\n    deviceID: \"1521\"\n    driver: igb\n    linkSpeed: 1000 Mb/s\n    linkType: ETH\n    mac: 24:6e:96:5a:64:67\n    mtu: 1500\n    name: eno4\n    numVfs: 7\n    pciAddress: \"0000:01:00.3\"\n    totalvfs: 7\n    vendor: \"8086\"\n  - deviceID: \"1528\"\n    driver: ixgbe\n    linkSpeed: 10000 Mb/s\n    linkType: ETH\n    mac: a0:36:9f:e5:14:bc\n    mtu: 9000\n    name: enp132s0f0\n    pciAddress: 0000:84:00.0\n    totalvfs: 63\n    vendor: \"8086\"\n  - deviceID: \"1528\"\n    driver: ixgbe\n    linkSpeed: 10000 Mb/s\n    linkType: ETH\n    mac: a0:36:9f:e5:14:bc\n    mtu: 9000\n    name: enp132s0f1\n    pciAddress: 0000:84:00.1\n    totalvfs: 63\n    vendor: \"8086\"\n  syncStatus: Succeeded\n  ```\n</code></pre> oc apply -f SriovNetworkNodePolicy.yaml Error from server (vendor/device 8086/1521 is not supported): error when creating \"SriovNetworkNodePolicy.yaml\": admission webhook \"operator-webhook.sriovnetwork.openshift.io\" denied the request: vendor/device 8086/1521 is not supported <pre><code> =&gt; Disable webhook:\n</code></pre> oc patch sriovoperatorconfig default --type=merge \\   -n openshift-sriov-network-operator \\   --patch '{ \"spec\": { \"enableOperatorWebhook\": false } }' <pre><code>\n</code></pre> sh-4.4# ethtool -i eno3 driver: igb version: 4.18.0-305.30.1.el8_4.x86_64 firmware-version: 1.67, 0x80000faa, 19.5.12 expansion-rom-version: bus-info: 0000:01:00.2 supports-statistics: yes supports-test: yes supports-eeprom-access: yes supports-register-dump: yes supports-priv-flags: yes</p> <p>sh-4.4# ip link show dev eno3 5: eno3:  mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000     link/ether 24:6e:96:5a:64:66 brd ff:ff:ff:ff:ff:ff     vf 0     link/ether 0a:3f:85:3d:a9:5d brd ff:ff:ff:ff:ff:ff, spoof checking on, link-state auto, trust off     vf 1     link/ether 3a:10:db:21:c2:9f brd ff:ff:ff:ff:ff:ff, spoof checking on, link-state auto, trust off     vf 2     link/ether 8e:f4:5f:5c:09:52 brd ff:ff:ff:ff:ff:ff, spoof checking on, link-state auto, trust off     vf 3     link/ether 5e:a1:54:68:99:2c brd ff:ff:ff:ff:ff:ff, spoof checking on, link-state auto, trust off     vf 4     link/ether 96:a8:0d:82:35:3c brd ff:ff:ff:ff:ff:ff, spoof checking on, link-state auto, trust off     vf 5     link/ether ba:15:a0:26:56:66 brd ff:ff:ff:ff:ff:ff, spoof checking on, link-state auto, trust off     vf 6     link/ether 6a:10:12:1c:e7:da brd ff:ff:ff:ff:ff:ff, spoof checking on, link-state auto, trust off sh-4.4# ip link set eno3 vf 0 state enable RTNETLINK answers: Operation not supported sh-4.4# <p>```</p> <p>oc new-build --name=dpdk-app-centos \\   --to-docker=true \\   --context-dir=samples/dpdk_app/dpdk-app-centos/ \\   https://github.com/openshift/app-netutil.git</p>"},{"location":"networking/sr-iov/sriov/#dpdk-ready","title":"DPDK ready","text":"<ul> <li>https://access.redhat.com/solutions/5688941</li> </ul> <p>https://www.youtube.com/watch?v=wbL0ap9U4G4 https://docs.google.com/presentation/d/1cGGHuzxJakFCfYVSpxpMKGKXB8JsgUYZGo7UN6Vjr8c/edit#slide=id.g1110007820d_0_558 https://docs.openshift.com/container-platform/4.9/scalability_and_performance/cnf-create-performance-profiles.html</p> <p>Message:               failed to find MCP with the node selector that matches labels \"kubernetes.io/hostname=storm5-10g.ocp5.stormshift.coe.muc.redhat.com\"</p> <p>oc label node storm5-10g.ocp5.stormshift.coe.muc.redhat.com node-role.kubernetes.io/dpdk= oc apply -f MCP-dpdk.yaml oc apply -f PerformanceProfile-dpdk-ready.yaml</p> <p>oc adm must-gather --image=registry.redhat.io/openshift4/performance-addon-operator-must-gather-rhel8:v4.9 --dest-dir=must-gather</p>"},{"location":"networking/udn/","title":"User-defined networks (UDN)","text":"<p>Official documentation:</p> <ul> <li>16.2.1. About user-defined networks</li> </ul> <p>Resources:</p> <ul> <li>https://github.com/maiqueb/fosdem2025-p-udn/tree/main</li> <li>https://asciinema.org/a/699323</li> <li><code>203.0.113.0/24</code> - IANA IPv4 Special-Purpose Address Registry</li> </ul> <p>Tested with:</p> Component Version OpenShift v4.18.8 OpenShift Virt v4.18.2 <p></p>","tags":["UDN","v4.18"]},{"location":"networking/udn/#namespaces","title":"Namespaces","text":"Tanent Namespace Color UDN or CUDN P-UDN IP Range tanent-1 namespace-1 red UDN (udn-1) <code>192.0.2.0/24</code> tanent-2 namespace-2 green UDN (udn-2) <code>10.255.0.0/16</code> tanent-3 namespace-3 blue CUDN (cudn-3) <code>203.0.113.0/24</code> tanent-4 namespace-4 orange CUDN (cudn-3) <code>203.0.113.0/24</code>","tags":["UDN","v4.18"]},{"location":"networking/udn/#deploy","title":"Deploy","text":"<pre><code>oc apply -k overlays/tentant-1\noc apply -k overlays/tentant-2\noc apply -k overlays/tentant-3\n</code></pre> <pre><code>$ oc get namespaces -l tentant -L tentant\nNAME          STATUS   AGE     TENTANT\nnamespace-1   Active   4m1s    tentant-1\nnamespace-2   Active   3m51s   tentant-2\nnamespace-3   Active   3m14s   tentant-3\nnamespace-4   Active   3m13s   tentant-3\n\n$ oc get pods -o go-template-file=podlist-with-p-udn.gotemplate -A -l tentant  | jq ' (.[] | [.node,.namespace, .udn[1].ips[0], .udn[0].ips[0], .name]) | @tsv' -r\nocp1-worker-0   namespace-1     192.0.2.10      10.131.0.201    agnhost-7f79bb7dc-t8rfg\nocp1-worker-0   namespace-1     192.0.2.9       10.131.0.202    rhel-support-tools-7c89889f94-wq2gj\nocp1-worker-1   namespace-1     192.0.2.17      10.128.3.55     simple-http-server-bb9ccffd4-74j47\nocp1-worker-2   namespace-1     192.0.2.11      10.129.2.131    simple-http-server-bb9ccffd4-jq4bb\nocp1-worker-0   namespace-1     192.0.2.16      10.131.0.223    simple-http-server-bb9ccffd4-xll6x\nocp1-worker-2   namespace-1     192.0.2.19      10.129.2.143    virt-launcher-simple-httpd-vm-5wxpj\nocp1-worker-0   namespace-2     10.255.2.6      10.131.0.220    agnhost-59964fb864-hp46z\nocp1-worker-0   namespace-2     10.255.2.8      10.131.0.221    rhel-support-tools-7cfb68d78f-89jkl\nocp1-worker-0   namespace-2     10.255.2.7      10.131.0.222    simple-http-server-7c567b8c4c-2pph6\nocp1-worker-1   namespace-2     10.255.3.4      10.128.3.54     simple-http-server-7c567b8c4c-brqtl\nocp1-worker-2   namespace-2     10.255.0.4      10.129.2.137    simple-http-server-7c567b8c4c-mjgwc\nocp1-worker-1   namespace-2     10.255.3.8      10.128.3.62     virt-launcher-simple-httpd-vm-w6hwn\nocp1-worker-2   namespace-3     203.0.113.22    10.129.2.135    agnhost-f4b987769-kcmvs\nocp1-worker-0   namespace-3     203.0.113.25    10.131.0.207    rhel-support-tools-5b999555b4-w2qv5\nocp1-worker-2   namespace-3     203.0.113.23    10.129.2.134    simple-http-server-6b84977478-7kkhd\nocp1-worker-1   namespace-3     203.0.113.29    10.128.3.56     simple-http-server-6b84977478-pbj4d\nocp1-worker-0   namespace-3     203.0.113.30    10.131.0.224    simple-http-server-6b84977478-wn6kl\nocp1-worker-0   namespace-3     203.0.113.39    10.131.1.39     virt-launcher-simple-httpd-vm-h8xr7\nocp1-worker-0   namespace-4     203.0.113.26    10.131.0.203    agnhost-f4b987769-vxtl7\nocp1-worker-0   namespace-4     203.0.113.24    10.131.0.204    rhel-support-tools-5b999555b4-5kgbs\nocp1-worker-0   namespace-4     203.0.113.33    10.131.0.225    simple-http-server-6b84977478-mqhj4\nocp1-worker-2   namespace-4     203.0.113.21    10.129.2.133    simple-http-server-6b84977478-rnc2c\nocp1-worker-1   namespace-4     203.0.113.35    10.128.3.57     simple-http-server-6b84977478-v7xhz\nocp1-worker-2   namespace-4     203.0.113.40    10.129.2.142    virt-launcher-simple-httpd-vm-lk4rc\n$ oc get vmi -l tentant -A\nNAMESPACE     NAME              AGE     PHASE     IP             NODENAME        READY\nnamespace-1   simple-httpd-vm   93s     Running   192.0.2.19     ocp1-worker-2   True\nnamespace-2   simple-httpd-vm   87s     Running   10.255.3.8     ocp1-worker-1   True\nnamespace-3   simple-httpd-vm   2m43s   Running   203.0.113.39   ocp1-worker-0   True\nnamespace-4   simple-httpd-vm   2m42s   Running   203.0.113.40   ocp1-worker-2   True\n</code></pre> <ul> <li>Why is ip's of UDN nocht in pod status <code>podIPs</code> ?</li> </ul>","tags":["UDN","v4.18"]},{"location":"networking/udn/#testing","title":"Testing","text":"","tags":["UDN","v4.18"]},{"location":"networking/udn/#network-policy","title":"\u2705 Network Policy","text":"<pre><code>$ oc get vmi -l tentant=tentant -A\nNAMESPACE     NAME              AGE   PHASE     IP             NODENAME        READY\nnamespace-3   simple-httpd-vm   32m   Running   203.0.113.41   ocp1-worker-0   True\nnamespace-4   simple-httpd-vm   32m   Running   203.0.113.42   ocp1-worker-2   True\n$  virtctl console -n namespace-3 simple-httpd-vm\nSuccessfully connected to simple-httpd-vm console. The escape sequence is ^]\n\nsimple-httpd-vm login: fedora\nPassword:\nLast login: Wed May  7 06:57:10 on ttyS0\n[systemd]\nFailed Units: 1\n  cloud-final.service\n[fedora@simple-httpd-vm ~]$ ip -br a\nlo               UNKNOWN        127.0.0.1/8 ::1/128\neth0             UP             203.0.113.41/24 fe80::858:cbff:fe00:7129/64\n[fedora@simple-httpd-vm ~]$ ping -c4 203.0.113.42\nPING 203.0.113.42 (203.0.113.42) 56(84) bytes of data.\n64 bytes from 203.0.113.42: icmp_seq=1 ttl=64 time=7.58 ms\n64 bytes from 203.0.113.42: icmp_seq=2 ttl=64 time=9.58 ms\n64 bytes from 203.0.113.42: icmp_seq=3 ttl=64 time=1.47 ms\n64 bytes from 203.0.113.42: icmp_seq=4 ttl=64 time=1.80 ms\n\n--- 203.0.113.42 ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 3007ms\nrtt min/avg/max/mdev = 1.474/5.107/9.581/3.545 ms\n[fedora@simple-httpd-vm ~]$ curl -I http://203.0.113.42/\nHTTP/1.1 403 Forbidden\nDate: Wed, 07 May 2025 07:06:54 GMT\nServer: Apache/2.4.46 (Fedora)\nLast-Modified: Tue, 28 Jan 2020 18:21:43 GMT\nETag: \"15bc-59d374bbd1bc0\"\nAccept-Ranges: bytes\nContent-Length: 5564\nContent-Type: text/html; charset=UTF-8\n</code></pre> <p>Apply Network Policy</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-other-namespaces\n  namespace: namespace-4\nspec:\n  podSelector: null\n  ingress:\n    - from:\n        - podSelector: {}\n</code></pre> <pre><code>[fedora@simple-httpd-vm ~]$ curl --connect-timeout 1 -I http://203.0.113.42/\ncurl: (28) Connection timed out after 1001 milliseconds\n[fedora@simple-httpd-vm ~]$\n</code></pre>","tags":["UDN","v4.18"]},{"location":"networking/udn/#multinetworkpolicy","title":"\u23f1\ufe0f MultiNetworkPolicy","text":"<ul> <li>Not tested yet</li> </ul>","tags":["UDN","v4.18"]},{"location":"networking/udn/#services","title":"\u2705/\u26a0\ufe0f Services","text":"<pre><code>$ oc get svc -n namespace-3\nNAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nagnhost              ClusterIP   172.30.222.33    &lt;none&gt;        9000/TCP   16h\nsimple-http-server   ClusterIP   172.30.209.160   &lt;none&gt;        8080/TCP   16h\nsimple-httpd-vm      ClusterIP   172.30.196.254   &lt;none&gt;        80/TCP     66m\n[fedora@simple-httpd-vm ~]$ getent ahosts simple-httpd-vm.namespace-3\nr.local\n172.30.196.254  STREAM simple-httpd-vm.namespace-3.svc.cluster.local\n172.30.196.254  DGRAM\n172.30.196.254  RAW\n[fedora@simple-httpd-vm ~]$ curl http://simple-httpd-vm.namespace-3.svc.cluster.local.\ncurl: (7) Failed to connect to simple-httpd-vm.namespace-3.svc.cluster.local port 80: Connection refused\n</code></pre> <ul> <li>\u274c Pod -&gt; Service -&gt; VM with L2Bridge in UDN</li> <li>\u2705 Pod -&gt; Service -&gt; Pod</li> </ul>","tags":["UDN","v4.18"]},{"location":"networking/udn/#ingress","title":"\u2705 Ingress","text":"<p>Works only because of pod annotation</p> <pre><code>        k8s.ovn.org/open-default-ports: |\n          - protocol: tcp\n            port: 8080\n</code></pre>","tags":["UDN","v4.18"]},{"location":"networking/udn/#livenessreadyness-probes","title":"\u2705 Liveness/Readyness probes","text":"<pre><code>oc rsh simple-http-server-6b84977478-chvj9\nsh-5.1$ rm /www/readiness-probe\nrm: remove write-protected regular empty file '/www/readiness-probe'? y\nsh-5.1$\n\n$ oc get pods --watch\nsimple-http-server-6b84977478-chvj9   1/1     Running             0             4s\nsimple-http-server-6b84977478-chvj9   0/1     Running             0             19s\n</code></pre>","tags":["UDN","v4.18"]},{"location":"networking/udn/#direct-access","title":"\u2705 Direct access","text":"<p>Check network policy part</p>","tags":["UDN","v4.18"]},{"location":"networking/udn/#vm-live-migration","title":"\u2705 VM Live Migration","text":"<pre><code>$ oc get vmi --watch\nNAME              AGE   PHASE     IP             NODENAME        READY\nsimple-httpd-vm   66m   Running   203.0.113.41   ocp1-worker-0   True\nsimple-httpd-vm   67m   Running   203.0.113.41   ocp1-worker-0   True\nsimple-httpd-vm   67m   Running   203.0.113.41   ocp1-worker-0   True\nsimple-httpd-vm   67m   Running   203.0.113.41   ocp1-worker-0   True\nsimple-httpd-vm   67m   Running   203.0.113.41   ocp1-worker-0   True\nsimple-httpd-vm   67m   Running   203.0.113.41   ocp1-worker-0   True\nsimple-httpd-vm   67m   Running   203.0.113.41   ocp1-worker-2   True\nsimple-httpd-vm   67m   Running   203.0.113.41   ocp1-worker-2   False\nsimple-httpd-vm   67m   Running   203.0.113.41   ocp1-worker-2   True\n</code></pre>","tags":["UDN","v4.18"]},{"location":"networking/udn/#secondary-udn","title":"\u23f1\ufe0f Secondary-UDN","text":"<ul> <li>Not tested yet</li> </ul>","tags":["UDN","v4.18"]},{"location":"networking/udn/#layer2","title":"\u2705 Layer2","text":"<ul> <li>Tentant 1 is Layer 2</li> </ul>","tags":["UDN","v4.18"]},{"location":"networking/udn/#layer3","title":"\u2705 Layer3","text":"<ul> <li>Tentant 2 is Layer 3</li> </ul>","tags":["UDN","v4.18"]},{"location":"networking/udn/#localnet-available-with-419","title":"\u23f1\ufe0f Localnet (Available with 4.19)","text":"","tags":["UDN","v4.18"]},{"location":"openshift-tv/","title":"OpenShift TV","text":"<p>https://www.youtube.com/c/OpenShift/live</p>"},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/","title":"vSphere IPI &amp; disconnected environment","text":"<p>Note</p> <p>This is not a complete documentation or copy &amp; past ready documentation! At least speaker notes or my personal notes, highlight only some key points. A complete documentation is available at docs.openshift.com or docs.redhat.com</p>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#recording","title":"Recording","text":"","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#definition","title":"Definition","text":"<ul> <li>disconnected: Cluster has no access to the internet DIRECTLY (firewall rules are in place). They may or maynot have a proxy.</li> <li>restricted: Cluster has SOME access to the internet (firewall rules allow some but not all). Install may fail. They may or maynot have a proxy. Term used in the documentation!</li> <li>airgapped. Cluster has NO access to the internet. Full Stop. Not even the router or switches or anything have access. Think of a military install.</li> </ul> <p>Kudos to Christian Hernandez</p>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#lab-information","title":"Lab information","text":"<ul> <li>vSphere 7</li> <li>Domain: example.com</li> <li>Network: <code>172.16.0.0/24</code><ul> <li><code>172.16.0.4</code> - jumphost-disconnected.example.com (proxy in the env., ntpd,   vcenter forwarder)</li> <li><code>172.16.0.5</code> - quay.example.com (image registry, dns, dhcp, git-server,   httpd)</li> <li>VIPS:</li> <li><code>172.16.0.10</code> - api.infra.example.com</li> <li><code>172.16.0.11</code> - *.apps.infra.example.com</li> <li><code>172.16.0.12</code> - api.demo1.example.com</li> <li><code>172.16.0.13</code> - *.apps.demo1.example.com</li> <li><code>172.16.0.14</code> - api.demo1.example.com</li> <li><code>172.16.0.15</code> - *.apps.demo1.example.com</li> </ul> </li> </ul>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#overview","title":"Overview","text":"<ul> <li>OpenShift installation<ul> <li>Mirror release images</li> <li>Mirror rhcos ova</li> <li>Prepare install-config.yaml<ul> <li>Important: imageContentSources \ud83d\udd34, additionalTrustBundle,   platform.vsphere.clusterOSImage (check <code>openshift-install explain</code>)</li> </ul> </li> <li>Run installation</li> </ul> </li> <li>OpenShift post-installation steps<ul> <li>Configure registry search path &amp; ca \ud83d\udca1</li> </ul> </li> <li>OperatorHub / OLM<ul> <li>Disable default catalog sources</li> <li>Create operator index</li> <li>Mirror operator images</li> <li>Apply operator source &amp; imageContentSourcePolicy</li> </ul> </li> <li>OpenShift Upgrade<ul> <li>Check update path</li> <li>Mirror release image &amp; image signature</li> <li>Run upgrade via cli</li> <li>Futur: OpenShift Update Service</li> </ul> </li> <li>Demo: Run a pipeline<ul> <li>OpenShift Pipeline operator is synced and installed</li> <li>Copy all necessary git resources into your own git-server</li> <li>Mirror all necessary container images</li> <li>Run the pipeline and fail because of missing pip mirror \ud83d\udd34</li> </ul> </li> </ul>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#documentation-use-full-resources","title":"Documentation &amp; use-full resources","text":"<ul> <li>Installing a cluster on vSphere in a restricted network</li> <li>Updating a restricted network cluster</li> <li>Using Operator Lifecycle Manager on restricted networks</li> <li>Creating CI/CD solutions for applications using OpenShift Pipelines</li> <li>OpenShift Update Path</li> <li>OpenShift Update Service</li> <li>Container images, multi-architecture, manifests, ids, digests \u2013 what\u2019s behind?</li> </ul>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#openshift-installation","title":"OpenShift Installation","text":"","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#mirror-relrease-images","title":"Mirror relrease images","text":"<pre><code>$ cat env.sh\nexport OCP_RELEASE=4.7.0\nexport LOCAL_REGISTRY='quay.example.com'\nexport LOCAL_REPOSITORY='infra/openshift4'\nexport PRODUCT_REPO='openshift-release-dev'\nexport LOCAL_SECRET_JSON='pullsecret.json'\nexport RELEASE_NAME=\"ocp-release\"\nexport ARCHITECTURE=x86_64\n\n$ ./env.sh\n\n$ oc adm release mirror  -a ${LOCAL_SECRET_JSON} \\\n  --from=quay.io/${PRODUCT_REPO}/${RELEASE_NAME}:${OCP_RELEASE}-${ARCHITECTURE} \\\n  --to=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY} \\\n  --to-release-image=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY}:${OCP_RELEASE}-${ARCHITECTURE}\n\n....\nimageContentSources:\n- mirrors:\n  - quay.example.com/infra/openshift4\n  source: quay.io/openshift-release-dev/ocp-release- mirrors:\n  - quay.example.com/infra/openshift4\n  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n</code></pre>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#install-openshift","title":"Install OpenShift","text":"<p>Example install-config <pre><code>apiVersion: v1\nbaseDomain: example.com\ncompute:\n- architecture: amd64\n  hyperthreading: Enabled\n  name: worker\n  platform: {}\n  replicas: 3\ncontrolPlane:\n  architecture: amd64\n  hyperthreading: Enabled\n  name: master\n  platform: {}  replicas: 3metadata:\n  creationTimestamp: null\n  name: infra\nnetworking:\n  clusterNetwork:\n  - cidr: 10.128.0.0/14\n    hostPrefix: 23\n  machineNetwork:\n  - cidr: 10.0.0.0/16\n  networkType: OpenShiftSDN\n  serviceNetwork:\n  - 172.30.0.0/16\nplatform:\n  vsphere:\n    apiVIP: 172.16.0.10\n    cluster: lab\n    datacenter: DC\n    defaultDatastore: datastore\n    ingressVIP: 172.16.0.11\n    network: Disconnected\n    password: xxx\n    username: xxx\n    vCenter: vcenter.example.com\n    folder: /DC/vm/rbohne/\n    clusterOSImage: http://quay.example.com:8080/rhcos-4.7.0-x86_64-vmware.x86_64.ova?sha256=13d92692b8eed717ff8d0d113a24add339a65ef1f12eceeb99dabcd922cc86d1\npublish: External\nimageContentSources:\n- mirrors:\n  - quay.example.com/infra/openshift4\n  source: quay.io/openshift-release-dev/ocp-release\n- mirrors:\n  - quay.example.com/infra/openshift4\n  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\npullSecret: '{\"auths\":{\"quay.example.com\":{\"auth\":\"YWRtaW46cmVkaGF0MDI=\"}}}'\nsshKey: \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCu9AV3k/ktphogW+Y28fZ0R+ncTxtXalVmpGjvZCuARZJQcgA72pNnXSvrTadJo5D48LgXOz5BwZnoml0toPkLVKBa4fU6kvQsQHDzvElpKbBH8/tmYRV72wt2kJjAS//Ycu9qz2scK+YAdjyle+WUh0qyEzgKKkLjwUmdZOYfJ0eZP+Jl5ljeXk3olCcAQc7JaBr3umREr5o/3+wnHsYPlOGZoSvGRTuEy81tQTL+Nl12LrN1ZxZQZ8jzIExIGRvk8/F2oufFfCigXkEiMf+8l2WXDVR/MGLVhEyle2tJAczBwaskh1nJFKfK6H88lm0fyVev9++GYClSvIxjBWmD88s09cei6C4gRSKANtANmtoOUNhVGcFXaJSPHnrOo7bKnU7XNAcmEZZtUuB05Oc1lJSNoBB9wDj65hzvWnyvQ/zgXeSmUjHObArZ054qPtscIV5QGQUdgVsRCgPWS9SlmYMje9O8AJe+Kqye3ykyMeRDZEUNvO+9Pg+ZGbgH7eM= root@jumphost-disconnected.localdomain\"\nadditionalTrustBundle: |\n  -----BEGIN CERTIFICATE-----\n  MIIEQzCCAyugAwIBAgIUVwbzbrQNDW3tU2xdZDVsF5VzTlEwDQYJKoZIhvcNAQEL\n  BQAwgagxCzAJBgNVBAYTAkRFMRAwDgYDVQQIDAdCYXZhcmlhMQ8wDQYDVQQHDAZN\n    ....\n  qGE5LlcUuR0SCO8yVcILcaiXhxGzqYlOZK26u4APntWyn4eUGuLpiReimgE4NvZr\n  ZYDgBfQ2I9ulhe0dAScUJz0c8iErjpJJ9kT0Ebar/UuPGQvyOnvM\n  -----END CERTIFICATE-----\n</code></pre></p> <p>Run <code>openshift-install create cluster</code></p>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#post-installation","title":"Post installation","text":"","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#configure-registry-search-path-ca","title":"Configure registry search path &amp; ca","text":"<p>https://docs.openshift.com/container-platform/4.7/openshift_images/image-configuration.html</p> <pre><code>oc create configmap additional-trusted-ca \\\n  --from-file=quay.example.com=/etc/pki/ca-trust/source/anchors/quay.ca.crt \\\n  -n openshift-config\n\noc apply -f - &lt;&lt;EOF\napiVersion: config.openshift.io/v1\nkind: Image\nmetadata:\n  name: cluster\nspec:\n  allowedRegistriesForImport:\n    - domainName: quay.example.com\n      insecure: false\n    # If not added update will fail\n    #  oc logs -n openshift-cluster-version -l k8s-app=cluster-version-operator\n    # I0413 08:12:21.986805       1 reflector.go:530] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: Watch close - *v1.Proxy total 0 items received\n    #E0413 08:12:22.072266       1 task.go:112] error running apply for imagestream \"openshift/cli\" (378 of 668): ImageStream.image.openshift.io \"cli\" is invalid: spec.tags[latest].from.name: Forbidden: registry \"quay.io\" not allowed by whitelist: \"image-registry.openshift-image-registry.svc:5000\", \"quay.example.com:443\"\n    #I0413 08:12:24.705418       1 leaderelection.go:273] successfully renewed lease openshift-cluster-version/version\n    - domainName: quay.io\n      insecure: false\n  additionalTrustedCA:\n    name: additional-trusted-ca\n  registrySources:\n    containerRuntimeSearchRegistries:\n    - quay.example.com\n    allowedRegistries:\n    - quay.example.com\n    - registry.redhat.io\n    - quay.io\n    - registry.access.redhat.com\n    - image-registry.openshift-image-registry.svc:5000\nEOF\n</code></pre>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#disable-catalog-sources","title":"Disable catalog sources","text":"<p>https://docs.openshift.com/container-platform/4.7/operators/admin/olm-restricted-networks.html#olm-restricted-networks-operatorhub_olm-restricted-networks</p> <pre><code>oc patch OperatorHub cluster --type json -p '[{\"op\": \"add\", \"path\": \"/spec/disableAllDefaultSources\", \"value\": true}]'\n</code></pre>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#configure-image-registry","title":"Configure image registry","text":"","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#create-pvc","title":"Create PVC","text":"<pre><code>oc create -f - &lt;&lt;EOF\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: image-registry-pvc\n  namespace: openshift-image-registry\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 100Gi\n  storageClassName: thin\nEOF\n</code></pre>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#patch-registry-operator-crd","title":"Patch registry operator crd","text":"<pre><code>oc patch configs.imageregistry.operator.openshift.io/cluster \\\n    --type='json' \\\n    --patch='[\n        {\"op\": \"replace\", \"path\": \"/spec/managementState\", \"value\": \"Managed\"},\n        {\"op\": \"replace\", \"path\": \"/spec/rolloutStrategy\", \"value\": \"Recreate\"},\n        {\"op\": \"replace\", \"path\": \"/spec/replicas\", \"value\": 1},\n        {\"op\": \"replace\", \"path\": \"/spec/storage\", \"value\": {\"pvc\":{\"claim\": \"image-registry-pvc\" }}}\n    ]'\n</code></pre>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#configure-ntp","title":"Configure NTP","text":"<pre><code>chronybase64=$(cat &lt;&lt; EOF | base64 -w 0\nserver 172.16.0.4 iburst\ndriftfile /var/lib/chrony/drift\nmakestep 1.0 3\nrtcsync\nkeyfile /etc/chrony.keys\nleapsectz right/UTC\nlogdir /var/log/chrony\nEOF\n)\n\noc apply -f - &lt;&lt; EOF\napiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfig\nmetadata:\n  labels:\n    machineconfiguration.openshift.io/role: worker\n  name: 50-worker-chrony\nspec:\n  config:\n    ignition:\n      version: 2.2.0\n    storage:\n      files:\n      - contents:\n          source: data:text/plain;charset=utf-8;base64,${chronybase64}\n        filesystem: root\n        mode: 0644\n        path: /etc/chrony.conf\nEOF\n\n\noc apply -f - &lt;&lt; EOF\napiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfig\nmetadata:\n  labels:\n    machineconfiguration.openshift.io/role: master\n  name: 50-master-chrony\nspec:\n  config:\n    ignition:\n      version: 2.2.0\n    storage:\n      files:\n      - contents:\n          source: data:text/plain;charset=utf-8;base64,${chronybase64}\n        filesystem: root\n        mode: 0644\n        path: /etc/chrony.conf\nEOF\n</code></pre>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#operator-installation","title":"Operator installation","text":"<ul> <li>https://bugzilla.redhat.com/show_bug.cgi?id=1874106</li> </ul> <pre><code>source env.sh\nexport REG_CREDS=${XDG_RUNTIME_DIR}/containers/auth.json\n\n# Login into all registries...\n\n\n\npodman run -p50051:50051 \\\n     -it registry.redhat.io/redhat/redhat-operator-index:v4.7\n\nopm index prune \\\n    -f registry.redhat.io/redhat/redhat-operator-index:v4.7 \\\n    -p advanced-cluster-management \\\n    -t quay.example.com/infra/redhat-operator-index:v4.7\n\n\ngrpcurl -plaintext localhost:50051 api.Registry/ListPackages &gt; packages.out\n\n grep -E '(advanc|pipeline)' packages.out\n\n\nopm index prune \\\n    -f registry.redhat.io/redhat/redhat-operator-index:v4.7 \\\n    -p advanced-cluster-management,openshift-pipelines-operator-rh,web-terminal   \\\n    -t quay.example.com/infra/redhat-operator-index:v4.7\n\npodman push quay.example.com/infra/redhat-operator-index:v4.7\n\n$ oc adm catalog mirror \\\n  -a ${REG_CREDS} \\\n  --manifests-only \\\n  --index-filter-by-os='.*' \\\n  quay.example.com/infra/redhat-operator-index:v4.7 \\\n  quay.example.com/infra\nsrc image has index label for database path: /database/index.db\nusing database path mapping: /database/index.db:/tmp/353326806\nwrote database to /tmp/353326806\nusing database at: /tmp/353326806/index.db\nno digest mapping available for quay.example.com/infra/redhat-operator-index:v4.7, skip writing to ImageContentSourcePolicy\nwrote mirroring manifests to manifests-redhat-operator-index-1618230619\n\ncd manifests-redhat-operator-index-1618230619\n\n\n\noc image mirror \\\n  --skip-multiple-scopes=true \\\n  -a ${REG_CREDS} \\\n  --filter-by-os='.*' \\\n  -f mapping.txt\n</code></pre>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#cluster-upgrade","title":"Cluster upgrade","text":"<p>https://docs.openshift.com/container-platform/4.7/updating/updating-restricted-network-cluster.html</p> <ul> <li>Check upgrade path: https://access.redhat.com/labs/ocpupgradegraph/update_path</li> <li>Mirror images again - new version</li> <li>Run oc adm oc adm upgrade --to ..</li> </ul>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#mirror","title":"Mirror","text":"<pre><code>source env.sh\nexport OCP_RELEASE=4.7.2\n\noc adm release mirror  -a ${LOCAL_SECRET_JSON} \\\n  --from=quay.io/${PRODUCT_REPO}/${RELEASE_NAME}:${OCP_RELEASE}-${ARCHITECTURE}  \\\n  --to=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY} \\\n  --apply-release-image-signature\n....\n</code></pre>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#update-command","title":"Update command","text":"<pre><code>oc adm release info -a $REG_CREDS quay.example.com/infra/openshift4:4.7.2-x86_64 | grep 'Pull From'\nPull From: quay.example.com/infra/openshift4@sha256:83fca12e93240b503f88ec192be5ff0d6dfe750f81e8b5ef71af991337d7c584\noc adm upgrade --allow-explicit-upgrade --to-image quay.example.com/infra/openshift4@sha256:83fca12e93240b503f88ec192be5ff0d6dfe750f81e8b5ef71af991337d7c584\n</code></pre>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#openshift-update-service","title":"OpenShift Update Service","text":"<p>https://www.openshift.com/blog/openshift-update-service-update-manager-for-your-cluster</p>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#demo-run-a-pipeline","title":"Demo: Run a pipeline","text":"<p>Creating Pipeline Tasks</p> <pre><code>oc create -f http://quay.example.com:3000/openshift/pipelines-tutorial/raw/pipelines-1.3/01_pipeline/01_apply_manifest_task.yaml\noc create -f http://quay.example.com:3000/openshift/pipelines-tutorial/raw/pipelines-1.3/01_pipeline/02_update_deployment_task.yaml\n\n\n\noc create -f http://quay.example.com:3000/openshift/pipelines-tutorial/raw/pipelines-1.3/01_pipeline/04_pipeline.yaml\n</code></pre>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#mirroring-images-to-run-pipelines-in-a-restricted-environment","title":"Mirroring images to run pipelines in a restricted environment","text":"<p>Mirroring images to run pipelines in a restricted environment</p> <pre><code>oc image mirror -a $REG_CREDS registry.redhat.io/ubi8/python-38:latest quay.example.com/ubi8/python-38:latest\noc image mirror -a $REG_CREDS registry.redhat.io/ubi8/go-toolset:1.14.7 quay.example.com/ubi8/go-toolset\n\noc tag quay.example.com/ubi8/python-38:latest python:latest --scheduled -n openshift\noc tag quay.example.com/ubi8/go-toolset:1.14.7  golang:latest --scheduled -n openshift\n\noc tag quay.example.com/infra/openshift4:4.7.0-x86_64-cli cli:latest --scheduled -n openshift\n</code></pre>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#run-pipeline","title":"Run pipeline","text":"<pre><code>tkn pipeline start build-and-deploy \\\n  -w name=shared-workspace,volumeClaimTemplateFile=http://quay.example.com:3000/openshift/pipelines-tutorial/raw/pipelines-1.3/01_pipeline/03_persistent_volume_claim.yaml \\\n  -p deployment-name=vote-api \\\n  -p git-url=http://quay.example.com:3000/openshift-pipelines/vote-api.git \\\n  -p IMAGE=image-registry.openshift-image-registry.svc:5000/pipeline/vote-api\n\ntkn pipeline start build-and-deploy \\\n  -w name=shared-workspace,volumeClaimTemplateFile=http://quay.example.com:3000/openshift/pipelines-tutorial/raw/pipelines-1.3/01_pipeline/03_persistent_volume_claim.yaml \\\n  -p deployment-name=vote-ui \\\n  -p git-url=http://quay.example.com:3000/openshift-pipelines/vote-ui.git \\\n  -p IMAGE=image-registry.openshift-image-registry.svc:5000/pipeline/vote-ui\n</code></pre>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#problem","title":"Problem","text":"<pre><code>[build-image : build] WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fdf07b05c10&gt;: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/flask/\n[build-image : build] WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fdf07b05280&gt;: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/flask/\n[build-image : build] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fdf07b05520&gt;: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/flask/\n[build-image : build] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fdf07b050d0&gt;: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/flask/\n[build-image : build] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fdf07afaf40&gt;: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/flask/\n[build-image : build] ERROR: Could not find a version that satisfies the requirement Flask (from -r requirements.txt (line 1)) (from versions: none)\n[build-image : build] ERROR: No matching distribution found for Flask (from -r requirements.txt (line 1))\n[build-image : build] subprocess exited with status 1\n[build-image : build] subprocess exited with status 1\n[build-image : build] error building at STEP \"RUN pip install -r requirements.txt\": exit status 1\n[build-image : build] level=error msg=\"exit status 1\"\n</code></pre> <p>Solution: You have to mirror....</p>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#appendix-setup-supporting-infrastructure","title":"Appendix - Setup supporting infrastructure","text":"","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#setup-quay","title":"Setup quay","text":"<p>Deploy Red Hat Quay for proof-of-concept (non-production) purposes</p>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#setup-git-server","title":"Setup Git-server","text":"<pre><code>mkdir /home/gogs\ncat &gt; /etc/systemd/system/gogs.service &lt;&lt;EOF\n[Unit]\nDescription=gogs\nAfter=network.target\n\n[Service]\nType=simple\nTimeoutStartSec=5m\n\nExecStartPre=-/usr/bin/podman rm gogs\nExecStart=/usr/bin/podman run --name gogs \\\n  -p 2222:22 \\\n  -p 3000:3000 \\\n  -v /home/gogs:/data:Z \\\n  docker.io/gogs/gogs:latest\n\nExecReload=-/usr/bin/podman stop gogs\nExecReload=-/usr/bin/podman rm gogs\nExecStop=-/usr/bin/podman stop gogs\nRestart=always\nRestartSec=30\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"openshift-tv/2021-04-14-vpshere-ipi-disconnected/#dns-dhcp","title":"DNS &amp; dhcp","text":"<p>Skiped, straigt foroward dnsmasq: <pre><code>[root@quay ~]# cat /etc/dnsmasq.d/openshift-4.conf\n#strict-order\ndomain=example.com\n#expand-hosts\n# Dynamisches DHCP\ndhcp-range=172.16.0.100,172.16.0.199,12h\n\naddress=/api.infra.example.com/172.16.0.10address=/apps.infra.example.com/172.16.0.11\n\naddress=/api.demo1.example.com/172.16.0.12\naddress=/apps.demo1.example.com/172.16.0.13\n</code></pre></p>","tags":["VMware","vSphere","air-gapped","restriced","disconnected","OpenShiftTV"]},{"location":"operators/","title":"Operators","text":""},{"location":"operators/#content","title":"Content","text":"<ul> <li> <p>Custom Resource Definition (CRD)</p> </li> <li> <p>Air-gapped/Disconnected</p> </li> <li> <p>Ansible Operator</p> </li> <li> <p>OperatorHub</p> </li> <li> <p>Install Operator as a User - WiP</p> </li> <li> <p>Cluster Configurator</p> </li> <li> <p>Simple Application Operator - WiP</p> </li> </ul>"},{"location":"operators/ansible-operator/","title":"Ansible Operator","text":""},{"location":"operators/ansible-operator/#ansible-operator-example","title":"Ansible Operator example","text":""},{"location":"operators/ansible-operator/#resources","title":"Resources","text":"<ul> <li>OperatorSDK ( Example )</li> </ul>"},{"location":"operators/ansible-operator/#create-first-ansible-operator","title":"Create first ansible operator","text":"<pre><code>mkdir sample-operator\ncd sample-operator\n\noperator-sdk init \\\n  --plugins=ansible.sdk.operatorframework.io/v1 \\\n  --domain=example.com \\\n  --group=app --version=v1alpha1 --kind=AppService \\\n  --generate-playbook \\\n  --generate-role\n\nmake docker-build docker-push \\\n  IMG=\"quay.io/openshift-examples/ansible-example-operator:latest\"\n\nmake deploy \\\n  IMG=\"quay.io/openshift-examples/ansible-example-operator:latest\"\n\nkubectl get pods -n sample-operator-system --watch\n\nkubectl apply -f config/samples/app_v1alpha1_appservice.yaml\n\nkubectl logs -n sample-operator-system \\\n    -l control-plane=controller-manager -c manager --tail=-1\n</code></pre>"},{"location":"operators/ansible-operator/#adjust-rolesappservicetasksmainyml","title":"Adjust <code>roles/appservice/tasks/main.yml</code>","text":"<pre><code>---\n# tasks\n- name: Print some debug information\n  vars:\n    msg: |\n        Module Variables (\"vars\"):\n        --------------------------------\n        {{ vars | to_nice_json }}\n\n        Environment Variables (\"environment\"):\n        --------------------------------\n        {{ environment | to_nice_json }}\n\n        GROUP NAMES Variables (\"group_names\"):\n        --------------------------------\n        {{ group_names | to_nice_json }}\n\n        GROUPS Variables (\"groups\"):\n        --------------------------------\n        {{ groups | to_nice_json }}\n\n        HOST Variables (\"hostvars\"):\n        --------------------------------\n        {{ hostvars | to_nice_json }}\n\n  debug:\n    msg: \"{{ msg.split('\\n') }}\"\n</code></pre>"},{"location":"operators/ansible-operator/#rebuild-and-redeploy","title":"Rebuild and redeploy","text":"<pre><code>make docker-build docker-push \\\n  IMG=\"quay.io/openshift-examples/ansible-example-operator:latest\"\n\nmake deploy \\\n  IMG=\"quay.io/openshift-examples/ansible-example-operator:latest\"\n</code></pre>"},{"location":"operators/ansible-operator/#cleanup","title":"Cleanup","text":"<pre><code>make undeploy\n</code></pre>"},{"location":"operators/cluster-configurator/","title":"How to apply your initial cluster configuration with OpenShift 4 ?","text":"<p>It is quite easy to deploy your OpenShift 4 cluster, if you don't know how easy it is check out the blog post: OpenShift 4: Install Experience</p> <p>To be honest, after the installation there are some customer-specific configurations, for examples authentication, custom certificates,you have to do. If you want to re-deploy on several clusters and save these changes no your Git repository, here are two posible ways to solve them.</p>"},{"location":"operators/cluster-configurator/#1-add-your-changes-at-installation-time","title":"1) Add your changes at installation time","text":""},{"location":"operators/cluster-configurator/#create-cluster-manifest","title":"Create cluster manifest","text":"<pre><code>openshift-install create manifests --dir=conf\n</code></pre>"},{"location":"operators/cluster-configurator/#add-your-additional-configurations","title":"Add your additional configurations","text":""},{"location":"operators/cluster-configurator/#for-example-add-htpasswd-authentication","title":"For example, add htpasswd authentication","text":""},{"location":"operators/cluster-configurator/#create-htpasswd-secret","title":"Create htpasswd secret","text":"<pre><code>cat &gt; conf/openshift/99_openshift-auth_htpasswd-secret.yaml &lt;&lt; EOF\napiVersion: v1\ndata:\n    htpasswd: dXNlcjE6JGFwcjEkejRVdE5ZczgkYjVmMzBIWkR1MHNaOTFCNzIuYXQ3LwoKYWRtaW46JGFwcjEkNkpQOS95eXUkTWZjSlRPU3hqMzRFWTNKYUo5Ui94MAoK\nkind: Secret\nmetadata:\n    name: htpass-secret\n    namespace: openshift-config\ntype: Opaque\nEOF\n</code></pre>"},{"location":"operators/cluster-configurator/#create-oauth-configuration","title":"Create oauth configuration","text":"<pre><code>cat &gt; conf/openshift/99_openshift-auth_htpasswd.yaml &lt;&lt; EOF\napiVersion: config.openshift.io/v1\nkind: OAuth\nmetadata:\n  name: cluster\nspec:\n  identityProviders:\n  - htpasswd:\n      fileData:\n        name: htpass-secret\n    mappingMethod: claim\n    name: Local\n    type: HTPasswd\n</code></pre>"},{"location":"operators/cluster-configurator/#install-your-cluster","title":"Install your cluster","text":"<pre><code>openshift-install create cluster --dir=conf\n</code></pre> <p>The caveat here is, you can not adjust objects there created by cluster version operator. Let's take a closer look, using the openshift ingress controller. The initial deploying of the openshift ingress (ex router) component is quite easy:</p> <p>The Cluster Version Operator (CVO) ensure the (1) openshift-ingress-operator is running and (2) create the default custom resource (CR) for the default router.</p> <p>Ingress part of the CVO is configurated via:</p> <pre><code>cat manifests/cluster-ingress-02-config.yml\napiVersion: config.openshift.io/v1\nkind: Ingress\nmetadata:\n  creationTimestamp: null\n  name: cluster\nspec:\n  domain: apps.ocp4.aws.bohne.io\nstatus: {}\n</code></pre> <p>The CR for the default router, created by the CVO:</p> <pre><code>$ oc get ingresscontrollers.operator.openshift.io/default -n openshift-ingress-operator -o yaml\napiVersion: operator.openshift.io/v1\nkind: IngressController\nmetadata:\n  creationTimestamp: \"2019-07-02T16:31:41Z\"\n  finalizers:\n  - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller\n  generation: 3\n  name: default\n  namespace: openshift-ingress-operator\n  resourceVersion: \"5297947\"\n  selfLink: /apis/operator.openshift.io/v1/namespaces/openshift-ingress-operator/ingresscontrollers/default\n  uid: df423307-9ce6-11e9-a438-525400116dcd\nspec:\nstatus:\n  availableReplicas: 2\n  conditions:\n  - lastTransitionTime: \"2019-07-02T16:32:13Z\"\n    status: \"True\"\n    type: Available\n  domain: apps.ocp4-upi.bohne.io\n  endpointPublishingStrategy:\n    type: HostNetwork\n  selector: ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default\n</code></pre> <p>The openshift-ingress-operator ensures that running router pods:</p> <pre><code>$ oc get pods -n openshift-ingress\nNAME                              READY   STATUS    RESTARTS   AGE\nrouter-default-5769db9885-9s2sl   1/1     Running   1          4d1h\nrouter-default-5769db9885-cvw52   1/1     Running   1          4d1h\n</code></pre> <p>But it is not supported to pass certificates from CVO to router pods via opemshift-ingress-operator. To solve the problem we need a kind of customer-cluster-operator to adjust the CR of the openshift-ingress-operator. How to write and deploy those operator check-out the next asset.</p>"},{"location":"operators/cluster-configurator/#2-create-your-own-cluster-configuration-operator-work-in-progress","title":"2) Create your own cluster configuration operator - WORK IN PROGRESS","text":""},{"location":"operators/cluster-configurator/#create-skeleton","title":"Create Skeleton","text":"<pre><code>operator-sdk new cluster-configurator  --type ansible \\\n  --kind=ClusterConfigurator --generate-playbook \\\n  --api-version clusterconfig.bohne.io/v1alpha1 \\\n</code></pre>"},{"location":"operators/cluster-configurator/#do-all-adjustments","title":"Do all adjustments","text":"<p>Checkout final version: https://github.com/openshift-examples/cluster-configurator</p>"},{"location":"operators/cluster-configurator/#build-push-container","title":"Build &amp; Push container","text":"<pre><code>operator-sdk build quay.io/rbo/cluster-configurator:latest\ndocker push quay.io/rbo/cluster-configurator:latest\n</code></pre>"},{"location":"operators/cluster-configurator/#build-yaml-to-deploy-via-manifests","title":"Build yaml to deploy via manifests","text":"<p>Pack all together with  build_installer_yaml.sh</p> <pre><code>./build_installer_yaml.sh &gt; 99_openshift-configurator.yaml\n</code></pre>"},{"location":"operators/cluster-configurator/#install-openshift-4-with-your-operator","title":"Install OpenShift 4 with your Operator","text":"<pre><code>mkdir conf\ncp install-config.yaml conf/\nopenshift-install create manifests --dir=conf\ncp -v 99_openshift-configurator.yaml conf/openshift/\nopenshift-install create cluster --dir=conf\n</code></pre>"},{"location":"operators/cluster-configurator/#resources","title":"Resources:","text":"<ul> <li><code>--cluster-scoped</code> is removed, https://github.com/operator-framework/operator-sdk/blob/master/doc/operator-scope.md</li> <li>https://github.com/openshift/cluster-ingress-operator</li> <li>https://github.com/operator-framework/operator-sdk/issues/1366</li> <li>https://github.com/operator-framework/operator-sdk/blob/master/doc/ansible/dev/developer_guide.md#extra-vars-sent-to-ansible</li> </ul>"},{"location":"operators/custom-ressource-definition/","title":"Custom Resource Definition (CRD)","text":""},{"location":"operators/custom-ressource-definition/#simple-example","title":"Simple example","text":"<pre><code>apiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: cars.openshift.pub\nspec:\n  group: openshift.pub\n  names:\n    kind: Car\n    listKind: CarList\n    plural: cars\n    singular: car\n  scope: Namespaced\n  subresources:\n    status: {}\n  version: v1\n</code></pre> <pre><code>apiVersion: openshift.pub/v1\nkind: Car\nmetadata:\n  name: bmw\nspec:\n  date_of_manufacturing: \"2014-07-01T00:00:00Z\"\n  engine: N57D30\n</code></pre>"},{"location":"operators/custom-ressource-definition/#advanced-example","title":"Advanced example","text":"<pre><code>apiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: cars.openshift.pub\nspec:\n  group: openshift.pub\n  names:\n    kind: Car\n    listKind: CarList\n    plural: cars\n    singular: car\n    shortNames:\n    - c\n  scope: Namespaced\n  subresources:\n    status: {}\n  version: v1\n  validation:\n  additionalPrinterColumns:\n  - JSONPath: .status.conditions[?(@.type==\"Succeeded\")].status\n    name: Succeeded\n    type: string\n  - JSONPath: .status.conditions[?(@.type==\"Succeeded\")].reason\n    name: Reason\n    type: string\n  - JSONPath: .spec.date_of_manufacturing\n    name: Produced\n    type: date\n  - JSONPath: .spec.engine\n    name: Engine\n    type: string\n    priority: 1\n</code></pre>"},{"location":"operators/custom-ressource-definition/#add-examples-andor-snippets-to-webconsole","title":"Add examples and/or snippets to webconsole","text":""},{"location":"operators/custom-ressource-definition/#snippet","title":"Snippet","text":"<pre><code>apiVersion: console.openshift.io/v1\nkind: ConsoleYAMLSample\nmetadata:\n  name: car-snippet-buick\nspec:\n  description: ...\n  snippet: true\n  targetResource:\n    apiVersion: openshift.pub/v1\n    kind: Car\n  title: Buick example\n  yaml: |\n    date_of_manufacturing: \"1955-01-01T00:00:00Z\"\n    engine: Nailhead\n</code></pre>"},{"location":"operators/custom-ressource-definition/#example","title":"Example","text":"<pre><code>apiVersion: console.openshift.io/v1\nkind: ConsoleYAMLSample\nmetadata:\n  creationTimestamp: '2020-02-20T09:18:12Z'\n  generation: 4\n  name: cars\n  resourceVersion: '2617237'\n  selfLink: /apis/console.openshift.io/v1/consoleyamlsamples/cars\n  uid: b1c75da9-e9d7-4b62-8324-7055b624c4de\nspec:\n  description: |\n    An example Car object\n\n    # asdf\n  targetResource:\n    apiVersion: openshift.pub/v1\n    kind: Car\n  title: New car example\n  yaml: |\n    apiVersion: openshift.pub/v1\n    kind: Car\n    metadata:\n      name: bmw\n    spec:\n      date_of_manufacturing: \"2014-07-01T00:00:00Z\"\n      engine: N57D30\n</code></pre>"},{"location":"operators/install-operator-as-a-user/","title":"WIP: Install Operator as a User","text":"<p>Official documentation: https://docs.openshift.com/container-platform/4.3/operators/olm-creating-policy.html</p>"},{"location":"operators/install-operator-as-a-user/#setup-htpasswd-auth-optional","title":"Setup htpasswd auth (Optional)","text":""},{"location":"operators/install-operator-as-a-user/#create-htpasswd-secret","title":"Create htpasswd secret","text":"<pre><code>oc create -f - &lt;&lt;EOF\napiVersion: v1\ndata:\n  htpasswd: \"$( (htpasswd -nb admin 'r3dh4t1!'; htpasswd -nb user 'r3dh4t1!') | base64 -b 0 )\"\nkind: Secret\nmetadata:\n  name: htpasswd\n  namespace: openshift-config\ntype: Opaque\nEOF\n</code></pre>"},{"location":"operators/install-operator-as-a-user/#change-oauth-config","title":"Change OAuth config","text":"<p>{% hint style=\"warning\" %} It overwrites the OAuth config</p> <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: config.openshift.io/v1\nkind: OAuth\nmetadata:\n  name: cluster\nspec:\n  identityProviders:\n    - htpasswd:\n        fileData:\n          name: htpasswd\n      mappingMethod: claim\n      name: Local\n      type: HTPasswd\nEOF\n</code></pre>"},{"location":"operators/install-operator-as-a-user/#add-your-private-marketplace","title":"Add your private marketplace","text":"<p>{% embed url=\"https://github.com/rbo/openshift-examples/tree/master/operator/simple-application-operator\" %}</p>"},{"location":"operators/install-operator-as-a-user/#add-application-operator-to-your-namespace","title":"Add application operator to your namespace","text":"<p>Create OLM Service Account</p> <pre><code>oc create -f - &lt;&lt;EOF\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: olm\n  namespace: simple-application-operator\nEOF\n</code></pre> <p>Setup roles and role bindings for OLM Service Account</p> <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: olm\n  namespace: simple-application-operator\nrules:\n- apiGroups: [\"*\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: olm-bindings\n  namespace: simple-application-operator\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: scoped\nsubjects:\n- kind: ServiceAccount\n  name: olm\n  namespace: simple-application-operator\nEOF\n</code></pre> <p>Create OperatorGroup as clusteradmin</p> <pre><code>oc create --as=system:admin -f - &lt;&lt;EOF\napiVersion: operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: simple-application-operator\n  namespace: simple-application-operator\nspec:\n  serviceAccountName: olm\n  targetNamespaces:\n  - simple-application-operator\nEOF\n</code></pre> <p>Install the operator</p> <pre><code># Get all need informations\n$ oc get catalogsources.operators.coreos.com\nNAME                  DISPLAY               TYPE   PUBLISHER      AGE\napplication-catalog   Application catalog   grpc   Robert Bohne   85m\n\n$ oc get svc\nNAME                  TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)     AGE\napplication-catalog   ClusterIP   172.30.96.52   &lt;none&gt;        50051/TCP   87m\n\n$ oc port-forward service/application-catalog 50051 50051\n\n# https://github.com/operator-framework/operator-registry#using-the-catalog-locally\n$ grpcurl -plaintext  localhost:50051 api.Registry/ListPackages\n{\n  \"name\": \"simple-application-operator\"\n}\n\n$ grpcurl -plaintext -d '{\"name\":\"simple-application-operator\"}' localhost:50051 api.Registry/GetPackage\n{\n  \"name\": \"simple-application-operator\",\n  \"channels\": [\n    {\n      \"name\": \"beta\",\n      \"csvName\": \"simple-application-operator.v0.1.0\"\n    },\n    {\n      \"name\": \"stable\",\n      \"csvName\": \"simple-application-operator.v0.1.0\"\n    }\n  ],\n  \"defaultChannelName\": \"stable\"\n}\n</code></pre> <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: simple-application-operator\n  namespace: simple-application-operator\nspec:\n  channel: stable\n  name: simple-application-operator\n  source: application-catalog\n  sourceNamespace: simple-application-operator\nEOF\n</code></pre>"},{"location":"operators/install-operator-as-a-user/#todo","title":"ToDo","text":"<ul> <li> Create doc bug: <code>The Subscription \"etcd\" is invalid: spec.sourceNamespace: Required value</code></li> </ul>"},{"location":"operators/operatorhub/","title":"Own Operator in OperatorHub","text":""},{"location":"operators/operatorhub/#build-helm-char","title":"Build helm char","text":"<pre><code>$ helm package helm-charts/chaos-professor/\nSuccessfully packaged chart and saved it to: /Users/rbohne/q/openshift-examples/chaos-professor-operator/chaos-professor-0.1.2.tgz\n</code></pre> <p>opm index add opm index add --from-index=quay.io/openshift-examples/chaos-professor-operator-index:latest   --build-tool docker   --bundles quay.io/openshift-examples/chaos-professor-operator-bundle:v0.1.2   --tag  quay.io/openshift-examples/chaos-professor-operator-index:latest</p>"},{"location":"operators/simple-application-operator/","title":"Draft/WIP Simple example ansible operator","text":"<pre><code>operator-sdk new simple-application-operator \\\n  --api-version=simple.application.openshift.pub/v1  \\\n  --kind=SimpleApp \\\n  --type=ansible\n\noperator-sdk build quay.io/rbo/demo-http-operator:latest\ndocker push quay.io/rbo/demo-http-operator:latest\n\nsed -i \"\" 's|{{ REPLACE_IMAGE }}|quay.io/rbo/demo-http-operator:latest|g' deploy/operator.yaml\nsed -i \"\" 's|{{ pull_policy\\|default('\\''Always'\\'') }}|Always|g' deploy/operator.yaml\n\noc new-project simple-application-operator\n# Setup Service Account\noc create -f deploy/service_account.yaml\n# Setup RBAC\noc create -f deploy/role.yaml\noc create -f deploy/role_binding.yaml\n# Setup the CRD\noc create -f deploy/crds/simple.application.openshift.pub_simpleapps_crd.yaml\n# Deploy the app-operator\noc create -f deploy/operator.yaml\n\n# Create an AppService CR\n# The default controller will watch for AppService objects and create a pod for each CR\noc create -f deploy/crds/simple.application.openshift.pub_v1_simpleapp_cr.yaml\n</code></pre>"},{"location":"operators/simple-application-operator/#redeploy","title":"Redeploy","text":"<pre><code>operator-sdk build quay.io/rbo/demo-http-operator:latest\ndocker push quay.io/rbo/demo-http-operator:latest\noc delete pods -l name=simple-application-operator\n</code></pre>"},{"location":"operators/simple-application-operator/#build-csv","title":"Build csv","text":"<p>Version operator-sdk version: \"v0.15.2\",</p> <pre><code>operator-sdk generate csv \\\n  --csv-version 0.1.0 \\\n  --operator-name simple-application-operator \\\n  --verbose\n</code></pre> <p>???</p> <pre><code>./operator-sdk-v0.12.0-x86_64-apple-darwin olm-catalog gen-csv --csv-version 0.1.0\nINFO[0000] Generating CSV manifest version 0.1.0\nINFO[0000] Fill in the following required fields in file deploy/olm-catalog/simple-application-operator/0.1.0/simple-application-operator.v0.1.0.clusterserviceversion.yaml:\n  spec.keywords\n  spec.maintainers\n  spec.provider\nINFO[0000] Created deploy/olm-catalog/simple-application-operator/0.1.0/simple-application-operator.v0.1.0.clusterserviceversion.yaml\nINFO[0000] Created deploy/olm-catalog/simple-application-operator/simple-application-operator.package.yaml\n</code></pre>"},{"location":"operators/simple-application-operator/#prepare-bundle","title":"Prepare bundle","text":"<pre><code>rm -rf prep-bundle\nmkdir prep-bundle/\ncp -v  deploy/*yaml deploy/crds/*crd.yaml deploy/olm-catalog/*/*/*.clusterserviceversion.yaml prep-bundle/\n</code></pre>"},{"location":"operators/simple-application-operator/#build-bundle","title":"Build bundle","text":"<p>An Operator Bundle is built as a scratch (non-runnable) container image that contains operator manifests and specific metadata in designated directories inside the image. Source</p>"},{"location":"operators/simple-application-operator/#options-1-use-operator-sdk-v0152-bundle-create","title":"Options 1) use operator sdk (v0.15.2) bundle create","text":"<pre><code>operator-sdk bundle create \\\n    --package simple-application-operator \\\n    --channels stable,beta \\\n    --default-channel stable \\\n    --directory deploy/olm-catalog/simple-application-operator/ \\\n    quay.io/rbo/demo-http-bundle:v0.1.0\n\ndocker push quay.io/rbo/demo-http-bundle:v0.1.0\n</code></pre>"},{"location":"operators/simple-application-operator/#option-2-use-opm-version-v159","title":"Option 2) use opm version v1.5.9","text":"<p>Documention</p> <pre><code>./opm alpha bundle build \\\n    --directory prep-bundle/ \\\n    --tag quay.io/rbo/demo-http-bundle:v0.1.0 \\\n    --package simple-application-operator \\\n    --channels stable,beta \\\n    --default stable\ndocker push quay.io/rbo/demo-http-bundle:v0.1.0\n</code></pre>"},{"location":"operators/simple-application-operator/#opm-put-manifest-into-bundle","title":"OPM - put manifest into bundle","text":"<pre><code>./opm index add --container-tool docker --bundles quay.io/rbo/demo-http-bundle:v0.1.0 --tag  quay.io/rbo/demo-http-catalog-index:v0.0.1\nINFO[0000] building the index                            bundles=\"[quay.io/rbo/demo-http-operator:v0.1.0]\"\nINFO[0000] running docker pull                           img=\"quay.io/rbo/demo-http-operator:v0.1.0\"\nINFO[0002] running docker save                           img=\"quay.io/rbo/demo-http-operator:v0.1.0\"\nINFO[0002] loading Bundle quay.io/rbo/demo-http-operator:v0.1.0  img=\"quay.io/rbo/demo-http-operator:v0.1.0\"\nINFO[0002] found annotations file searching for csv      dir=bundle_tmp261748450 file=bundle_tmp261748450/metadata load=annotations\nFATA[0002] permissive mode disabled                      bundles=\"[quay.io/rbo/demo-http-operator:v0.1.0]\" error=\"error loading bundle from image: no csv found in bundle\"\n</code></pre> <p>https://github.com/operator-framework/operator-registry/releases/download/v1.5.9/darwin-amd64-opm</p> <p>**FAIL WITH:  **</p> <pre><code>FROM quay.io/operator-framework/upstream-registry-builder AS builder\n....\nError: error building at STEP \"COPY --from=builder /build/bin/opm /opm\": no files found matching \"/var/lib/containers/storage/overlay/f33d0f91af5f71963c55a31f7b941fb4da13585df10bcdd7b49b182cbfd50ba9/merged/build/bin/opm\": no such file or directory\n. exit status 125\n</code></pre> <p>Work-a-round</p> <pre><code>./opm index add --container-tool docker --binary-image quay.io/operator-framework/upstream-registry-builder --bundles quay.io/rbo/demo-http-bundle:v0.1.0 --tag  quay.io/rbo/demo-http-catalog-index:v0.0.1 --generate\n# Change index.Dockerfile\n# - COPY --from=builder /build/bin/opm /opm\n# + COPY --from=builder /build/bin/linux-amd64-opm /opm\n\ndocker build -t quay.io/rbo/demo-http-catalog-index:v0.0.1 -f index.Dockerfile .\ndocker push quay.io/rbo/demo-http-catalog-index:v0.0.1\n</code></pre>"},{"location":"operators/simple-application-operator/#test-catalog-index","title":"Test catalog index","text":"<pre><code>docker run -p 50051:50051 -ti quay.io/rbo/demo-http-catalog-index:v0.0.1\n\ngrpcurl -plaintext  localhost:50051 api.Registry/ListPackages\ngrpcurl -plaintext -d '{\"name\":\"simple-application-operator\"}' localhost:50051 api.Registry/GetPackage\n{\n  \"name\": \"simple-application-operator\",\n  \"channels\": [\n    {\n      \"name\": \"beta\",\n      \"csvName\": \"simple-application-operator.v0.1.0\"\n    },\n    {\n      \"name\": \"stable\",\n      \"csvName\": \"simple-application-operator.v0.1.0\"\n    }\n  ],\n  \"defaultChannelName\": \"stable\"\n}\n</code></pre>"},{"location":"operators/simple-application-operator/#add-to-openshift","title":"Add to OpenShift","text":""},{"location":"operators/simple-application-operator/#notes-resources","title":"Notes &amp; Resources","text":"<ul> <li>https://github.com/operator-framework/operator-sdk/blob/master/doc/ansible/dev/finalizers.md</li> <li>https://operator-framework.github.io/olm-book/</li> </ul>"},{"location":"operators/air-gapped/","title":"Air-gapped/Disconnected","text":"<ul> <li>Support Sparse Manifests https://issues.redhat.com/browse/OCPSTRAT-1808</li> </ul>","tags":["OLM","air-gapped","v4.17","v4.18","v4.19","operators"]},{"location":"operators/air-gapped/#imagesetconfiguration-examples-oc-mirror-v2","title":"ImageSetConfiguration examples (oc mirror v2)","text":"<p>Syncronize only the latest version of the Operator!!</p> <pre><code>  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.19\n    packages:\n      - name: kubevirt-hyperconverged\n        channels:\n        - name: stable\n</code></pre> <p>Recommended add a minVersion is sync from minVersion all newer versions!</p> <pre><code>  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.19\n    packages:\n      - name: kubevirt-hyperconverged\n        channels:\n        - name: stable\n          minVersion: v4.17.4\n</code></pre> <p>More usefull examples: https://github.com/openshift/oc-mirror/tree/main/docs/examples</p>","tags":["OLM","air-gapped","v4.17","v4.18","v4.19","operators"]},{"location":"operators/air-gapped/#operator-upgrade-stuck","title":"Operator upgrade stuck","text":"","tags":["OLM","air-gapped","v4.17","v4.18","v4.19","operators"]},{"location":"operators/air-gapped/#check-the-available-versions-in-your-index","title":"Check the available versions in your index","text":"<p>For example 4.17.4 is installed, there a missing versions to get to 4.17.11. Because 4.17.11 just replace 4.17.7.</p> <pre><code># oc mirror list operators --catalog mirror-registry.disco.coe.muc.redhat.com:5000/disco/redhat/redhat-operator-index:v4.17 --package kubevirt-hyperconverged --channel stable                            [192/192]\nW0711 07:07:20.497749  129134 mirror.go:86]\n\n\u26a0\ufe0f  oc-mirror v1 is deprecated (starting in 4.18 release) and will be removed in a future release - please migrate to oc-mirror --v2\nVERSIONS\n4.17.11\n</code></pre> <pre><code># oc mirror list operators --catalog mirror-registry.disco.coe.muc.redhat.com:5000/disco/redhat/redhat-operator-index:v4.17 --package kubevirt-hyperconverged --channel stable\nW0711 07:52:14.847377  129286 mirror.go:86]\n\n\u26a0\ufe0f  oc-mirror v1 is deprecated (starting in 4.18 release) and will be removed in a future release - please migrate to oc-mirror --v2\nVERSIONS\n4.17.11\n4.17.4\n4.17.5\n4.17.7\n</code></pre> <pre><code>podman run -ti --rm --entrypoint cat mirror-registry.disco.coe.muc.redhat.com:5000/disco/redhat/redhat-operator-index:v4.17   /configs/kubevirt-hyperconverged/catalog.json | jq -r 'select(.schema==\"olm.channel\")' | jq\n{\n  \"schema\": \"olm.channel\",\n  \"name\": \"stable\",\n  \"package\": \"kubevirt-hyperconverged\",\n  \"entries\": [\n    {\n      \"name\": \"kubevirt-hyperconverged-operator.v4.17.4\",\n      \"replaces\": \"kubevirt-hyperconverged-operator.v4.17.3\",\n      \"skipRange\": \"&gt;=4.16.6 &lt;4.17.0\"\n    },\n    {\n      \"name\": \"kubevirt-hyperconverged-operator.v4.17.5\",\n      \"replaces\": \"kubevirt-hyperconverged-operator.v4.17.4\",\n      \"skipRange\": \"&gt;=4.16.6 &lt;4.17.0\"\n    },\n    {\n      \"name\": \"kubevirt-hyperconverged-operator.v4.17.7\",\n      \"replaces\": \"kubevirt-hyperconverged-operator.v4.17.5\",\n      \"skipRange\": \"&gt;=4.16.6 &lt;4.17.0\"\n    },\n    {\n      \"name\": \"kubevirt-hyperconverged-operator.v4.17.11\",\n      \"replaces\": \"kubevirt-hyperconverged-operator.v4.17.7\",\n      \"skipRange\": \"&gt;=4.16.7 &lt;4.17.0\"\n    }\n  ]\n}\n</code></pre> <pre><code>grpcurl -plaintext  localhost:50051 api.Registry/ListBundles | jq '{\"packageName\",\"replaces\",\"version\",\"skipRange\",\"csvName\",\"channel\"} | select(.packageName==\"kubevirt-hyperconverged\") '\n{\n  \"packageName\": \"kubevirt-hyperconverged\",\n  \"replaces\": \"kubevirt-hyperconverged-operator.v4.17.7\",\n  \"version\": \"4.17.11\",\n  \"skipRange\": \"&gt;=4.16.7 &lt;4.17.0\",\n  \"csvName\": \"kubevirt-hyperconverged-operator.v4.17.11\",\n  \"channel\": null\n}\n{\n  \"packageName\": \"kubevirt-hyperconverged\",\n  \"replaces\": \"kubevirt-hyperconverged-operator.v4.17.3\",\n  \"version\": \"4.17.4\",\n  \"skipRange\": \"&gt;=4.16.6 &lt;4.17.0\",\n  \"csvName\": \"kubevirt-hyperconverged-operator.v4.17.4\",\n  \"channel\": null\n}\n{\n  \"packageName\": \"kubevirt-hyperconverged\",\n  \"replaces\": \"kubevirt-hyperconverged-operator.v4.17.4\",\n  \"version\": \"4.17.5\",\n  \"skipRange\": \"&gt;=4.16.6 &lt;4.17.0\",\n  \"csvName\": \"kubevirt-hyperconverged-operator.v4.17.5\",\n  \"channel\": null\n}\n{\n  \"packageName\": \"kubevirt-hyperconverged\",\n  \"replaces\": \"kubevirt-hyperconverged-operator.v4.17.5\",\n  \"version\": \"4.17.7\",\n  \"skipRange\": \"&gt;=4.16.6 &lt;4.17.0\",\n  \"csvName\": \"kubevirt-hyperconverged-operator.v4.17.7\",\n  \"channel\": null\n}\n</code></pre>","tags":["OLM","air-gapped","v4.17","v4.18","v4.19","operators"]},{"location":"operators/air-gapped/#visulize-version-graph","title":"Visulize version graph","text":"<ul> <li>Download opm tool: https://github.com/operator-framework/operator-registry</li> </ul> <p>Minimum <code>~/.config/containers/policy.json</code></p> <pre><code>{\n    \"default\": [{\"type\": \"insecureAcceptAnything\"}]\n}\n</code></pre> <pre><code>cat &lt;&lt; EOF &gt; ./mermaid.json\n{ \"maxTextSize\": 300000 }\nEOF\n\nopm alpha render-graph -p kubevirt-hyperconverged mirror-registry.disco.coe.muc.redhat.com:5000/disco/redhat/redhat-operator-index:v4.17 | podman run --rm -i -v \"$PWD\":/data ghcr.io/mermaid-js/mermaid-cli/mermaid-cli -c /data/mermaid.json -o /data/operatorhubio-catalog.svg\n</code></pre> <p></p>","tags":["OLM","air-gapped","v4.17","v4.18","v4.19","operators"]},{"location":"operators/air-gapped/#constraints-not-satisfiable-subscription","title":"<code>constraints not satisfiable: subscription....</code>","text":"<p>Try:</p> <pre><code>oc delete pods -n openshift-operator-lifecycle-manager -l app=catalog-operator\noc delete pods -n openshift-operator-lifecycle-manager -l app=olm-operator\n</code></pre>","tags":["OLM","air-gapped","v4.17","v4.18","v4.19","operators"]},{"location":"operators/air-gapped/#discover-the-operator-index","title":"Discover the operator index","text":"","tags":["OLM","air-gapped","v4.17","v4.18","v4.19","operators"]},{"location":"operators/air-gapped/#via-oc-mirror","title":"via oc mirror","text":"<pre><code># oc mirror list operators --catalog registry.redhat.io/redhat/redhat-operator-index:v4.18\nNAME                                            DISPLAY NAME  DEFAULT CHANNEL\n3scale-operator                                               threescale-2.15\nadvanced-cluster-management                                   release-2.13\namq-broker-rhel8                                              7.12.x\namq-broker-rhel9                                              7.13.x\namq-online                                                    stable\namq-streams                                                   stable\namq-streams-console                                           alpha\namq7-interconnect-operator                                    1.10.x\nansible-automation-platform-operator                          stable-2.5\n...\n\n# oc mirror list operators --catalog registry.redhat.io/redhat/redhat-operator-index:v4.18 --package kubevirt-hyperconverged\nW0711 09:40:18.654683  130182 mirror.go:86]\n\n   oc-mirror v1 is deprecated (starting in 4.18 release) and will be removed in a future release - please migrate to oc-mirror --v2\n\nNAME                     DISPLAY NAME  DEFAULT CHANNEL\nkubevirt-hyperconverged                stable\n\nPACKAGE                  CHANNEL      HEAD\nkubevirt-hyperconverged  candidate    kubevirt-hyperconverged-operator.v4.18.9\nkubevirt-hyperconverged  dev-preview  kubevirt-hyperconverged-operator.v4.99.0-0.1723448771\nkubevirt-hyperconverged  stable       kubevirt-hyperconverged-operator.v4.18.8\n[coe@bastion mirror]$ oc mirror list operators --catalog registry.redhat.io/redhat/redhat-operator-index:v4.18 --package kubevirt-hyperconverged --channel stable\nW0711 09:44:44.362761  130240 mirror.go:86]\n\n\u26a0\ufe0f  oc-mirror v1 is deprecated (starting in 4.18 release) and will be removed in a future release - please migrate to oc-mirror --v2\n\nVERSIONS\n4.12.2\n4.13.2\n4.14.3\n4.15.2\n4.17.4\n4.18.2\n4.16.0\n4.18.8\n4.12.1\n4.14.1\n4.17.0\n4.14.0\n4.14.2\n4.15.0\n4.16.1\n4.13.1\n4.16.2\n4.17.2\n4.18.0\n4.12.0\n4.16.3\n4.13.3\n4.13.4\n4.15.1\n4.17.1\n4.18.1\n4.13.0\n4.17.3\n4.18.3\n</code></pre>","tags":["OLM","air-gapped","v4.17","v4.18","v4.19","operators"]},{"location":"operators/air-gapped/#via-grpcurl","title":"via grpcurl","text":"<ul> <li>grpcurl is available here https://github.com/fullstorydev/grpcurl</li> <li>https://github.com/operator-framework/operator-registry#using-the-catalog-locally</li> </ul> <p>Start index images:</p> <pre><code>podman run -p 50051:50051 -ti --rm   registry.redhat.io/redhat/redhat-operator-index:v4.18\n</code></pre> <p>And dicover</p> <pre><code># grpcurl -plaintext localhost:50051 api.Registry/ListPackages | jq -r '.name'\nkubevirt-hyperconverged\n...\n\n# grpcurl -plaintext -d '{\"name\":\"rhods-operator\"}' localhost:50051 api.Registry/GetPackage\n{\n  \"name\": \"kubevirt-hyperconverged\",\n  \"channels\": [\n    {\n      \"name\": \"stable\",\n      \"csvName\": \"kubevirt-hyperconverged-operator.v4.17.11\"\n    }\n  ],\n  \"defaultChannelName\": \"stable\"\n}\n\n# grpcurl -plaintext  localhost:50051 api.Registry/ListBundles | jq '{\"packageName\",\"replaces\",\"version\",\"skipRange\",\"csvName\",\"channel\"} | select(.packageName==\"kubevirt-hyperconverged\")'\ngrpcurl -plaintext  localhost:50051 api.Registry/ListBundles | jq '{\"packageName\",\"replaces\",\"version\",\"skipRange\",\"csvName\",\"channel\"} | select(.packageName==\"kubevirt-hyperconverged\")'\n{\n  \"packageName\": \"kubevirt-hyperconverged\",\n  \"replaces\": \"kubevirt-hyperconverged-operator.v4.18.2\",\n  \"version\": \"4.18.3\",\n  \"skipRange\": null,\n  \"csvName\": \"kubevirt-hyperconverged-operator.v4.18.3\",\n  \"channel\": null\n}\n{\n  \"packageName\": \"kubevirt-hyperconverged\",\n  \"replaces\": \"kubevirt-hyperconverged-operator.v4.18.3\",\n  \"version\": \"4.18.4\",\n  \"skipRange\": null,\n  \"csvName\": \"kubevirt-hyperconverged-operator.v4.18.4\",\n  \"channel\": null\n}\n...\n</code></pre>","tags":["OLM","air-gapped","v4.17","v4.18","v4.19","operators"]},{"location":"operators/air-gapped/#discover-grpcurl-api","title":"Discover grpcurl api","text":"<pre><code># grpcurl -plaintext localhost:50051 list api.Registry\napi.Registry.GetBundle\napi.Registry.GetBundleForChannel\napi.Registry.GetBundleThatReplaces\napi.Registry.GetChannelEntriesThatProvide\napi.Registry.GetChannelEntriesThatReplace\napi.Registry.GetDefaultBundleThatProvides\napi.Registry.GetLatestChannelEntriesThatProvide\napi.Registry.GetPackage\napi.Registry.ListBundles\napi.Registry.ListPackages\n\n# grpcurl -plaintext localhost:50051 describe api.Registry.GetBundle\napi.Registry.GetBundle is a method:\nrpc GetBundle ( .api.GetBundleRequest ) returns ( .api.Bundle );\n\n# grpcurl -plaintext localhost:50051 describe .api.GetBundleRequest\n\napi.GetBundleRequest is a message:\nmessage GetBundleRequest {\n  string pkgName = 1;\n  string channelName = 2;\n  string csvName = 3;\n}\n\n# grpcurl -plaintext -d '{\"csvName\": \"update-service-operator.v5.0.3\",\"pkgName\": \"cincinnati-operator\",\"channelName\": \"v1\"}' localhost:50051 api.Registry/GetBundle | jq '{\"packageName\",\"replaces\",\"version\",\"skipRange\",\"csvName\",\"channel\"}'\n{\n  \"packageName\": \"cincinnati-operator\",\n  \"replaces\": null,\n  \"version\": \"5.0.3\",\n  \"skipRange\": null,\n  \"csvName\": \"update-service-operator.v5.0.3\",\n  \"channel\": null\n}\n</code></pre>","tags":["OLM","air-gapped","v4.17","v4.18","v4.19","operators"]},{"location":"storage/","title":"Storage","text":"","tags":["storage"]},{"location":"storage/#content","title":"Content","text":"<ul> <li>IBM Fussion Access SAN</li> </ul>","tags":["storage"]},{"location":"storage/ibm-fusion-access-san/","title":"IBM Fusion Access SAN","text":"<p>Official documentation:</p> <ul> <li>https://www.ibm.com/docs/en/fusion-software/2.12.0?topic=san-deploying-fusion-access</li> </ul> <p>Tested with:</p> Component Version OpenShift v4.20.4 OpenShift Virt v4.20.x IBM Fussion Access for SAN 2.12.0","tags":["storage","v4.20"]},{"location":"storage/ibm-fusion-access-san/#prerequisites","title":"Prerequisites","text":"","tags":["storage","v4.20"]},{"location":"storage/ibm-fusion-access-san/#create-an-ibmid","title":"Create an IBMid","text":"<p>https://www.ibm.com/account/reg/us-en/signup?formid=urx-19776</p>","tags":["storage","v4.20"]},{"location":"storage/ibm-fusion-access-san/#request-a-60day-ibm-storage-fusion-trail-version","title":"Request a 60day IBM Storage Fusion trail version","text":"<p>https://www.ibm.com/docs/en/storage-fusion/storage/2.6.0?topic=overview-storage-fusion-trial-version</p>","tags":["storage","v4.20"]},{"location":"storage/ibm-fusion-access-san/#create-an-entitlement-key","title":"Create an Entitlement key","text":"<p>https://myibm.ibm.com/products-services/containerlibrary</p>","tags":["storage","v4.20"]},{"location":"storage/ibm-fusion-access-san/#create-an-openshift-cluster","title":"Create an OpenShift cluster","text":"<ul> <li>With at least 3 worker nodes each with ~32 GB memory</li> <li>All nodes need a shared disk. Via iSCSI, FC and KVM</li> <li>OpenShift internal registry is running with storage</li> </ul> <p>Warning</p> <p>OpenShift internal registry is mandatory because IBM Fusion Access for SAN builds via Kernel Module Management a container image with the kernel module.</p> <p>It means you need highly available storage for the internal registry to start highly available storage.</p>","tags":["storage","v4.20"]},{"location":"storage/ibm-fusion-access-san/#at-this-point-there-is-a-demo-video-available","title":"At this point, there is a demo video available","text":"<p>https://www.youtube.com/watch?v=ayXgD4e61K4</p>","tags":["storage","v4.20"]},{"location":"storage/ibm-fusion-access-san/#install-operator","title":"Install Operator","text":"<p>https://www.ibm.com/docs/en/fusion-software/2.12.0?topic=san-installing-fusion-access-operator</p>","tags":["storage","v4.20"]},{"location":"storage/ibm-fusion-access-san/#create-a-kubernetes-pull-secret","title":"Create a Kubernetes pull secret","text":"<p>with the entitlement key from above:</p> <pre><code>oc create secret -n ibm-fusion-access generic fusion-pullsecret \\\n--from-literal=ibm-entitlement-key=&lt;ibm-entitlement-key&gt;\n</code></pre> <p>oc create secret -n ibm-fusion-access generic fusion-pullsecret \\ --from-literal=ibm-entitlement-key=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJJQk0gTWFya2V0cGxhY2UiLCJpYXQiOjE3NjcwODkwMTQsImp0aSI6Ijg5MmYwY2UxYzQxYjRjY2I4MTJmZjFkNzU5NmE5MTY3In0._mY9MXvSNVbdxjYAcyPQ8Dc_H68ppXET7_aK2ULxd-I</p>","tags":["storage","v4.20"]},{"location":"storage/ibm-fusion-access-san/#creating-the-fusionaccess-cr","title":"Creating the FusionAccess CR","text":"<p>https://www.ibm.com/docs/en/fusion-software/2.12.0?topic=san-creating-fusionaccess-cr</p>","tags":["storage","v4.20"]},{"location":"storage/ibm-fusion-access-san/#creating-a-storage-cluster","title":"Creating a storage cluster","text":"<p>https://www.ibm.com/docs/en/fusion-software/2.12.0?topic=san-creating-storage-cluster</p> <p>After creating the storage cluster, it's building the kernel module container image. Check builds in ibm-fusion-access project.</p> <p>Check the pods i folowing projects</p> <ul> <li><code>ibm-fusion-access</code></li> <li><code>ibm-spectrum-scale</code></li> </ul> <p>more details around the deployment.</p>","tags":["storage","v4.20"]},{"location":"storage/ibm-fusion-access-san/#creating-a-filesystem","title":"Creating a filesystem","text":"<p>https://www.ibm.com/docs/en/fusion-software/2.12.0?topic=san-creating-filesystem</p>","tags":["storage","v4.20"]},{"location":"storage/ibm-fusion-access-san/#notes-for-various-lab-environments","title":"Notes for various lab environments","text":"<p>Add a shared disk to all worker nodes</p>","tags":["storage","v4.20"]},{"location":"storage/ibm-fusion-access-san/#plain-kvm-environment","title":"Plain KVM environment","text":"<p>I deployed via https://github.com/RedHat-EMEA-SSA-Team/hetzner-ocp4. Now let's add a shared lvm disk becaus all is running on one node.</p> <p>Warning</p> <p>In my lab environment, I had following issue during the load of gpfs related kernel modules:</p> <pre><code>I0105 18:25:24.083670 1 funcs_kmod.go:12] \"Starting worker\" logger=\"kmm-worker\" version=\"\" git commit=\"\"\nI0105 18:25:24.083696 1 funcs_kmod.go:24] \"Reading config\" logger=\"kmm-worker\" path=\"/etc/kmm-worker/config.yaml\"\nI0105 18:25:24.083968 1 worker.go:77] \"preparing firmware for loading\" logger=\"kmm-worker\" image directory=\"/tmp/opt/lxtrace\" host mount directory=\"/var/lib/firmware\"\nI0105 18:25:24.084219 1 modprobe.go:33] \"Running modprobe\" logger=\"kmm-worker\" command=\"/usr/sbin/modprobe -vd /tmp/opt mmfs26\"\nI0105 18:25:24.086346 1 cmdlog.go:70] \"modprobe: ERROR: could not insert 'mmfs26': Key was rejected by service\" logger=\"kmm-worker.modprobe.stderr\"\nI0105 18:25:24.086394 1 cmdlog.go:70] \"insmod /tmp/opt/lib/modules/5.14.0-570.72.1.el9_6.x86_64/tracedev.ko \" logger=\"kmm-worker.modprobe.stdout\"\nE0105 18:25:24.086679 1 cmdutils.go:11] \"Fatal error\" err=\"error while waiting on the command: exit status 1\" logger=\"kmm-worker\"\n</code></pre> <p>Tested with following CPUs <code>Intel(R) Xeon(R) W-2145 CPU @ 3.70GHz</code> or <code>AMD Ryzen 9 3900 12-Core Processor</code></p> <pre><code>lvcreate -L1T -n fusion vg0\n</code></pre> <pre><code>export CLUSTER_NAME=demo\nfor node in ${CLUSTER_NAME}-compute-0  ${CLUSTER_NAME}-compute-1  ${CLUSTER_NAME}-compute-2 ; do\n    virsh attach-disk $node /dev/mapper/vg0-fusion sdb --targetbus scsi --cache none --persistent --live --wwn 5000c500155a3456\ndone\n</code></pre>","tags":["storage","v4.20"]},{"location":"storage/ibm-fusion-access-san/#iscsi-rhcos","title":"iSCSI &amp; RHCOS","text":"<p>This is ugly as hel, but works for quick testing.</p> Apply iscsi helperiscsi-helper.yaml <pre><code>oc apply -f https://examples.openshift.pub/pr-133/storage/ibm-fusion-access-san/iscsi-helper.yaml\n</code></pre> <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: iscsi-helper\nspec: {}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: iscsi-helper\n  namespace: iscsi-helper\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: system:openshift:scc:privileged\n  namespace: iscsi-helper\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:openshift:scc:privileged\nsubjects:\n  - kind: ServiceAccount\n    name: iscsi-helper\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: iscsi-helper\n  namespace: iscsi-helper\nspec:\n  selector:\n    matchLabels:\n      name: iscsi-helper\n  template:\n    metadata:\n      annotations:\n        openshift.io/required-scc: \"privileged\"\n      labels:\n        name: iscsi-helper\n    spec:\n      nodeSelector:\n        node-role.kubernetes.io/worker: \"\"\n      serviceAccountName: iscsi-helper\n      tolerations:\n        - key: \"node-role.kubernetes.io/master\"\n          operator: \"Exists\"\n          effect: \"NoSchedule\"\n      containers:\n        - name: iscsi-helper\n          image: registry.access.redhat.com/ubi10-micro:10.1\n          securityContext:\n            privileged: true\n            capabilities:\n              add:\n                - SYS_CHROOT\n            runAsGroup: 0\n            runAsUser: 0\n          env:\n            - name: ISCSI_HELPER_TARGET\n              value: \"iqn.1992-08.com.netapp:sn.4f0586e4962411efb20d00a0987cd31a:vs.19\"\n          command:\n            - /usr/sbin/chroot\n            - /host\n            - sh\n            - -c\n            - |\n\n              iscsiadm --mode discovery --op update --type sendtargets --portal   10.32.97.20\n              for i in $(seq 20 23); do  iscsiadm -m node --targetname ${ISCSI_HELPER_TARGET} -p 10.32.97.${i} --login ; done\n\n              multipath -ll\n              sleep infinity\n          volumeMounts:\n            - name: host-root\n              mountPath: /host\n              readOnly: false\n      hostNetwork: true\n      hostPID: true\n      hostIPC: true\n      volumes:\n        - name: host-root\n          hostPath:\n            path: /\n            type: Directory\n</code></pre>","tags":["storage","v4.20"]},{"location":"troubleshooting/","title":"Troubleshooting","text":"","tags":["troubleshooting"]},{"location":"troubleshooting/#the-openshift-console-is-not-coming-up-1","title":"The openshift-console is not coming up #1","text":"<pre><code>E1011 08:07:56.183305       1 auth.go:231] error contacting auth provider (retrying in 10s): request to OAuth issuer endpoint https://oauth-openshift.apps.hal.openshift.airgapped/oauth/token failed: Head \"https://oauth-openshift.apps.hal.openshift.airgapped\": EOF\nE1011 08:08:06.187357       1 auth.go:231] error contacting auth provider (retrying in 10s): request to OAuth issuer endpoint https://oauth-openshift.apps.hal.openshift.airgapped/oauth/token failed: Head \"https://oauth-openshift.apps.hal.openshift.airgapped\": EOF\n</code></pre> <p>Issue: Router pods scheduled on master.</p> <p>Solution: Find and delete pods using the below command:</p> <pre><code>oc get pod -n openshift-ingress -o wide\noc delete pod router-default-65c56bb644-2ldfp router-default-65c56bb644-lqznd -n openshift-ingress\n</code></pre>","tags":["troubleshooting"]},{"location":"troubleshooting/#the-openshift-console-is-not-coming-up-2","title":"The openshift-console is not coming up #2","text":"<pre><code>oc describe pod &lt;podname&gt;\n\nFailedScheduling: 0/7 nodes are available: 3 nodes had taint, that the pod didn\u2019t tolerate (node-role.kubernetes.io/master), 4 insufficient cpu\n</code></pre> <p>Issue: Because of insufficient CPU the pod(s) cannot be scheduled.</p> <p>Solution: Allocate more physical or virtual CPU or use the Cluster Resource Override Operator to override the ratio between requests and limits set on containers/ pods:</p> <ol> <li>Install the Cluster Resource Override Operator</li> <li>Add the below custom resource definition</li> </ol> <pre><code>apiVersion: operator.autoscaling.openshift.io/v1\nkind: ClusterResourceOverride\nmetadata:\n  name: cluster\nspec:\n  podResourceOverride:\n    spec:\n       cpuRequestToLimitPercent: 25\n</code></pre> <p>If a container CPU limit has been specified or defaulted, this will override the CPU request to 25% percentage of the limit.</p> <ol> <li>Apply the following label to the Namespace object for each project (overrides can be enabled per-project):</li> </ol> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n....\n  labels:\n    clusterresourceoverrides.admission.autoscaling.openshift.io/enabled: \"true\"\n....\n</code></pre>","tags":["troubleshooting"]},{"location":"troubleshooting/#the-bootstrap-is-running-but-the-customer-cant-pull-from-the-mirror-registry","title":"The bootstrap is running but the customer can't pull from the mirror registry","text":"<pre><code>Error pulling candidate abc.def.ghi/company-openshift-docker/openshift-release-dev/ocp-release@sha256:97410a5db655a9d3017b735c2c0747c849d09ff551765e49d5272b80c024a844: initializing source docker://abc.def.ghi/company-openshift-docker/openshift-release-dev/ocp-release@sha256:97410a5db655a9d3017b735c2c0747c849d09ff551765e49d5272b80c024a844: pinging container registry abc.def.ghi: Get \"https://abc.def.ghi/v2/ &lt;https://abc.def.ghi/v2/&gt; \": x509: certificate signed by unknown authority\n\nError: initializing source docker://abc.def.ghi/company-openshift-docker/openshift-release-dev/ocp-release@sha256:97410a5db655a9d3017b735c2c0747c849d09ff551765e49d5272b80c024a844: pinging container registry abc.def.ghi: Get \"https://abc.def.ghi/v2/ &lt;https://abc.def.ghi/v2/&gt; \": x509: certificate signed by unknown authority\n</code></pre> <p>Issue: The mirror registry's certificate isn't trusted.</p> <p>Solution: Use curl and openssl to identify the correct certificate/chain of certificates needed to be able to securely connect to the mirror registry and add them to the additionalTrustBundle section inside the <code>install-config.yaml</code> file with the right intendation, for example.</p>","tags":["troubleshooting"]},{"location":"troubleshooting/#ignition-fails-connection-refused-errors-during-the-installation-process","title":"Ignition fails - connection refused errors during the installation process","text":"<pre><code>#Checking the bootstrap via journalctl shows below error:\nSep 13 11:58:08 v0004369.abc.def.ghi cluster-bootstrap[46455]: [#602]\nfailed to fetch discovery: Get \"https://localhost:6443/api?timeout=32s\":\ndial tcp [::1]:6443: connect: connection refused\nSep 13 11:58:08 v0004369.abc.def.ghi bootkube.sh[46444]: [#602] failed to\nfetch discovery: Get \"https://localhost:6443/api?timeout=32s\": dial tcp\n[::1]:6443: connect: connection refused\n</code></pre> <p>Problem determination: Check whether the Kubernetes API (<code>https://api.&lt;cluster-id&gt;.&lt;domain&gt;:port</code>) is accessible. This helps to verify that the DNS resolution on the bootstrap server is set up correctly. 6443 is the (API) port used by all nodes to communicate with the control plane (master nodes). For reference see Network connectivity requirements.</p> <pre><code>#$ curl -k -I -v https://api.&lt;cluster-id&gt;.&lt;domain&gt;:port\n\n#The result output hinted at a certificate issue\n[core@v0004369 ~]$ curl -k -I -v https://api.&lt;cluster-id&gt;.&lt;domain&gt;:port\n* Rebuilt URL to: https://api.&lt;cluster-id&gt;.&lt;domain&gt;:port/\n*  Trying x.x.x.x...\n* TCP_NODELAY set\n* Connected to api.&lt;cluster-id&gt;.&lt;domain&gt; (&lt;ip address&gt;) port 6443 (#0)\n* ALPN, offering h2\n* ALPN, offering http/1.1\n* successfully set certificate verify locations:\n*  CAfile: /etc/pki/tls/certs/ca-bundle.crt\n CApath: none\n* TLSv1.3 (OUT), TLS handshake, Client hello (1):\n* OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to\napi.&lt;cluster-id&gt;.&lt;domain&gt;:6443\ncurl: (35) OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to\napi.&lt;cluster-id&gt;.&lt;domain&gt;:6443\n</code></pre> <p>Run the below debug command from the OpenShift installation directory. This command can be used to follow the installation process.</p> <pre><code>$ openshift-install wait-for bootstrap-complete --log-level debug\n\n#The result output hinted at a certificate issue\n\n[openshift@v0004314 cluster]$ openshift-install wait-for bootstrap-complete\n--log-level debug\nDEBUG OpenShift Installer 4.11.1\nDEBUG Built from commit 1d2450c520b70765b53b71da5e8544657d50d6e2\nINFO Waiting up to 20m0s (until 3:28PM) for the Kubernetes API at\nhttps:api.&lt;cluster-id&gt;.&lt;domain&gt;:6443 ...\nDEBUG Still waiting for the Kubernetes API: Get \"https:api.&lt;cluster-id&gt;.&lt;domain&gt;:6443\": EOF\nDEBUG Still waiting for the Kubernetes API: Get \"https:api.&lt;cluster-id&gt;.&lt;domain&gt;:6443\": x509: certificate has\nexpired or is not yet valid: current time 2022-09-13T15:10:07+02:00 is\nafter 2022-09-10T08:45:15Z\nDEBUG Still waiting for the Kubernetes API: Get \"https://api.&lt;cluster-id&gt;.&lt;domain&gt;:6443\": x509: certificate has\nexpired or is not yet valid: current time 2022-09-13T15:10:38+02:00 is\nafter 2022-09-10T08:45:15Z\nDEBUG Still waiting for the Kubernetes API: Get \"https://api.&lt;cluster-id&gt;.&lt;domain&gt;:6443\": EOF\n</code></pre> <p>Issue: The Ignition config files that the openshift-install program generates contain certificates that expire after 24 hours. Expired certificates cause the installation to fail.</p> <p>Solution: Verify the validity of the certificate being presented by the bootstrap node.</p> <pre><code>openssl s_client -connect api-int.cluster.fqdn:22623 | openssl x509 -noout -text\n</code></pre> <p>Check that all certificates are valid, especially the certificates from which the ignition files are created. If the openshift-install create ignition-configs command needs to be re-run, then delete all files - including hidden files - except install_config.yaml and openshift-install. Otherwise, the date of the certificates could be pinned to the first run, i.e. the certificates have expired.</p> <p>Note: It is recommended that you use Ignition config files within 12 hours after they are generated because the 24-hour certificate rotates from 16 to 22 hours after the cluster is installed</p> <p>For reference, please see</p> <ul> <li>Creating the Kubernetes manifest and Ignition config files</li> <li>Masters and Workers Fail to Ignite Reporting Error 'x509: certificate has expired or not yet valid'</li> </ul>","tags":["troubleshooting"]},{"location":"troubleshooting/#ignition-fails-connection-error-no-such-host-during-the-installation-process","title":"Ignition fails - connection error \"no such host\" during the installation process","text":"<pre><code>#OCP installer cannot progress after bootstrap and master nodes are created in vSphere; bootstrap does not become reachable for the master nodes via API VIP\n\nDEBUG Built from commit 6db5fb9d56c9284124cf9147afd8f3e79345e907\nINFO Waiting up to 20m0s (until 8:33AM) for the Kubernetes API at https://api.ocpinstall.gym.lan:6443...\nDEBUG Still waiting for the Kubernetes API: Get \"https://api.ocpinstall.gym.lan:6443/version\": dial tcp: lookup api.ocpinstall.gym.lan on 192.168.127.1:53: no such host\nDEBUG Still waiting for the Kubernetes API: Get \"https://api.ocpinstall.gym.lan:6443/version\": dial tcp: lookup api.ocpinstall.gym.lan on 192.168.127.1:53: no such host\n</code></pre> <p>Problem determination: To break down the issue and determine the root cause, ssh into the bootstrap machine and check if the bootstrapping process is progressing. In particular, check for the following root causes:</p> <ul> <li>Firewall / Proxy settings: Make sure quay.io is reachable from the bootstrap machine and Redhat images can be pulled. In case of vSphere installation, make sure the bootstrap and master machines can reach vCenter API.</li> <li> <p>Bootstrapping progress:</p> </li> <li> <p>Check the <code>bootkube.service</code> log for abnormalities with</p> <pre><code>journalctl -b -f -u bootkube.service\n</code></pre> </li> <li> <p>Check podman container logs for abnormalities with</p> <pre><code>for pod in $(sudo podman ps -a -q); do sudo podman logs $pod; done\n</code></pre> </li> </ul> <p>Issue: The <code>httpProxy</code> and <code>httpsProxy</code> settings might be erroneous, causing bootstrap fail to authenticate at the proxy server and thus cannot reach the internet. Additionally, the firewall could be blocking bootstrap and master nodes from reaching the proxy server.</p> <p>Solution: Verify the correctness of the proxy settings in the install config yaml:</p> <pre><code>proxy:\n  httpProxy: &lt;http://user:pw@proxy:8080&gt;\n  httpsProxy: &lt;http://user:pw@proxy:8080&gt;\n  noProxy: &lt;api, ingress VIP, DHCPrange, intranet&gt;\n</code></pre> <p>Verify that the firewall is not blocking the communications between proxy server and machines.</p> <p>It is very recommendable to install OCP in a bastion host located inside the same network segment of the installed cluster. By doing this, network issues can be identified timely.</p>","tags":["troubleshooting"]},{"location":"troubleshooting/#after-bootstrapping-openshift-apiserver-and-ingress-keep-crashlooping-while-no-workers-can-be-provisioned","title":"After bootstrapping, <code>openshift-apiserver</code> and ingress keep crashlooping, while no workers can be provisioned","text":"<p>Problem determination: Determine whether the proxy and firewall settings are setup correctly for the master and worker hosts. The following criteria must be met:</p> <ul> <li>Master nodes can reach vCenter API to provision worker nodes;</li> <li>In the installation yaml, <code>machineNetwork</code> must correspond to the actual IPs assigned to the nodes, otherwise the proxy settings won't get propagated correctly to the nodes.</li> </ul> <p>Issue: There are two potential issues:</p> <ul> <li>Master node cannot reach vSphere API to provision worker nodes due to firewall blockage;</li> <li>Apiserver and ingress pods' health checks fail, because the <code>machineNetwork</code> does not contain the IPs of the machines. Thus the machines are not under <code>noProxy</code> and the health checks arrive at the proxy server.</li> </ul> <p>Solution: Fill out the <code>machineNetwork</code> correctly in the install config yaml. In case of DHCP, put the entire DHCP range into <code>machineNetwork</code> or under <code>noProxy</code> in order to be absolutely sure. Check out https://docs.openshift.com/container-platform/4.12/networking/enable-cluster-wide-proxy.html for more detailed instructions.</p>","tags":["troubleshooting"]},{"location":"troubleshooting/#troubleshooting-network-issues","title":"Troubleshooting network issues","text":"<pre><code>oc get nodes -o wide\noc get events -n openshift-sdn\noc get co\n</code></pre> <p>Move networking resources to the control plane on vsphere</p> <p>OCP 4 Node not ready after cluster upgrade or node restart</p>","tags":["troubleshooting"]},{"location":"troubleshooting/#worker-nodes-are-not-visible-when-running-oc-get-nodes","title":"Worker nodes are not visible when running oc get nodes","text":"<p><code>oc get nodes</code> only shows master nodes.</p> <p>Issue: The nodes' certificate requests haven't been approved.</p> <p>Solution: The new worker node(s) will still be missing or in pending state. Add them by signing the respective client and server CSR requests. Run <code>oc get csr</code> and then sign each request.</p> <pre><code>oc get csr\noc adm certificate approve &lt;csr_name&gt;\n</code></pre> <p>There will be multiple CSRs created per worker, so run the commands above multiple times until the workers show up as ready.</p> <p>Alternatively, to approve all pending CSRs, run the following command:</p> <pre><code>oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{\"\\n\"}}{{end}}{{end}}' | xargs --no-run-if-empty oc adm certificate approve\n</code></pre> <p>After all client and server CSRs have been approved, the machines should have the ready status. Verify this by running the following command:</p> <pre><code>oc get nodes\n</code></pre>","tags":["troubleshooting"]},{"location":"troubleshooting/#installation-using-ova-template-fails","title":"Installation using OVA template fails","text":"<p>Issue: The OVA image has been started prior to cloning.</p> <p>Solution: Create a new template for the OVA image and then clone the template as needed. Starting the OVA image prior to cloning will kick off the ignition process and, as a result, the ignition of the templates fails.</p>","tags":["troubleshooting"]},{"location":"troubleshooting/#troubleshooting-ingress-issues","title":"Troubleshooting ingress issues","text":"<p>To check the status of the ingress operator use</p> <pre><code>oc get co\noc get ingresscontroller/default -o yaml -n openshift-ingress-operator\n</code></pre> <p>Place a nodeSeclector of this deployment on a master node provided that master nodes are running and ready. To verify that masters are unschedulable ensure that the masterSchedulable field is set to false.</p> <pre><code>$ oc edit schedulers.config.openshift.io cluster\n\n#The result is something like\napiVersion: config.openshift.io/v1\nkind: Scheduler\nmetadata:\n  creationTimestamp: \"2022-09-11T07:02:04Z\"\n  generation: 1\n  name: cluster\n  resourceVersion: \"623\"\n  uid: 3b595176-4b45-4ac4-99d7-bd09adc2eb3a\nspec:\n  mastersSchedulable: false\n  policy:\n    name: \"\"\nstatus: {}\n</code></pre>","tags":["troubleshooting"]},{"location":"troubleshooting/#troubleshooting-node-startup-issues","title":"Troubleshooting node startup issues","text":"<p>To monitor machine-config-operator logs in case any node fails to start:</p> <pre><code>oc get pods -n openshift-machine-config-operator\noc logs -f -n openshift-machine-config-operator machine-config-daemon-&lt;XXXX&gt; -c machine-config-daemon\n</code></pre> <p>OpenShift Container Platform 4: How does Machine Config Pool work?</p>","tags":["troubleshooting"]},{"location":"troubleshooting/#troubleshooting-icsp-related-node-startup-issues","title":"Troubleshooting ICSP related node startup issues","text":"<p>To check the content of <code>/etc/containers/registries.conf</code> on each node use</p> <pre><code>$ oc debug node/&lt;worker or master node&gt;\n#chroot /host\n# less /etc/containers/registries.conf\n</code></pre> <p>If <code>/etc/containers/registries.conf</code> changes, do the nodes purge their internal cache? NO - If a new container is deployed and if the image requested is not on node the image will be pull from the \u201cmirror\u201d registry mentioned in /etc/containers/registries. This file is just for crio to download the image to the correct location.</p>","tags":["troubleshooting"]},{"location":"troubleshooting/#resizing-the-vm-disk","title":"Resizing the VM disk","text":"<p>https://unix.stackexchange.com/questions/678677/in-an-ubuntu-vm-in-vmware-i-increased-the-hard-disk-space-how-do-i-add-that-to</p>","tags":["troubleshooting"]},{"location":"troubleshooting/#how-to-deletedestroy-a-failed-installation","title":"How to delete/destroy a failed installation","text":"<pre><code>./openshift-install destroy cluster --dir &lt;installation_directory&gt; --log-level info\n</code></pre> <p>Reference</p>","tags":["troubleshooting"]},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#agent-based","title":"Agent-based","text":"<ul> <li>Agent-based non-integrated</li> </ul>"},{"location":"tags/#egress","title":"Egress","text":"<ul> <li>Egress IP</li> </ul>"},{"location":"tags/#gpu","title":"GPU","text":"<ul> <li>Cluster entitlement</li> <li>GPU nodes</li> <li>GPU debugging</li> <li>GPU on AWS</li> <li>GPU on-prem</li> </ul>"},{"location":"tags/#github","title":"GitHub","text":"<ul> <li>GitHub receiver</li> </ul>"},{"location":"tags/#hostedcontrolplane","title":"HostedControlPlane","text":"<ul> <li>Hosted Control Plane</li> <li>Hosted Control Plane - KubeVirt Networking</li> </ul>"},{"location":"tags/#ipi","title":"IPI","text":"<ul> <li>vSphere IPI &amp; Proxy</li> </ul>"},{"location":"tags/#mco","title":"MCO","text":"<ul> <li>MachineConfig</li> </ul>"},{"location":"tags/#mtc","title":"MTC","text":"<ul> <li>Storage Migration (Container)</li> </ul>"},{"location":"tags/#machineconfig","title":"MachineConfig","text":"<ul> <li>MachineConfig</li> </ul>"},{"location":"tags/#networkpolicy","title":"NetworkPolicy","text":"<ul> <li>Network Policy with OVNKubernetes</li> <li>Network Policy with OpenShiftSDN</li> </ul>"},{"location":"tags/#olm","title":"OLM","text":"<ul> <li>Air-gapped/Disconnected</li> </ul>"},{"location":"tags/#ota","title":"OTA","text":"<ul> <li>Over-the-Air Upgrades</li> </ul>"},{"location":"tags/#ovnkubernetes","title":"OVNKubernetes","text":"<ul> <li>Network Policy with OVNKubernetes</li> </ul>"},{"location":"tags/#openshiftsdn","title":"OpenShiftSDN","text":"<ul> <li>Network Policy with OpenShiftSDN</li> </ul>"},{"location":"tags/#openshifttv","title":"OpenShiftTV","text":"<ul> <li>OpenShift.tv - vSphere IPI in a disconnected environment</li> </ul>"},{"location":"tags/#over-the-air","title":"Over-the-Air","text":"<ul> <li>Over-the-Air Upgrades</li> </ul>"},{"location":"tags/#proxy","title":"Proxy","text":"<ul> <li>vSphere IPI &amp; Proxy</li> </ul>"},{"location":"tags/#scc","title":"SCC","text":"<ul> <li>SCC anyuid example</li> </ul>"},{"location":"tags/#telegram","title":"Telegram","text":"<ul> <li>Telegram receiver</li> </ul>"},{"location":"tags/#udn","title":"UDN","text":"<ul> <li>User-defined networks</li> </ul>"},{"location":"tags/#upi","title":"UPI","text":"<ul> <li>Create MachineSets on VMware UPI (integrated)</li> <li>VMware example installation (UPI, Static IP, non-integrated)</li> </ul>"},{"location":"tags/#vmware","title":"VMware","text":"<ul> <li>Create MachineSets on VMware UPI (integrated)</li> <li>vSphere IPI &amp; Proxy</li> <li>VMware example installation (UPI, Static IP, non-integrated)</li> <li>OpenShift.tv - vSphere IPI in a disconnected environment</li> </ul>"},{"location":"tags/#aaq","title":"aaq","text":"<ul> <li>Application Aware Quota</li> </ul>"},{"location":"tags/#air-gapped","title":"air-gapped","text":"<ul> <li>Air-gapped installation</li> <li>JFrog Artifactory Enterprise Operator</li> <li>OpenShift.tv - vSphere IPI in a disconnected environment</li> <li>Air-gapped/Disconnected</li> </ul>"},{"location":"tags/#alertmanager","title":"alertmanager","text":"<ul> <li>Debug receiver</li> <li>GitHub receiver</li> <li>Microsoft Teams receiver</li> <li>Telegram receiver</li> </ul>"},{"location":"tags/#argocd","title":"argocd","text":"<ul> <li>GitOps</li> </ul>"},{"location":"tags/#authentication","title":"authentication","text":"<ul> <li>Authentication</li> <li>Client Certificate</li> </ul>"},{"location":"tags/#autoscaler","title":"autoscaler","text":"<ul> <li>Cluster autoscaler</li> </ul>"},{"location":"tags/#autoscaling","title":"autoscaling","text":"<ul> <li>Cluster autoscaler</li> <li>Pod Autoscaling</li> </ul>"},{"location":"tags/#backup","title":"backup","text":"<ul> <li>Etcd Backup</li> </ul>"},{"location":"tags/#billy","title":"billy","text":"<ul> <li>Agent-based non-integrated</li> </ul>"},{"location":"tags/#build","title":"build","text":"<ul> <li>OpenShift Build's</li> <li>Entitled builds on OpenShift 4</li> <li>Tekton / OpenShift Pipelines</li> </ul>"},{"location":"tags/#buildpacks","title":"buildpacks","text":"<ul> <li>Cloud Native BuildPacks</li> </ul>"},{"location":"tags/#certificate","title":"certificate","text":"<ul> <li>Client Certificate</li> </ul>"},{"location":"tags/#certificates","title":"certificates","text":"<ul> <li>Certificates</li> <li>Service Serving Certificate Secrets Example</li> </ul>"},{"location":"tags/#client","title":"client","text":"<ul> <li>OpenShift/Kubernetes Client</li> </ul>"},{"location":"tags/#cnv","title":"cnv","text":"<ul> <li>Hosted Control Plane - KubeVirt Networking</li> <li>SNO on OCP-V</li> <li>OpenShift Virtualization (CNV/KubeVirt)</li> <li>Node Health Check</li> <li>PCI passthrough</li> <li>Storage</li> <li>OpenShift Virt Template example</li> <li>Application Aware Quota</li> <li>Cross Cluster Live Migration</li> <li>Networking</li> </ul>"},{"location":"tags/#control-plane","title":"control-plane","text":"<ul> <li>Add Node to an existing cluster</li> <li>Control plane</li> <li>Restoring etcd quorum (lost quorum)</li> </ul>"},{"location":"tags/#coreos","title":"coreos","text":"<ul> <li>How to adjust the an RHEL CoreOS ISO</li> </ul>"},{"location":"tags/#csi","title":"csi","text":"<ul> <li>CSI Driver NFS</li> <li>Storage</li> </ul>"},{"location":"tags/#debug","title":"debug","text":"<ul> <li>Debug receiver</li> </ul>"},{"location":"tags/#deploy","title":"deploy","text":"<ul> <li>Java/JAR Deployment</li> </ul>"},{"location":"tags/#disconnected","title":"disconnected","text":"<ul> <li>Air-gapped installation</li> <li>JFrog Artifactory Enterprise Operator</li> <li>OpenShift.tv - vSphere IPI in a disconnected environment</li> </ul>"},{"location":"tags/#dns","title":"dns","text":"<ul> <li>External DNS</li> </ul>"},{"location":"tags/#entitlement","title":"entitlement","text":"<ul> <li>Entitled builds on OpenShift 4</li> <li>Cluster entitlement</li> <li>Entitlement debugging</li> </ul>"},{"location":"tags/#etcd","title":"etcd","text":"<ul> <li>Etcd Backup</li> <li>Restoring etcd quorum (lost quorum)</li> </ul>"},{"location":"tags/#fedora","title":"fedora","text":"<ul> <li>Fedora Workstation</li> </ul>"},{"location":"tags/#freeipa","title":"freeipa","text":"<ul> <li>External DNS</li> </ul>"},{"location":"tags/#gitops","title":"gitops","text":"<ul> <li>GitOps</li> </ul>"},{"location":"tags/#hcp","title":"hcp","text":"<ul> <li>Hosted Control Plane</li> <li>Hosted Control Plane - KubeVirt Networking</li> </ul>"},{"location":"tags/#hypershift","title":"hypershift","text":"<ul> <li>Hosted Control Plane</li> <li>Hosted Control Plane - KubeVirt Networking</li> </ul>"},{"location":"tags/#installation","title":"installation","text":"<ul> <li>SNO on OCP-V</li> </ul>"},{"location":"tags/#integrated","title":"integrated","text":"<ul> <li>Create MachineSets on VMware UPI (integrated)</li> </ul>"},{"location":"tags/#jar","title":"jar","text":"<ul> <li>Java/JAR Deployment</li> </ul>"},{"location":"tags/#java","title":"java","text":"<ul> <li>Java/JAR Deployment</li> </ul>"},{"location":"tags/#kenrel","title":"kenrel","text":"<ul> <li>Build and load Kernel Model</li> </ul>"},{"location":"tags/#keycloak","title":"keycloak","text":"<ul> <li>Keycloak</li> </ul>"},{"location":"tags/#kubeconfig","title":"kubeconfig","text":"<ul> <li>Restore</li> </ul>"},{"location":"tags/#kubectl","title":"kubectl","text":"<ul> <li>OpenShift/Kubernetes Client</li> </ul>"},{"location":"tags/#kubevirt","title":"kubevirt","text":"<ul> <li>CSI Driver NFS</li> <li>Hosted Control Plane - KubeVirt Networking</li> <li>SNO on OCP-V</li> <li>OpenShift Virtualization (CNV/KubeVirt)</li> <li>Node Health Check</li> <li>PCI passthrough</li> <li>Storage</li> <li>OpenShift Virt Template example</li> <li>Descheduler</li> <li>Cross Cluster Live Migration</li> <li>Networking</li> </ul>"},{"location":"tags/#laptop","title":"laptop","text":"<ul> <li>Fedora Workstation</li> </ul>"},{"location":"tags/#lifecycle","title":"lifecycle","text":"<ul> <li>Shutdown</li> </ul>"},{"location":"tags/#lldp","title":"lldp","text":"<ul> <li>LLDP</li> </ul>"},{"location":"tags/#microsoft-teams","title":"microsoft-teams","text":"<ul> <li>Microsoft Teams receiver</li> </ul>"},{"location":"tags/#monitoring","title":"monitoring","text":"<ul> <li>Monitoring</li> <li>Debug receiver</li> <li>GitHub receiver</li> <li>Microsoft Teams receiver</li> <li>Telegram receiver</li> </ul>"},{"location":"tags/#networking","title":"networking","text":"<ul> <li>Networking</li> </ul>"},{"location":"tags/#nfs","title":"nfs","text":"<ul> <li>CSI Driver NFS</li> </ul>"},{"location":"tags/#node","title":"node","text":"<ul> <li>Add Node to an existing cluster</li> </ul>"},{"location":"tags/#non-integrated","title":"non-integrated","text":"<ul> <li>Agent-based non-integrated</li> </ul>"},{"location":"tags/#oc","title":"oc","text":"<ul> <li>OpenShift/Kubernetes Client</li> </ul>"},{"location":"tags/#ocp-v","title":"ocp-v","text":"<ul> <li>CSI Driver NFS</li> <li>Hosted Control Plane - KubeVirt Networking</li> <li>SNO on OCP-V</li> <li>OpenShift Virtualization (CNV/KubeVirt)</li> <li>Node Health Check</li> <li>Storage</li> <li>OpenShift Virt Template example</li> <li>Descheduler</li> <li>Cross Cluster Live Migration</li> <li>Networking</li> </ul>"},{"location":"tags/#operators","title":"operators","text":"<ul> <li>Air-gapped/Disconnected</li> </ul>"},{"location":"tags/#pipeline","title":"pipeline","text":"<ul> <li>Tekton / OpenShift Pipelines</li> <li>Cloud Native BuildPacks</li> </ul>"},{"location":"tags/#prometheus","title":"prometheus","text":"<ul> <li>Debug receiver</li> <li>GitHub receiver</li> <li>Microsoft Teams receiver</li> <li>Telegram receiver</li> </ul>"},{"location":"tags/#pvc","title":"pvc","text":"<ul> <li>Storage Migration (Container)</li> </ul>"},{"location":"tags/#redhatsso","title":"redhatsso","text":"<ul> <li>Keycloak</li> </ul>"},{"location":"tags/#restart","title":"restart","text":"<ul> <li>Shutdown</li> </ul>"},{"location":"tags/#restore","title":"restore","text":"<ul> <li>Restore</li> </ul>"},{"location":"tags/#restriced","title":"restriced","text":"<ul> <li>OpenShift.tv - vSphere IPI in a disconnected environment</li> </ul>"},{"location":"tags/#restricted-network","title":"restricted-network","text":"<ul> <li>Air-gapped installation</li> <li>JFrog Artifactory Enterprise Operator</li> </ul>"},{"location":"tags/#rhcos","title":"rhcos","text":"<ul> <li>How to adjust the an RHEL CoreOS ISO</li> </ul>"},{"location":"tags/#s2i","title":"s2i","text":"<ul> <li>S2I - R shiny</li> </ul>"},{"location":"tags/#shutdown","title":"shutdown","text":"<ul> <li>Shutdown</li> </ul>"},{"location":"tags/#sno","title":"sno","text":"<ul> <li>SNO on OCP-V</li> </ul>"},{"location":"tags/#storage","title":"storage","text":"<ul> <li>Storage</li> <li>NFS Client provisioner</li> <li>Storage Migration (Container)</li> <li>Storage</li> <li>Storage</li> <li>IBM Fusion Access SAN</li> </ul>"},{"location":"tags/#taga","title":"tagA","text":"<ul> <li>Live Migration</li> </ul>"},{"location":"tags/#tagb","title":"tagB","text":"<ul> <li>Faketime for your application</li> <li>Live Migration</li> </ul>"},{"location":"tags/#tekton","title":"tekton","text":"<ul> <li>Tekton / OpenShift Pipelines</li> <li>Cloud Native BuildPacks</li> </ul>"},{"location":"tags/#template","title":"template","text":"<ul> <li>OpenShift Virt Template example</li> </ul>"},{"location":"tags/#time","title":"time","text":"<ul> <li>Faketime for your application</li> </ul>"},{"location":"tags/#troubleshooting","title":"troubleshooting","text":"<ul> <li>Troubleshooting</li> </ul>"},{"location":"tags/#ubi","title":"ubi","text":"<ul> <li>Universal Base Images</li> </ul>"},{"location":"tags/#v412","title":"v4.12","text":"<ul> <li>Cross Cluster Live Migration</li> </ul>"},{"location":"tags/#v417","title":"v4.17","text":"<ul> <li>MachineConfig</li> <li>Storage Migration (Container)</li> <li>Add Node to an existing cluster</li> <li>Restoring etcd quorum (lost quorum)</li> <li>PCI passthrough</li> <li>Live Migration</li> <li>Networking</li> <li>Air-gapped/Disconnected</li> </ul>"},{"location":"tags/#v418","title":"v4.18","text":"<ul> <li>Entitled builds on OpenShift 4</li> <li>Application Aware Quota</li> <li>LLDP</li> <li>User-defined networks</li> <li>Air-gapped/Disconnected</li> </ul>"},{"location":"tags/#v419","title":"v4.19","text":"<ul> <li>Egress IP</li> <li>Air-gapped/Disconnected</li> </ul>"},{"location":"tags/#v420","title":"v4.20","text":"<ul> <li>IBM Fusion Access SAN</li> </ul>"},{"location":"tags/#vsphere","title":"vSphere","text":"<ul> <li>Create MachineSets on VMware UPI (integrated)</li> <li>vSphere IPI &amp; Proxy</li> <li>VMware example installation (UPI, Static IP, non-integrated)</li> <li>OpenShift.tv - vSphere IPI in a disconnected environment</li> </ul>"},{"location":"tags/#vmware_1","title":"vmware","text":"<ul> <li>Agent-based non-integrated</li> </ul>"},{"location":"tags/#vsphere_1","title":"vsphere","text":"<ul> <li>Agent-based non-integrated</li> </ul>"},{"location":"tags/#work-in-progress","title":"work-in-progress","text":"<ul> <li>Quota - WiP</li> </ul>"},{"location":"tags/#workstation","title":"workstation","text":"<ul> <li>Fedora Workstation</li> </ul>"},{"location":"tags/#x509","title":"x509","text":"<ul> <li>Client Certificate</li> </ul>"}]}